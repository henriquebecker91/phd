%
% exemplo genérico de uso da classe iiufrgs.cls
% $Id: iiufrgs.tex,v 1.1.1.1 2005/01/18 23:54:42 avila Exp $
%
% This is an example file and is hereby explicitly put in the
% public domain.
%
\documentclass[ppgc,tese,english,formais,babel]{iiufrgs}
% Para usar o modelo, deve-se informar o programa e o tipo de documento.
% Programas :
%   * cic       -- Graduação em Ciência da Computação
%   * ecp       -- Graduação em Ciência da Computação
%   * ppgc      -- Programa de Pós Graduação em Computação
%   * pgmigro   -- Programa de Pós Graduação em Microeletrônica
%
% Tipos de Documento:
%   * tc                -- Trabalhos de Conclusão (apenas cic e ecp)
%   * diss ou mestrado  -- Dissertações de Mestrado (ppgc e pgmicro)
%   * tese ou doutorado -- Teses de Doutorado (ppgc e pgmicro)
%   * ti                -- Trabalho Individual (ppgc e pgmicro)
%
% Outras Opções:
%   * english    -- para textos em inglês
%   * openright  -- Força início de capítulos em páginas ímpares (padrão da
%                   biblioteca)
%   * oneside    -- Desliga frente-e-verso
%   * nominatalocal -- Lê os dados da nominata do arquivo nominatalocal.def

% Use unicode
\usepackage[utf8]{inputenc}   % pacote para acentuação

% Necessário para que as tabelas tenham separador correto:
% '--' (travessão) ao invés de ':' (dois-pontos).
\usepackage{float}

% Necessário para incluir figuras
\usepackage{graphicx}           % pacote para importar figuras

\usepackage{times}              % pacote para usar fonte Adobe Times
% \usepackage{palatino}
% \usepackage{mathptmx}          % p/ usar fonte Adobe Times nas fórmulas

\usepackage[alf,abnt-emphasize=bf]{abntex2cite}	% pacote para usar citações abnt

% Packages added by Henrique Becker
\usepackage{ragged2e} % for \justifying to be used in the legends
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{color}
\usepackage[table]{xcolor}
\definecolor{gray-table-row}{gray}{0.90}
\definecolor{gray-inner-row}{gray}{0.95}
\newcommand{\ditto}{\textquotedbl}
% Packages for computer code
\usepackage{algorithm}
\usepackage{algpseudocode}
% Package for multiline comments
\usepackage{verbatim}
% Packages for formatting the mathematical formulation
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm} % for correct font and emphasis in formulation max/min
% For better treatment of nested lists.
\usepackage{enumitem}
% For better referencing (\cref, \Cref).
\usepackage[nameinlink]{cleveref}

% tries to break texttt tidbits PROBLEM: it adds hyphens
%\usepackage[htt]{hyphenat}
\catcode`_=12 %
\renewcommand{\texttt}[1]{%
  \begingroup
  \ttfamily
  \begingroup\lccode`~=`/\lowercase{\endgroup\def~}{/\discretionary{}{}{}}%
  \begingroup\lccode`~=`[\lowercase{\endgroup\def~}{[\discretionary{}{}{}}%
  \begingroup\lccode`~=`.\lowercase{\endgroup\def~}{.\discretionary{}{}{}}%
  \begingroup\lccode`~=`_\lowercase{\endgroup\def~}{_\discretionary{}{}{}}%
  \catcode`/=\active\catcode`[=\active\catcode`.=\active\catcode`_=\active
  \scantokens{#1\noexpand}%
  \endgroup
}
\catcode`_=8 %

% We want to use \cref and \Cref correctly (i.e., cref in the middle of
% a sentence, and Cref only in the beginning of a sentence), but we also
% need to follow the standard that says that all references are capitalized
% independent of where they are in a sentence.
\crefname{chapter}{Chapter}{Chapters}
\crefname{section}{Section}{Sections}
\crefname{table}{Table}{Table}
\crefname{figure}{Figure}{Figure}
% Fix the mess that is the theorem/definition environment defined by
% iiufrgs.cls (formais.def).
\crefname{envtheorem}{Theorem}{Theorems}
\crefname{envdefinition}{Definition}{Definitions}

\usepackage{tikz}
\usetikzlibrary{patterns}
% The booktabs import below allow us to use cmidrule, which should replace
% cline in tables and avoid the extracolsep hack (which breaks rowcolors).
\usepackage{booktabs}

\newif\iffinalversion
\finalversiontrue
\newcommand{\newtext}[1]{\iffinalversion%
#1%
\else%
\textcolor{blue}{#1}%
\fi%
}
\newcommand{\oldtext}[1]{\iffinalversion%
\else%
\textcolor{red}{#1}%
\fi%
}

\newcommand{\myproblem}{G2KP}
\newcommand{\modelBCE}{BCE}
\newcommand{\modelFMT}{FMT}
\newcommand{\modelBecker}{BBA}
\newcommand{\modelGrid}{MLB}
\newcommand{\modelHierarchical}{MM1}
\newcommand{\modelImplicit}{MM2}
\newcommand{\modelOrigami}{MM3}
\newcommand{\tilderange}{\raisebox{0.5ex}{\texttildelow}}

\newcommand{\isep}{\mathrel{{.}\,{.}}\nobreak} % for integer ranges
\newcommand{\bestcolumnemph}[1]{\textbf{#1}}

% Necessary for formulation layout workaround.
\newcommand{\pushright}[0]{\hskip \textwidth minus \textwidth}
\makeatletter
\newcommand{\specialcell}[1]{\ifmeasuring@#1\else\omit$\displaystyle#1$\ignorespaces\fi}

% Avoids problem with citet in section names (the name of the reference
% becomes all-caps and then it is not found in the bib file).
\def\citethopperthesis{\citet{hopper_thesis}}

%
% Informações gerais
%
\title{The state of the art in MILP formulations for the guillotine 2D knapsack and related problems %An enhanced formulation for guillotine 2D cutting problems
%\thanks{This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code 001}
}

\author{Becker}{Henrique}

% orientador e co-orientador são opcionais (não diga isso pra eles :))
\advisor[Prof.~Dr.]{Buriol}{Luciana Salete}
\coadvisor[Prof.~Dr.]{Araújo}{Olinto}

% a data deve ser a da defesa; se nao especificada, são gerados
% mes e ano correntes
%\date{maio}{2001}

% o local de realização do trabalho pode ser especificado (ex. para TCs)
% com o comando \location:
%\location{Itaquaquecetuba}{SP}

% itens individuais da nominata podem ser redefinidos com os comandos
% abaixo:
% \renewcommand{\nominataReit}{Prof\textsuperscript{a}.~Wrana Maria Panizzi}
% \renewcommand{\nominataReitname}{Reitora}
% \renewcommand{\nominataPRE}{Prof.~Jos{\'e} Carlos Ferraz Hennemann}
% \renewcommand{\nominataPREname}{Pr{\'o}-Reitor de Ensino}
% \renewcommand{\nominataPRAPG}{Prof\textsuperscript{a}.~Joc{\'e}lia Grazia}
% \renewcommand{\nominataPRAPGname}{Pr{\'o}-Reitora Adjunta de P{\'o}s-Gradua{\c{c}}{\~a}o}
% \renewcommand{\nominataDir}{Prof.~Philippe Olivier Alexandre Navaux}
% \renewcommand{\nominataDirname}{Diretor do Instituto de Inform{\'a}tica}
% \renewcommand{\nominataCoord}{Prof.~Carlos Alberto Heuser}
% \renewcommand{\nominataCoordname}{Coordenador do PPGC}
% \renewcommand{\nominataBibchefe}{Beatriz Regina Bastos Haro}
% \renewcommand{\nominataBibchefename}{Bibliotec{\'a}ria-chefe do Instituto de Inform{\'a}tica}
% \renewcommand{\nominataChefeINA}{Prof.~Jos{\'e} Valdeni de Lima}
% \renewcommand{\nominataChefeINAname}{Chefe do \deptINA}
% \renewcommand{\nominataChefeINT}{Prof.~Leila Ribeiro}
% \renewcommand{\nominataChefeINTname}{Chefe do \deptINT}

% A seguir são apresentados comandos específicos para alguns
% tipos de documentos.

% Relatório de Pesquisa [rp]:
% \rp{123}             % numero do rp
% \financ{CNPq, CAPES} % orgaos financiadores

% Trabalho Individual [ti]:
% \ti{123}     % numero do TI
% \ti[II]{456} % no caso de ser o segundo TI

% Monografias de Especialização [espec]:
% \espec{Redes e Sistemas Distribuídos}      % nome do curso
% \coord[Profa.~Dra.]{Weber}{Taisy da Silva} % coordenador do curso
% \dept{INA}                                 % departamento relacionado

%
% palavras-chave
%
\keyword{Combinatorial optimization}
\keyword{2D knapsack}
\keyword{Guillotine cuts}
\keyword{Mathematical formulation}

%
% inicio do documento
%
\begin{document}

% folha de rosto
% às vezes é necessário redefinir algum comando logo antes de produzir
% a folha de rosto:
% \renewcommand{\coordname}{Coordenadora do Curso}
\maketitle

% dedicatoria
\clearpage
\begin{flushright}
\mbox{}\vfill
{\sffamily\itshape
``The optimal solution of a model is not an optimal solution of a problem\\
unless the model is a perfect representation of the problem,\\
which it never is.''\\}
--- \textsc{The Future of Operational Research is Past, Russell L. Ackoff, 1979}
\end{flushright}

\chapter*{Dedication and Acknowledgments}

\noindent
To my parents for their support and patience, especially in these trying times of forced cohabitation.\\\\
To my advisor, Buriol, for all the opportunities given, and for not losing hope.\\\\
To my co-advisor, Olinto, for saving my doctorate with a change of topic and one whole year of close assistance.\\\\
To Maurício Araldi for the daily dose of gaming and conversation, albeit being on the other side of the atlantic ocean.\\\\
To Ana for lending me Maurício this way, our weekend gaming, and shipping discussions.\\\\
To my hometown friends for keeping contact (and, therefore, my sanity), and even hopping on board with my online birthday party idea.\\\\
To my brother, Artur, for remembering me that the academic journey is tortuous even for the most applied and, also, for every manuscript revised.\\\\
To Mateus Martin for lending me his codes of many methods and writing a paper with me.
To Dennis and Tadeu which had first shown me what Artur remembered me.\\\\
To my college teachers for always seeing in me a future colleague of the profession.\\\\
To Marcus Ritt, Manuel Iori, and Reinaldo Morabito, for all their comments related to the proposal of this thesis; special thanks to Marcus for also being the best teacher I knew at UFRGS.\\\\
To everyone in the lab, which relied on me for our shared problems and for which I could rely on back; especially Artur, Alberto, and Gabriel, on the matter of our shared experiment servers (both the old servers antics and the new servers purchase).\\\\

% resumo na língua do documento
\begin{abstract}

This thesis advances the state of the art in Mixed-Integer Linear Programming (MILP) formulations for Guillotine 2D Cutting Problems by \oldtext{(i) adapting a previously-known reduction to our preprocessing phase and by (ii) enhancing a previous formulation by cutting down its size and symmetries}\newtext{(i) proposing a (re-)formulation that improves on a state-of-the-art formulation by cutting down its size and symmetries; (ii) adapting a previously-known reduction in a novel way for the preprocessing phase of the mentioned formulations; (iii) providing extensive experiments comparing the state of the art and the proposed formulation over many literature datasets; (iv) proposing a hybridised variant of the mentioned formulations which improves the performance for some hard instances; (v) proposing and validating a rotation-only symmetry-breaking strategy for the mentioned formulations}.
This thesis focuses on the Guillotine 2D Knapsack Problem with orthogonal and unrestricted cuts, constrained demand, unlimited stages, and no rotation.
However, the formulation may be adapted to many related problems \newtext{including the Guillotine 2D Multiple Knapsack Problem, the Guillotine 2D Cutting Stock Problem, and the Guilltone 2D Orthogonal Packing Problem, all three of which are approached and experimented upon in this thesis}.
The code is available.

Concerning the set of 59 instances used to benchmark the \oldtext{original}\newtext{the state-of-the-art} formulation \newtext{in which the author took inspiration}, and summing the statistics for all models generated, the \oldtext{enhanced}\newtext{proposed} formulation has only a small fraction of the variables and constraints of the original model (respectively, 3.07\% and 8.35\%).
The enhanced formulation also takes about 4 hours to solve all instances while the original formulation takes 12 hours to solve 53 of them (the other 6 runs hit a three-hour time limit each).
\oldtext{The author integrates, to both formulations, a pricing framework proposed for the original formulation; the enhanced formulation keeps a significant advantage in this situation.}
In a recently proposed set of 80 harder instances, the enhanced formulation (with and without the pricing framework) found: 22 optimal solutions for the unrestricted problem (5 already known, 17 new); 22 optimal solutions for the restricted problem (all new for the problem and none is the same as the optimal unrestricted solution); better lower bounds for 25 instances; better upper bounds for 58 instances.
\newtext{
Concerning other formulations for the problem in the literature, the proposed formulation has shorter run times, and it proves the optimality for more instances.
The proposed formulation only fails to deliver good solutions in the datasets that no formulation was able to solve any instance. In such datasets, other formulations did deliver good primal solutions even if they could not solve any instance.
}
\oldtext{
For future research, we intend to (i) adapt the formulation to some related problems and variants and (ii) systematically approach all datasets in the literature.
The related (guillotine) problems include the Orthogonal Packing Problem, the Strip Packing Problem and the two-dimensional versions of the Multiple Knapsack Problem and the Cutting Stock Problem.
The homogeneous variants of these two last problems are simpler to formulate, but the and heterogeneous variant may also be explored; the rotation variant may be explored for all the related problems.
}
\end{abstract}

% resumo na outra língua
% como parametros devem ser passados o titulo e as palavras-chave
% na outra língua, separadas por vírgulas
\begin{englishabstract}{O estado da arte em formulações de PLI para a mochila guilhotinada 2D e problemas relacionados}{Otimização combinatorial. Mochila 2D. Cortes guilhotinados. Formulação matemática}%{Uma formulação melhorada para problemas de corte guillotinado 2D}{Otimização combinatorial. Mochila 2D. Cortes guilhotinados. Formulação matemática}

Essa tese avança o estado da arte em formulações de Programação Linear Inteira (PLI) para Problemas de Corte Guilhotinado 2D pela \oldtext{(i) adaptação de uma redução previamente conhecida para a nossa fase de pré-processamento e pela (ii) melhora de uma formulação anterior diminuindo o seu tamanho e as suas simetrias}\newtext{(i) propondo uma (re-)formulação que melhora uma formulação do estado da arte por meio da redução do seu tamanho e suas simetrias; (ii) adaptando uma redução já conhecida de forma inovadora para a fase de pré-processamento dessas formulações; (iii) provendo extensivos experimentos comparando o estado da arte e a formulação proposta sobre vários conjuntos de instância da literatura; (iv) propondo uma variante hibridizada das formulações mencionadas que melhora a performance para algumas instâncias difíceis; (v) propondo e validando uma estratégia de quebra de simetrias para as formulações mencionadas}.
O nosso foco é o Problema da Mochila 2D Guilhotinado com cortes ortogonais e irrestritos, demanda limitada, estágios ilimitados, e sem rotação -- entretanto, a formulação pode ser adaptada para vários problemas relacionados \newtext{incluindo o problema da mochila múltipla 2D guilhotinada, o problema do corte de estoque 2D guilhotinado, e o problema de empacotamento ortogonal 2D guilhotinado, todos os três são abordados e alvo de experimentos nessa tese}.
O código está disponível.

\oldtext{The author integrates, to both formulations, a pricing framework proposed for the original formulation; the enhanced formulation keeps a significant advantage in this situation.}
Considerando as 59 instâncias usadas nos experimentos da formulação \oldtext{original}\newtext{em que o autor se inspirou}, e somando os valores para todos os modelos gerados, a formulação \oldtext{melhorada}\newtext{proposta} tem apenas uma pequena fração das variáveis e restrições do modelo original (respectivamente, 3.07\% e 8.35\%).
A formulação melhorada soluciona todas as 59 instâncias em cerca de 4 horas enquanto a formulação original soluciona 53 em 12 horas (as outras 6 instâncias não são solucionadas dentro do limite de 3 horas por instância).
\oldtext{Nós integramos, em ambas formulações, uma estrutura de precificação proposta para a formulação original; a formulação melhorada mantém uma vantagem significativa nessa situação.}
Em um conjunto de 80 instâncias difíceis recentemente proposto, a formulação melhorada (com e sem a estrutura de precificação) encontrou: 22 soluções ótimas para o problema com cortes irrestritos (5 já conhecidas, 17 novas); 22 soluções ótimas para o problema com cortes restritos (todas novas para o problema e nenhuma é a mesma que do problema de cortes irrestritos); melhores limitantes inferiores para 25 instâncias; melhores limitantes superiores para 58 instâncias.
\newtext{Considerando outras formulações para o problema na literatura, a formulação proposta apresenta tempos de execução menores, e prova a otimalidade para mais instâncias.
Somente nos conjuntos de instâncias em que nenhuma formulação solucionou instância alguma é que a formulação proposta falhou em encontrar boas soluções primais enquanto outras formulações obtiveram êxito.
A formulação proposta somente falhou em obter soluções de boa qualidade nos conjuntos de instâncias em que nenhuma formulação conseguiu solucionar instância alguma.
Nesses conjuntos de dados, outras formulações obtiveram boas soluções primais mesmo não sendo capazes de solucionar instância alguma.
}

\oldtext{
Para pesquisa futura, nós pretendemos (i) adaptar a formulação para alguns problemas relacionados e suas variantes além de (ii) abordar sistematicamente todos conjuntos de instâncias da literatura.
Os problemas (guilhotinados) relacionados incluem o Problema de Empacotamento Ortogonal, o Problema de Empacotamento em Faixas, e a versão 2D do Problema da Mochila Múltipla e o Problema de Corte de Estoque.
As versões homogêneas desses últimos dois problemas são mais simples de formular, mas as versões heterogêneas podem ser exploradas também; a variante com rotação pode ser explorada em todos os problemas relacionados.
}
\end{englishabstract}

% lista de figuras
%\listoffigures

% lista de tabelas
\listoftables

% lista de abreviaturas e siglas
% o parametro deve ser a abreviatura mais longa
\begin{listofabbrv}{APTAS}
\input{abbreviations.tex}
\end{listofabbrv}

% idem para a lista de símbolos
%\begin{listofsymbols}{$\alpha\beta\pi\omega$}
%       \item[$\sum{\frac{a}{b}}$] Somatório do produtório
%       \item[$\alpha\beta\pi\omega$] Fator de inconstância do resultado
%\end{listofsymbols}

% sumario
\tableofcontents

% aqui comeca o texto propriamente dito

% introducao
\chapter{Introduction}

The problem \oldtext{we focus on} this work \newtext{centres around} is the Guillotine 2D Knapsack Problem with orthogonal (and unrestricted) cuts, constrained demand, unlimited stages, and no rotation.
The author will refer to this specific variant as G2KP\newtext{, and fully describe it in the next section}.
The G2KP is a strongly NP-hard problem~\newtext{\citep{korf:initial:2003,dolatabadi:2012}; more details in~\cref{sec:np_hard_related_problems}}.
% The following two phrases are useful for foreshadowing but ultimately redundant.
\newtext{This work also examines a specific kind of restricted cuts, three distinct problems closely related to the G2KP, and the variant that allows piece rotation (in all the studied problems); Other problems and variants may be mentioned to contextualize this work but are not experimented upon.}
\oldtext{We propose two simple but effective enhancements regarding a state-of-the-art MILP formulation for the G2KP (which may also benefit some closely related problem variants).}
The work \oldtext{also} focuses on obtaining optimal solutions \oldtext{for this problem} through Mixed-Integer Linear Programming (MILP).
%\newtext{The contributions of this work are summarised in~\cref{sec:contributions}.}

% TODO: this needs to be reworked now that we have the related problems section.
\newtext{
The three distinct problems mentioned above are the Multiple Knapsack Problem (MKP), the Orthogonal Packing Problem (OPP), and the Cutting Stock Problem (CSP). The Bin Packing Problem (BPP) is also covered, but the author treats it as a special case of the CSP and distinguishes between the two only when their difference (i.e., piece type diversity) becomes relevant.
The next section explains the G2KP, its variants, and the basics of the chosen mathematical notation.
The other three problems share most of the variants and the notation but are discussed in a separate chapter (\cref{sec:other_problems}).
}

\section{Explanation of the \oldtext{problem}\newtext{G2KP} and its variants}

An instance of the G2KP consists of: a rectangle of length~\(L\) and width~\(W\) (hereafter called \emph{original plate}); a set of rectangles~\(\bar{J}\) (also referred to as \emph{pieces}) where each rectangle~\(i \in \bar{J}\) has a length~\(l_i\), a width~\(w_i\), a profit~\(p_i\), and a demand~\(u_i\)\newtext{.}
This work assumes, without loss of generality, that all such values are positive integers.

The G2KP seeks to maximise the profit of the pieces obtained by cutting the original plate.
The \emph{guillotine} qualifier means every cut always goes from one side of a plate to \newtext{the} other; a cut never stops or starts from the middle of a plate.
\oldtext{A consequence of this rule is that we often do not obtain the pieces directly from the original plate.}
The original plate is cut into intermediary plates \(j \in J\), \(J \supseteq \bar{J}\), which are further cut following the same rule.

If a plate is not cut further, then it may either be: thrown away as trim/waste for no profit; or, if it has the same size as a piece, sold by the piece profit value.
\emph{Orthogonal cuts} are always parallel to one side of a plate (and perpendicular to the other).
\oldtext{Consequently,}\newtext{In conjunction with only using guillotine cuts, this means that} any intermediary plate~\(j\) is always a rectangle, and has a well-defined~\(l_j\) and~\(w_j\).
\oldtext{The modifier \emph{unrestricted cuts} means each non-waste child plate from a series of parallel horizontal (vertical) cuts over the same plate do \emph{not} need to have the same width (length) of an existing piece.}
\newtext{\emph{Unrestricted cuts} mean the machine is allowed to make horizontal (vertical) cuts different from the length (width) of a piece.}
\oldtext{We will mention the G2KP with restricted cuts further in the text, as solving it}\newtext{In contrast, restricted cuts mean horizontal (vertical) cuts can only happen at positions that match a piece length (width), it may also mean that, in addition to this, a piece with matching length (width) \emph{must} be extracted from the first child plate of a restricted cut. In this paper, \emph{restricted} means only that the position of the cuts is restricted (not that the cut force a posterior piece extraction); the author creates and employ the term \emph{position-only restricted} to keep the reader aware of what is meant. Solving the position-only restricted problem} exactly is a costly but high-quality primal heuristic for the G2KP.

\emph{Constrained demand} means at most~\(u_i\) copies of piece~\(i\) can be sold, i.e., converted into profit.
The G2KP with \emph{unconstrained demand} is not strongly NP-hard, it is weakly NP-Hard; exact algorithms of pseudo-polynomial time complexity exist~\citep{beasley:1985:guillotine}.
\oldtext{Consequently, interesting G2KP instances have~\(u_j < \lceil L / l_j \rceil \times \lceil W / w_j \rceil \) for at least one piece~\(j\) (if not for all pieces).}
\newtext{Consequently, if~\(u_i \geq \beta_i : \forall i \in \bar{J}\), where \(\beta_i\) is an upper bound on the number of copies of piece~\(i\) that can be produced from the original plate, then the instance is probably better solved as an instance of the unconstrained G2KP instead. The author avoids this kind of instance in the experiments.}
The modifier \emph{unlimited stages} means there is no limit to the number of times the guillotine switches between horizontal and vertical orientations.
In the exact \(k\)-staged G2KP, the guillotine is switched at most \(k-1\) times.
Consequently, in a solution of the two-staged G2KP, all cuts in some orientation (and, consequently, parallel to each other) are done before any cuts in the other orientation are done (over the remains of the previous stage).
The non-exact \(k\)-staged G2KP adds one extra stage in which the only cuts allowed are the ones that trim plates to the size of pieces (i.e., one of the children of \oldtext{the}\newtext{this last} cut \oldtext{is}\newtext{must be} waste).
The \emph{no-rotation} qualifier means the plates never are rotated as to switch their length and width during the cutting process; especially, a plate~\(j\) cannot be sold as a piece of length~\(w_j\) and width~\(l_j\).

\newtext{If the text further qualifies the G2KP, it only means to discard the qualifiers above that directly conflict with the extra qualifiers, if any.}
\newtext{For example, suppose the text refers to the \emph{unconstrained G2KP}. In that case, it means only to discard the \emph{constrained} qualifier but keep the remaining qualifiers, i.e., no rotation, unlimited stages, as well as guillotined, orthogonal, and unrestricted cuts.}
\newtext{\autoref{fig:qualifier_examples} may help understand some of the discussed characteristics.}

\begin{figure}[h]
  \caption{\newtext{Examples of valid patterns for most of the discussed problem variants.}}
  \center
  \input{diagrams/qualifier_examples.tex}
  \legend{\justifying \newtext{In \emph{Non-Orthogonal}, \emph{Unrestricted cuts}, and \emph{Restricted cuts}, the dashed line indicate the first cut of the pattern. The pattern using only restricted cuts packs one less piece because no matter which restricted cut is done first, it is impossible to obtain the same six pieces obtained by the pattern with unrestricted cuts. The initial unrestricted cut is essential to obtain the six-piece pattern. In \emph{Inexact 3-staged}, the dashed lines indicate the last cut of each stage. Consider this last diagram. The bottom three horizontal cuts are done in the first stage. In the second stage, two pieces from the first stage are trimmed by a vertical cut and the three leftmost vertical cuts are done. In the third stage, two of the three pieces of the second stage are trimmed with horizontal cuts and the two rightmost horizontal cuts are done. The fact the variant is inexact only matters for the topmost piece among the two pieces cut in the third stage; the vertical cut that trims that piece \emph{must} be done after the third stage and therefore happens at the trim-only fourth stage only allowed in a three-staged variant if it is \emph{inexact}. Souce: the author.}}
  \label{fig:qualifier_examples}
\end{figure}

The literature further distinguishes between \emph{weigthed} and \emph{unweighted} problem variants.
In the weighted variant, pieces have an arbitrary profit value, while in the unweighted variant, the profit value is always equivalent to the piece area.
Consequently, the unweighted variant is equivalent to minimising waste and is a particular case of the weighted variant.
Any algorithm that solves the weighted variant (as is the case in this thesis) can solve the unweighted variant by setting the piece profit values to their areas.

\section{Motivation}

\oldtext{The G2KP and its closely related variants are of undisputable interest of the industry, especially wood, paper, metal, and glass cutting industries.}
\newtext{Guillotine cutting problems are of interest of the industry, especially the wood~\cite{yanasse:linear:2008,morabito:hardboard:2007} and glass cutting industries~\cite{clautiaux:2019,parreno:2020}, often because of machinery limitations.}
\oldtext{The vast and growing literature on the subject examined by~\citet{iori:2020} and by~\citet{russo:2020} is enough proof of such interest.}
\newtext{There is a vast and growing literature on the subject as evidenced by~\citet{iori:2020} and by~\citet{russo:2020}.}
\oldtext{To pick a single recent case study see~\citet{clautiaux:2019}, which solves a unique variant of the Guillotine 2D Cutting Stock Problem for a glass factory manufacturing double-paned windows.}
\newtext{The cutting optimization problem proposed in the \emph{ROADEF/EURO Challenge 2018} was a guillotine cutting problem. The challenge was developed in collaboration with Saint-Gobain Glass France (a reference on flat glass manufacture). See~\citet{parreno:2020} for more details on this challenge.}

This work focuses on MILP as the solving method (instead of \emph{ad hoc} solutions) because its adaptability amplifies the value of any enhancements discovered.
A better MILP formulation means: a better solving procedure for the many (already mentioned) closely related problem variants;
a better continuous relaxation for computing an optimistic guess on the objective value of all these variants (some \emph{ad hoc} algorithms of the literature use MILP solvers to compute their bounds);
not only a better exact method but also a better base for heuristics or anytime procedures;
an immediate benefit from parallelisation, automatic problem decomposition, and solver-implemented heuristics;
and, finally, better ageing of the method over the years through the current trends of multiple-cores processors and ever-advancing solver performance.

\section{Contributions and thesis \oldtext{proposal} outline}
\label{sec:contributions}

\oldtext{
The main contributions of this work are:
an enhanced MILP formulation based on a previous state-of-the-art formulation, its proof of correctness, and empirical evidence of its better performance;
a straightforward adaptation of a previously known reduction procedure for both the original and the enhanced formulations, and empirical evidence of its positive impact on their performance;
new upper and lower bounds, as well as optimal values, for many recently proposed hard instances from~\citet{velasco:2019}.
For such, the author reimplemented a state-of-the-art MILP formulation and an optional pricing procedure used by it.
This reimplementation allows us to compare both approaches fully.
All code used is available in the first author's repository ({\small\url{https://github.com/henriquebecker91/GuillotineModels.jl/tree/0.2.4}}).}
% TODO: remove below only after we had put a similar disclaimer in each experiment section.
% \newtext{For reproducibility, the exact version of the code employed in this paper is available at~\cite{code:0:2:4}. However, we suggest using the better documented and maintained master branch ({\small\url{https://github.com/henriquebecker91/GuillotineModels.jl}}) if perfect reproduction is not necessary.}

\newtext{
The contributions of this thesis include:
\begin{itemize}
\item an enhanced MILP formulation based on a previous state-of-the-art formulation, its proof of correctness, and empirical evidence of its better performance;
\item a novel way to employ a previously known property (plate-size normalisation) for both the original and the enhanced formulations, and empirical evidence of its positive impact on their performance;
\item new upper and lower bounds, as well as optimal values, for many recently proposed hard instances from~\citet{velasco:2019};
\item a direct comparison with recent formulations of the literature highlighting the weak and strong points of each;
\item the adaptation of the proposed formulation for three related problems (G2MKP, G2OPP, and G2CSP) and empirical results over literature datasets to serve as base for future comparisons between formulations;
\item an hybridisation of the proposed formulation (with the formulation from~\citet{silva:2010}) that has moderate success in further reducing the run time for instances in which most time is spent at the B\&B phase (by disallowing some symmetries).
\end{itemize}
For such, the author reimplemented a state-of-the-art MILP formulation and an optional pricing procedure used by it.
}

The rest of the thesis is organised in the following way.
\Cref{sec:related_work} contextualises the topic of the thesis in the broader literature and lists the prior work on it.
\Cref{sec:tech_background} introduces the necessary mathematical concepts as well as the formulation used as the basis for the proposed formulation, the proposed formulation itself, and adaptations for the variant allowing rotation.
\Cref{sec:furini_vs_enhanced_comparison} experimentally examine the difference in model size and overall performance between the reimplementation of the base formulation, the original data on it, and the proposed (re-)formulation.
\Cref{sec:martin_chapter} experimentally examine the performance of the proposed formulation against other recent formulations of the literature for the G2KP.
\Cref{sec:other_problems} explains how to adapt the proposed formulation to three related problems and empirically test the adaptations over literature datasets; it also reports a generation mistake found in the T dataset.
\Cref{sec:hybridisation} presents an adaptation of the proposed formulation that hybridises it with a prior formulation for a more restricted problem without losing the optimality for the general case; experiments are included.
\Cref{sec:conclusions} delivers the conclusions based on the experiments presented in the previous chapters.
\Cref{sec:datasets} provides a detailed description of each instance dataset employed in this thesis.

\chapter{Related work}
\label{sec:related_work}

The literature on 2D cutting and packing, in general, is vast, as pointed out by \citet{iori:2020}, which catalogues exact methods and relaxations of \newtext{problems from} this field.
Even if the scope is further restricted to exact methods for the G2KP, the author could write a forty-page survey on it, as it was done by~\citet{russo:2020}.
Consequently, the author strongly suggests both surveys mentioned above for any reader interested in a broader understanding of the literature.

The review presented here is divided into two parts. The first part contextualises the reader by contraposing aspects of the thesis topic to other aspects found in the broader cutting and packing literature. The second part focus on the brief history of our main topic (i.e., MILP formulations for the G2KP).

\section{Broader literature contextualisation}

This section presents a list of aspects \textbf{not} shared by the thesis topic.
For each item, the literature concerning that aspect is briefly summarised.
If adequate, the item also contrasts the aspect with its opposite present in the main topic.

\subsection{Non-Guillotine\newtext{d problems}}

Both the G2KP and the 2KP are strongly NP-hard \citep{iori:2020}, and none is a generalisation of the other.
Therefore, theoretically, no problem is overall more challenging than the other.
In terms of modelling, however, the 2KP has the additional challenge that, once the first piece is cut, the remaining space is nonconvex \citep{fekete:1997}.
The G2KP has a better subproblem structure in this regard.
In the G2KP, once a cut is defined, there are two convex spaces, and the problem may be seen as multiple heterogeneous \newtext{G}2KPs\footnote{In fact, in the \cref{sec:adaptation_to_g2mkp} it will be shown that adapting the proposed formulation to the Multiple Knapsack Problem (homogeneous variant) is very straightforward because of this property.}.
However, this nicer structure cannot be fully exploited if a 2KP solving method is adapted to support guillotine cuts.
In \citet{nascimento:2019}, for example, the solving method has a harder time solving the G2KP because it is a method for the 2KP with the overhead of identifying guillotine cuts over it.

\subsection{Unconstrained \newtext{problems}}

The unconstrained G2KP was introduced by~\citet{gg:1965}, some mistakes of this first work were posteriorly corrected by~\citet{herz:1972}\footnote{\newtext{\citet{herz:1972} points out a mistake in an \emph{improved} algorithm presented in~\citet{gg:1965}. The improved algorithm reduces computational effort by skipping some horizontal and vertical cuts based on four criteria that should not impact the guarantee of optimality. A counterexample shows that this is not the case as the only optimal solution of the counterexample is unatainable when the four criteria are followed.}}
and by~\citet{beasley:1985:guillotine}\footnote{\newtext{\citet{beasley:1985:guillotine} points out a mistake in the \(k\)-staged recursion presented in~\citet{gg:1965}. In their recursion, the cut orientation of the first stage (either horizontal or vertical) changes depending on the current stage, e.g., the third stage assumes the first stage had vertical cuts, but the fourth stage assumes the first stage had horizontal cuts instead.}}.
Differently from the constrained variant, which is strongly NP-Hard, the unconstrained variant is weakly NP-Hard (it generalises the 1D Unbounded Knapsack Problem).
Consequently, exact solving methods in pseudo-polynomial time are possible, especially through Dynamic Programming (DP).
The lack of the combinatorial explosion caused by keeping track of the residual demand is what makes DP the most popular approach for the unconstrained variant and mostly unused for the constrained variant.
Such is the gap in difficulty between both variants that the state-of-the-art heuristic for the constrained variant solves the unconstrained variant repeatedly \citep{velasco:2019}.
The state-of-the-art method for the unconstrained problem appears to be \citet{russo:2014}, which is a DP method; they regarded the B\&B algorithm of \citet{kang:2011} as the previous state of the art.

\subsection{Heuristic \newtext{methods}}

As it is common for classic (strongly) NP-Hard problems, the literature on heuristic methods is colossal.
For example, \citet{ortmann:2010} compares across 252 heuristics for the 2D Strip Packing Problem (guillotine and non-guillotine variants).
For the thesis main problem, the G2KP, the author believes the best heuristic method is the one proposed by~\citet{velasco:2019}.
The method is inspired by the dynamic programming state-space relaxation of~\citet{nicos:1995:ssr}, which also inspired the earlier state-of-the-art heuristic that is~\citet{morabito:2010}.
The procedures described by \citet{velasco:2019} give both lower and upper bounds.
These methods were run over 500 instances of the literature for both rotation and no-rotation variants; they obtained either optimal or better bounds in all cases.
The bounds proved the optimality of 348 instances for the no-rotation variant and 385 for the rotation variant; some instances were open.
\citet{velasco:2019} then proposes 80 more challenging instances (which are included in this thesis experiments) and proves the optimality of many of them.

\subsection{Approximative \newtext{methods}}

For the G2KP, with and without rotation or weights, \citet{abed:2015} provide a quasi-Polynomial Time Approximation Scheme (quasi-PTAS), assuming the input data to be quasi-polynomially bounded integers (\citet{anna:2015:quasi} has a similar result for the non-guillotine variant).
For the Guillotine 2D Bin Packing Problem (without rotation), \citet{bansal:2005} gives an Asymptotic PTAS (APTAS).
\citet{christensen:2017} informs us that finding a PTAS for the Non-Guillotine 2D Knapsack Problem (with or without rotation or weights) is an open problem and that, for the non-guillotine variant, ``\citet{bansal:2009} gave a PTAS for the special case when the range of the profit-to-area ratio of the rectangles is bounded by a constant for both the cases with and without rotations'' and ``there is no FPTAS unless \(P = NP\), even for packing squares into squares \citep{leung:1990}.
"\citet{galvez:2017} propose a polynomial-time 1.89-approximation for the Non-Guillotine 2D Knapsack Problem\footnote{\citet{galvez:2017} defines the original plate as a square and the pieces as rectangles.
It is possible to employ lossless scaling of the plate and the pieces to, without changing the optimal value, transform the original plate of any problem instance into a square.
One downside is an increase in the absolute dimensions (i.e., precision).}, and a polynomial-time \(3/2 + \varepsilon\)-approximation for the rotation case, improved to a \(4/3 + \varepsilon\)-approximation if all piece profits are set to one.
These are the best results for the respective variants as far as the author knows.
For a survey on approximation algorithms for 2D cutting, the author refers to \citet{christensen:2017} and to \citet{iori:2020} (which briefly updates the former).

\subsection{Restricted \newtext{cuts}}

Considering only restricted cuts allows a simplified branching model: instead of having to consider a pseudo-polynomial number of horizontal and vertical cuts over each plate, only \(2n\) possibilities need to be considered, two for each piece, the one with the horizontal cut first, and the one with the vertical cut first (both generate the respective piece and up to two residual plates).
This exact branching model is employed by~\citet{silva:2010}.
For the unconstrained variant, \citet{song:2010}, proves a worst-case ratio of at least \(6/7\) between restricted and unrestricted optimal solutions for the same instance.
In contrast, \citet{furini:2016}, for the constrained variant, puts the ratio at most \(5/6\).
However, at least for the considered literature datasets, most instances have restricted and unrestricted optimal solutions of the same value.
The heuristic for the unconstrained variant described by \citet{song:2010} (which only employs restricted cuts) finds the unrestricted optimal value in ``94.84\%, 86.67\% and 77.83\% for small, medium and large sized unweighted instances'' and ``99.67\%, 99.50\% and 97.00\% for small, medium and large sized weighted instances''.
\citet{furini:2016} found that 47 of 50 instances, solved optimally by both restricted and unrestricted MILP models, shared the optimal solution value.

\subsection{Limited number of guillotine stages}

Unrestricted cuts are unnecessary to obtain any optimal solution of a two-staged variant (exact or non-exact), so the distinction between restricted and unrestricted two-staged variants do not exist.
For \(k\)-staged variants in which~\(k \geq 3\) there is such distinction, e.g., \citet{puchinger:2007} first models a formulation for the restricted three-staged G2CSP to then extend it to the unrestricted three-staged G2CSP.
For the unconstrained variant, and a small dataset, \citet{beasley:1985:guillotine} reported an average of about 0.4\% difference between the two-staged optimal value and the unlimited stages optimal value (about the same between two and three-staged, because three-staged is only 0.02\% behind unlimited stages).
For the constrained variant, however, \citet{martin:2020:models} presents an average difference of 3.6\% between two-staged and unlimited stages optimal solution values.
None of these papers poses this difference as a research question; these percentages are the byproduct of data gathered to answer other questions.
For the Guillotine 2D Bin Packing Problem, there is a theoretical result stating a two-staged optimal solution value may be at most 1.691 times worse than a solution with unlimited stages, but this is not a tight bound \citep{bansal:2005}.

\subsection{Exact but not pure MILP}

Besides solving methods with no relation to MILP models, this section also includes techniques that use MILP models but interleave their use with indispensable calls to non-MILP methods.
This section concentrates on works tackling the G2KP, and it is noted if another problem is considered instead.
The exact procedures observed here often fall into three categories: (i) branch-and-bound (or, as reported in the seminal works, \emph{tree search}); (ii) graph-based algorithms; (iii) repeated piece subset selections and packing tentatives.
The first category (i.e., B\&B) may be divided into top-down and bottom-up approaches.
Top-down approaches start from the original plate and branch on cuts over it and its subdivisions; this approach is probably the oldest one and is used in~\citet{cw:1977} and in~\citet{nicos:1995:ssr}.
Bottom-up approaches start from the pieces and combine them into builds, and the builds with each other, while they are smaller than the original plate.
This approach has many examples: \citet{bagchi:1993}, \citet{hifi:1997}, \citet{cung:2000}, and \citet{yoon:2013}.
The graph-based approaches include \citet{morabito:1996} and \citet{clautiaux:2018} (designed for four-staged but tested on unlimited stages, too); many previous papers by Clautiaux use graph representations for the G2OPP.

Finally, there are methods (often aided by MILP solvers) that solve a problem to select the most profitable subset of pieces, and then check if the subset can be guillotine-packed.
\citet{dolatabadi:2012} uses this approach, as did \citet{pisinger:2007}, but the latter focus on the G2CSP and the G2KP is solved just as a subproblem.
\citet{russo:2020} consider the methods of both \citet{dolatabadi:2012} and \citet{yoon:2013} to be state of the art, but the same work also points out that both methods have bounding flaws that may lead to incorrect results.

\newtext{Comprehensive counterexamples for the three bounding flaws found are given in the appendix of \citet{russo:2020}.
A brief overview of the roots of these three flaws follows.
The \emph{first two flaws} one in the antiredundancy strategy D1 of~\citet{cung:2000} and another in the unnamed strategy from~\citet{dolatabadi:2012} are, in essence, manifestations of the same oversight.
The oversight consists of restricting the search for an optimal solution to combinations of optimal solutions of subproblems (which are the same as the original problem but with the original plate dimensions reduced).
Such strategy is the basis of all dynamic programming and works flawlessly for the \emph{unconstrained} G2KP.
However, in the constrained case, a suboptimal solution to a subproblem may be necessary to assemble the optimal solution to the original problem.
A suboptimal solution for a subproblem may be necessary because, if an optimal solution for the subproblem were used instead, the combined solution would end up with more copies of some piece type than allowed by the piece demand, immediately becoming an invalid solution.
Removing pieces of the combined solution until the number of copies respects the demand does not (always) solve the problem; it is possible that the combination of two suboptimal solutions for the same subproblems would have a better value than this partially dismantled combination of optimal solutions.
The \emph{third flaw} was found in the works of~\citet{kang:2003,kang:2011,yoon:2013} and consists on a rounding problem.
Two formulae expressing upper bounds on the optimal solution value (i.e., optimal profit) have each two terms rounded down before they are summed.
\citet{russo:2020} presents a counterexample in which both \emph{upper} bounds give a result one unit \emph{lower} than the optimal solution value.
The bounds are valid if the rounding down only happens after the sum and not before it.
The method of~\citet{yoon:2013} also carries the flaw from~\citet{cung:2000}, which has affected other bottom-up methods in the literature too (see~\citet{russo:2020} for a complete overview).
%\emph{The first mistake} comes from a strategy employed by~\citet{dolatabadi:2012} and consists of only taking into account the best solution of a subproblem (the subproblem is the same problem but with a smaller original plate) when assembling the optimal solution for the main problem. This is a classic DP strategy and it works flawlessly for the unconstrained version of the problem. However, when applied to the constrained problem, it voids the guarantee of optimality because a suboptimal solution of a subproblem to be required, i.e., the optimal pattern of the subproblem cannot be repeated as many times as necessary in the main problem because of the demand constraints.
%\emph{The second mistake} comes from the antiredundancy strategy D1 from~\citet{cung:2000} and, again, it is related to discarding suboptimal subproblem solutions (and the subproblem is the problem with smaller original plate dimensions). The D1 strategy proposes that, when two subproblem solutions are combined, the result should be discarded if the dimensions of the merged solution clearly allow for an extra piece to be packed together with the pieces of both smaller solutions. The rationale for this strategy is that, as piece profits are always positive, the \emph{merged solution plus an extra piece that fits} will always have a better value for the same dimensions in comparison to the merge without the extra piece. However, again, the optimal pattern for the original problem may need the 
}
\oldtext{\citet{yoon:2013} flaw comes since \citet{cung:2000} and has affected other bottom-up methods too.}
Consequently, there is no clear definition of which is the best method currently.

\section{MILP formulations for the G2KP}

\citet{russo:2020}~identify three strategies employed by previous exact methods which cause loss of the optimality guarantee\newtext{, i.e., these methods cannot be considered exact anymore}.
No previous MILP formulation, nor this work, employs any of these strategies.
One of these strategies is a dominance rule valid for the unconstrained case but not for the constrained one.
Interestingly, \citet{herz:1972}~proposed a dominance rule for the unconstrained G2KP based on the same principle and warned about the possibility of misusing the rule in the constrained case.

The first MILP formulation dealing with guillotine cuts and unlimited stages was proposed by~\citet{messaoud:2008}.
The problem considered by~\citet{messaoud:2008} is the Strip Packing Problem\footnote{The Strip Packing Problem is a two-dimensional cutting/packing problem in which the pieces do not have profit values, and the original plate does not have a predefined length (`height' in the context of the problem); the objective is to minimize the height of the original plate while packing every piece.}, but adapting the formulation to the knapsack variant would not change its fundamentals.
Previously, \citet{lodi:2003}~had proposed two MILP formulations for two-staged G2KP.
As noted by~\citet{belov_thesis:2003}, modeling \(k\)-staged cuts for \(k \geq 3\) (unlimited stages included) was considered difficult at the time.
The size of most \(k\)-staged formulations is exponential on the number of stages (i.e.,~\(k\)).
The formulation of~\citet{messaoud:2008} had about \(3n^4/4\) variables and \(2n^4\) constraints (where \(n\) is the number of pieces); it also employed, according to the authors, a ``very loose linear relaxation'' due to which ``the practical interest of this formulation is still limited''.
The characterization of guillotine cuts proposed by~\citet{messaoud:2008} seems to have been simultaneously proposed by~\citet{pisinger:2007}. % This is the "Using decomposition" paper.

The first MILP formulation specifically for the G2KP was proposed by~\citet{furini:2016}.
\newtext{The formulation is classified as an extension of the one-cut formulation from~\citet{dyckhoff:1981} for the one-dimensional Cutting Stock Problem.
However, the formulation from~\citet{silva:2010} can be seen as an intermediary step between these two: it had already extended the one-cut formulation for two dimensions in a similar fashion but did not alter the problem from the cutting stock and was limited to two stages and restricted three stages.
} An extended version of~\citet{furini:2016} appears in~\citet{dimitri_thesis} (a PhD thesis)\newtext{, and a prelude to it in~\citet{furini:conference:2016}}.
Their formulation has pseudo-polynomial size, \(O((L + W) \times L \times W)\)~variables and \(O(L \times W)\) constraints, and its relaxation provides a stronger bound than~\citet{messaoud:2008}.
It was the first formulation able to solve medium-sized instances of the literature.
Besides the formulation, \citet{furini:2016}~proposes two reductions and one pricing procedure; these three are reimplemented in this work.
They also present and prove a theorem to assure the correctness of one of their reductions~(\emph{Cut-Position}).
A similar theorem and proof appear in~\citet{song:2010} but for the unconstrained variant.

In this work, the author proposes an enhanced formulation based on the formulation from~\citet{furini:2016} mentioned above.
A significant advantage of the enhancement is to avoid the enumeration of any cuts after the middle of a plate.
This advantage appears in many works since~\citet{herz:1972}.
Recently, \citet{delorme:2019} \oldtext{adapted}\newtext{enhanced} a \newtext{pseudo-polynomial} formulation for the 1D Cutting Stock Problem to obtain this same advantage.
However, the way \citet{delorme:2019}~changes their formulation to obtain this advantage is different from the approach taken here.
\newtext{The mechanisms involved in each approach are distinct.
\citet{delorme:2019} mechanism adds only a single reflecting dummy node per bin stock size to compensate for the removed arcs/variables, but it is meant for the 1D variant of the problem.
The mechanism proposed here adds a set of extraction variables in which the cardinality depends on the geometric properties of the specific instance considered, but it is meant for the 2D variant and also cuts down some symmetries that only exist in the 2D variant (i.e., are not present in the 1D variant).}

The most recent MILP formulations for the G2KP come from the following three works by Martin et alii:~\citet{martin:2020:models,martin:2020:bottom,martin:2020:top}.\oldtext{For the sake of conciseness, in this paragraph, we will refer to these three works as the \emph{Martin's works}, also we will refer to the formulation of~\citet{furini:2016} as the \emph{FMT formulation}.}\oldtext{The formulations in the Martin's works are compared against the FMT formulation.}\oldtext{We base our enhanced formulation on the FMT formulation and also compare against it.}\oldtext{The formulations in the Martin's works have a looser relaxation bound compared to the FMT formulation, but perform better than the FMT formulation in instances for which the FMT formulation has a much larger number of variables.}\oldtext{Considering the instances used in~\citet{furini:2016}, our enhanced formulation dominates the FMT formulation.}\oldtext{Our formulation also dramatically improves the running times of instances in which the FMT formulation performed worse than the ones from Martin's works (e.g., the gcut1--gcut12 instances).}\oldtext{Consequently, while it may be interesting for completeness sake, we do not compare against the formulations proposed in Martin's works in the current work.} \newtext{Besides the formulations from these three works, a previous formulation from \citet{martin:2019} targets the G2KP with defects (i.e., the original plate has regions that cannot be part of any piece).}
\newtext{A direct comparison between formulations with and without defects is unfair; modelling the defects burdens a formulation greatly.}
\newtext{Moreover, most formulations cannot be straightforwardly adapted to the G2KP with defects, including the one proposed in this work.}
\newtext{However, the results may be interpreted not as a direct comparison, but as an assessment of the impact of modelling defects over a formulation.}
\newtext{These four formulations are discussed in details in~\cref{sec:martin_chapter} in which they participate in experiments.}

% We discuss the related works in the topic of cut position discretisation in the following section.
% The topic demands more notation and connects with the reduction we adapt from the previous literature for our enhanced formulation.

% TODO: Should all words in the title be capitalized?
% TODO: Is this title ok? maybe Normalizing Plate Size?
\chapter{Technical background and the proposed formulation}
\label{sec:tech_background}

This chapter introduces all the technical details necessary to contextualize the proposed formulation and the formulation itself.
Outside of this chapter, the proposed formulation (also referred to as the enhanced formulation) will also be referred to as BBA (Becker, Buriol, and Araújo) as to follow the same naming convention applied to FMT (Furini, Malaguti, and Thomopulos) from~\citet{furini:2016}.
\Cref{sec:psn} introduces notation for the chosen discretisation, discusses some alternative discretisations, outlines the cut-and-plate enumeration procedure, and dives deeply into the known (but innovatively employed) plate-size normalisation.
\Cref{sec:furini_model} summarises the formulation proposed in~\citet{furini:2016}, which is the basis for the formulation proposed by this thesis and gives it an intuitive interpretation.
\Cref{sec:enhanced_model} describes the proposed formulation, how it differs from its basis, and intuition for why the changes (generally) lead to a performance improvement.
\Cref{sec:proof_of_correctness} presents a proof for the claim that the proposed formulation can find a cutting pattern for any multiset of pieces for which the original formulation also finds a cutting pattern, this is, that the guarantee of optimality is retained.
\Cref{sec:adaptation_for_rotation} shows how both discussed formulations can be adapted to allow piece rotation and \cref{sec:mirror_plate} shows a reduction that is specific to those formulations with rotation enabled.
Finally, \cref{sec:pricing} presents a summary of the pricing procedure employed in~\citet{furini:2016} and reimplemented by the author for the experiments of this thesis.

\section{Notation, Discretisation, and Plate-Size Normalisation}
\label{sec:psn}

The performance of solving methods for cutting and packing problems often heavily depends on the number of cut/packing positions considered.
Since the seminal works of~\citet{cw:1977} and~\citet{herz:1972}, solving methods avoid considering each possible position but instead consider only a subset necessary to guarantee optimality.
The literature includes many such subsets, which are often referred to as \emph{discretisations}.
The most common way of computing these discretisations is Dynamic Programming (DP) algorithms.
These DP algorithms usually only take a small fraction of the running time, but the size of the position subset outputted by them strongly affects the time spent by the rest of the solving method.

Both the proposed formulation and its basis have one constraint for each attainable distinctly-sized plate and one variable for each potential cut over each of these plates.
Therefore, eliminating a single cutting position has the following effects:
\textbf{(i)} it removes one variable for each distinctly-sized plate that allowed that cutting position;
\textbf{(ii)} if that cutting position was the only way to produce some distinctly-sized plates\footnote{Note that the same cutting position, when applied to distinctly-sized plates, may generate different children.}, then it also removes the constraints associated with these plates;
\textbf{(iii)} if (ii) excludes one or more constraints/plates, then it also excludes all variables representing possible cuts over the excluded plates;
\textbf{(iv)} finally, if (iii) eliminates one or more variables/cuts, then it may trigger (ii) again (i.e., other plates stop being attainable), cyclically.

In this work, the only cut subset (discretisation) considered are the canonical dissections of~\citet{herz:1972}, hereafter referred to as \emph{normal cuts} instead.
The author acknowledges the existence of stricter discretizations: the raster points of~\citet{terno:1987,guntram:1966}, the regular normal patterns of~\citet{boschetti:2002} (named this way by~\citet{cote:2018}), and the Meet-in-the-Middle (MiM) of~\citet{cote:2018}.
The reasons for the author's choice of discretisation are numerous: it works well with the \emph{Plate-Size Normalisation} procedure described in the sequence; it is the same discretisation employed by Furini's formulation (from which the proposed formulation is based on);\oldtext{MiM main gain}\newtext{The main gain of MiM} is reducing the number of cut positions after the middle of a plate, which the enhanced formulation already discards anyway; the regular normal patterns compute a distinct subset-sum for each pair of plate and piece, which the author considers excessive (there may exist hundreds of thousands of intermediary plate possibilities); finally, the raster points complicate the proofs presented here and the \emph{Plate-Size Normalisation} weakens its benefits.

\oldtext{The set~\(O = \{v, h\}\) denotes the cut orientation: \(v\) is vertical (parallel to width, perpendicular to length); \(h\) is horizontal (parallel to length, perpedicular to width).}
\newtext{The set~\(O = \{h, v\}\) denotes the cut orientation: \(h\) is horizontal (parallel to width, perpendicular to length); \(v\) is vertical (parallel to length, perpedicular to width).}
Let us recall that the demand of a piece~\(i \in \bar{J}\) is denoted by~\(u_i\).
By defining the set of pieces fitting a plate~\(j\) as~\(I_j = \{i \in \bar{J} : l_i \leq l_j \land w_i \leq w_j \}\), the set \(N_{jo}\) (i.e., the set of the normal cuts of orientation~\(o\) over plate~\(j\)) can be defined as:

{\iffinalversion\else\color{blue}\fi
\begin{equation}
N_{jo}= \left\{
\begin{array}{lllr}
  \{q: 0 < q < l_j; & \exists n_i \in [0 \isep u_i], \forall i \in I_j, q = \sum_{i\in I_j} n_i l_i \} & \quad \text{if } o = h,\\
  \{q: 0 < q < w_j; & \exists n_i \in [0 \isep u_i], \forall i \in I_j, q = \sum_{i\in I_j} n_i w_i \} & \quad \text{if } o = v.
\end{array}\right.
\end{equation}
}

The sets defined above never include cuts at the plate extremities (i.e., \(0\), \(l_j\) for \oldtext{\(N_{j_v}\)}\newtext{\(N_{jh}\)}, and \(w_j\) for \oldtext{\(N_{jh}\)}\newtext{\(N_{jv}\)}).
Any of these cuts will always create (i)~a~zero-area plate and (ii)~a~copy of the plate that is being cut.
Consequently, these cuts only add symmetries and may be disregarded.

\newtext{
The set \(J\) can now be defined by the following procedure: the original plate (plate~\(0\)) is added to \(J\), then for every plate~\(j \in J\) every cut in~\(N_{jv} \cup N_{jh}\) is applied to~\(j\), and each child generated is added to \(J\) if it can fit at least one piece.
The process finishes when every plate in~\(J\) was considered for cutting, and no new plates were generated.
Such procedure guarantees each piece~\(i \in \bar{J}\) will always be present in~\(J\) unless the piece does not fit the original plate (in which case it is irrelevant to the problem and could be removed a priori).
}

The goal of the \emph{Plate-Size Normalisation} procedure propose here is to reduce the number of distinctly-sized plates considered.
Fewer distinctly-sized plates mean fewer constraints and trigger the same cascading effect described by items (ii)--(iv) above.
The property exploited by the procedure is already known and \oldtext{similarly} exploited by~\citet{alvarez:2009} and by~\citet{dolatabadi:2012}.
The author chose to state the property the following way:

\begin{proposition}
\label{pro:normalisation}
% Without loss of optimality, plate~\(j\) may always be replaced by plate~\(j\prime\) with \(w_{j\prime} = w_j\) but \(l_{j\prime} = max\{q : q \in N_{kv}, q \leq l_j\}\) in which \(w_k = w_j\) but \(l_k > l_j\).
Given a plate~\(j \in J\), \(l_j\) may always be replaced by \(l^\prime_j = max\{q : q \in N_{kh}, q \leq l_j\}\) in which \(k \in J\), \(w_k = w_j\), but \(l_k > l_j\), without loss of optimality.
The analogue is valid for the width.
\end{proposition}

\newtext{In other words, if increasing the length (width) of plate~\(j\) reveals that the original length (width) did not match a normal cut position in the enlarged plate, then plate~\(j\) may be replaced by a shorter plate in which the length (width) is reduced to the largest normal cut position smaller than the original length (width). For example, given \(l = [5, 7]\), \(w = [3, 2]\), a 13x3 plate may be reduced to 12x3 (13 does not match a normal cut while \(5 + 7 = 12\) does), and a 13x2 plate may be reduced to 7x2 (13 does not match a normal cut while 7 does).}
No proof is replicated here. The following can now be defined:

\begin{definition}{Size-normalised plate}
The length of a plate~\(j\) is considered normalised if, and only if, \(l_j = l^\prime_j\).
The analogue is valid for the width.
The size of a plate is normalised if, and only if, both its length and its width are normalised.
\end{definition}

The \emph{Plate-Size Normalisation} procedure proposed here consists only of replacing every non-size-normalised plate enumerated by its normalised counterpart.
\newtext{In summary, every intermediary plate \emph{for which the length (width) is not a linear combination of the length (width) of the pieces} have its dimensions reduced to the closest linear combination smaller than itself; this combines multiple plate types that could only pack the same set of pieces into a single plate type.
}\oldtext{The number of distinctly-sized plates diminishes because the procedure replaces many plates of distinct but similar dimensions by a single plate.
}The only extra effort added by \emph{Plate-Size Normalisation} consists of binary searches over~\(N_{jo}\) sets for each plate~\(j\)\newtext{, and these may be carried out without increasing the overall complexity, given the setup of~\(O(LW)\) vectors of size \(O(L + W)\); a setup step which also does not increase the overall complexity.
However, in the implementation, the author opted to increase the overall complexity from \(O(L^2W + LW^2)\) to \(O(L^2Wlog(L) + LW^2log(W))\) because the fraction of time spent on the enumeration was not enough to justify the memory and code complexity trade-off.
In practice, even if the worst-case complexity increases, the time spent decreases because the actual number of plates (denoted in the complexity by~\(O(LW)\) becomes more distant from the worst-case}.
A suitable \(N_{ko}\) set for each plate~\(j\) was already computed by the plate enumeration procedure before introducing the \emph{Plate-Size Normalisation} (no extra effort required).

\begin{remark}
\oldtext{If a normal cut~\(q\) divides the size-normalised plate~\(j\), the first child is always size normalised, but the second child may not be size normalised.}
\newtext{If a normal cut divides a size-normalised plate, then the dimension perpendicular to the cut, in the first child, is normalised. The dimension parallel to the cut in the first child, and both dimensions of the second child, are not guaranteed to be normalised.}
\end{remark}

% The concept of normal cuts is introduced by
% Every optimal solution with non-normal cuts can be mapped to an optimal solution only using normal cuts.
% The definition used here is almost the same of~\citet{furini:2016}, as we are trying to keep our notation compatible with theirs.

% TODO: the definitions below need the definition of orientation.
% TODO: define the sizes set `S_{oj}` where? "given vertical cuts are parallel to length and horizontal cuts parallel to width, we define the size set ..."
% TODO: define Q_{jo} here, like in furini:2016, maybe change paragraph above to say we will be using the same definition of furini:2016

% is the set of pieces that fit plate~\(j\).
%\(O = \{v, h\}\) defines both vertical and horizontal orientations.
%Considering that vertical is parallel to length, and horizontal is parallel to width, we can use them to index plate sizes as in~\(S_{jv} = l_j\) and \(S_{jh} = w_j\).
%Allowing us to define normal cuts as:

%\begin{definition}
%\(Q_{jo} = \{ 0 < q < S_{oj}; \forall i \in I_j, \exists n_i \in \mathbb{N}, n_i \leq u_i, q = \sum_{i\in I_j n_i S_{oi}}\}\)
%\end{definition}

% Let us denote the \emph{original plate} as plate~\(0\).
% Every cut c in Q_{0o} define a plate with a side of size S_{0o} and another of size S_{co}

% The \emph{plate-size normalisation} works by replacing groups of plates (with similar but distinct size) by a single plate.
% If there exists a plate~\(j : S_{oj} > max\{N_{oj}\}\) then \(j\) may be replaced by a plate~\(k : S_{ok} = max\{N_{oj}\}\).

%In this section, we prove that no valid solution is lost if plate dimensions are shortened to a discretisation point.
%For convenience, we denote the set of pieces that fit a plate~\(j\) by \(\bar{J}_j\); a piece~\(i\) fits a plate~\(j\) iff \(l_i \leq L_j \land w_i \leq W_j\).%, and that plates cut from a shortened plate may need additional shortening after the cut.

%\begin{definition}
%The set of pieces that fit a plate~\(j\) is denoted by \(\bar{J}_j\), a piece~\(i\) fit a plate~\(j\) if \(l_i \leq L_j \land w_i \leq W_j\).
%\end{definition}

%\begin{definition}
%The set of horizontal normal cuts of a plate~\(j\) is the set of all non-trivial linear combinations \(\sum_{i \in \bar{J}_j} a_i \times l_i \leq L\) for which the coefficients \(a_i\) are restricted by \(0 \leq a_i \leq u_i~\forall.~i \in \bar{J}_j\). The analogue is valid for vertical cuts.
%\end{definition}

%In~\citet{cw:1977}, \emph{normal cuts} are defined for the variant of the problem without the demand constraints.
%Our definition of normal cuts is extended to take into consideration the demand.
%The following theorem and its proof (provided in \cref{app:proof_only_normal_cuts_needed}) are also extensions to account for the demand.
%We also extend a theorem, and its proof, that restricting the cuts to (our definition of) normal cuts allows packing any demand-abiding piece mulstiset that could be packed with non-normal cuts.
%We also extend a theorem, and its proof, that restricting the cuts to (our definition of) normal cuts allows packing any demand-abiding piece mulstiset that could be packed with non-normal cuts.

%\begin{theorem}\label{only_normal_cuts_needed}
%For every guillotine packing with non-normal cuts packing a set of pieces~\(s\), there is a guillotine packing of only normal cuts packing the largest subset of~\(s\) that respects the demand.
%producing the same pieces or, if the former packing produced an amount exceeding the demand for some piece type, the latter paproduces the maximum amount allowed by the demand for such piece types.
%\end{theorem}

%is in the appendix, as similar proofs are presented in the literature.

%\begin{corollary}\label{co:size_normalised_plate}
%Given a plate~\(j\) in which the right (or top) border do not overlap with the rightmost vertical (or topmost horizontal) normal cut, any demand-abiding piece multiset packed in~\(j\) can be packed in its size-normalised alternative, in which the plate size is reduced until the borders overlap with normal cuts.
%\end{corollary}

%\begin{proof}
%The proof of the~\cref{only_normal_cuts_needed} describes an alternative packing with the property that any space between the rightmost vertical cut and the plate right border (topmost horizontal cut and the plate top border) is waste. Therefore, if a plate is replaced by its size-normalised alternative, no used space would be lost, and no packing would be invalid.\qed
%\end{proof}

%\begin{remark}
%If a size-normalised plate is cut by a normal cut, the first child is also size-normalised. The second child, however, may or may not be size-normalised.
%\end{remark}

%In the dimension parallel to the cut, the border of the first child will overlap with the normal cut applyed to the parent plate; on the other dimension, the border already overlapped a normal cut.

\begin{example}{Denormalisation after normalised cut}\label{ex:renormalisation_after_cut}
\oldtext{Given two pieces with \(l = [5, 7]\), \(u = [2, 3]\), and a size-normalised plate of length~\(21\), a normal cut at~\(12\) creates a non-normalised second child of length~ \(9\).}
\newtext{Consider three pieces with~\(l = [5, 7, 9]\), \(w = [6, 4, 11]\), \(u = [3, 1, 1]\), and a plate of dimensions \(15\)x\(15\). The plate dimensions are already normalised. The plate length matches stacking the three copies of the first piece. The plate width matches the other two pieces lying side-by-side. An horizontal cut at length~\(7\) is a normal cut because it matches the length of the second piece. If the cut is done, the width of both children is not normalised anymore, nor is the length of the second child. The width of both children is not normalised because the third piece does not fit either child so, for both children, the largest width a valid packing may reach is~\(12\). The length of the second child is not normalised because the largest length a valid packing inside the second child may reach is 7. The dimensions of both children may be normalised to \(7\)x\(12\). This example already shows an immediate gain: instead of creating two new plate sizes, the enumeration only creates a single new plate type. The cut creates two copies of this single type of plate.}
\end{example}

\begin{figure}[h]
  \caption{\newtext{Diagram of Example~\ref{ex:renormalisation_after_cut}}}
  \center
  \input{diagrams/renormalization_after_cut.tex}
  \legend{\justifying \newtext{Notes about the diagrams: (a) the three copies of the first piece stacked; (b) the second and third pieces side-by-side; (c) both children of an horizontal normal cut over a normalised plate are not normalised themselves.}}
  \label{fig:renormalisation_after_cut}
\end{figure}

\newtext{
While the basic property (\cref{pro:normalisation}) is well known by the literature, some of the developments given here are not.
Generally, in the literature, the procedure is employed a single time just for the original plate, and \emph{not} for every plate generated during an enumeration procedure like it is done here.
Because the procedure is commonly applied once, the denormalisation effect mentioned here is not studied or discussed.
Also, applying the procedure a single time often is done to get a better relaxation, which is not the case for the proposed formulation.
In the proposed formulation and its basis, the dimensions are abstracted by a flow graph connecting cuts and plates; the dimensions are not right-hand values of binding constraints.
The plate-size normalisation does not affect the relaxation; instead, it reduces the model size because multiple intermediary plate types are conflated into a single type.
}

%%%%%%%%%%%%%%%%%%%%% SECTION BREAK %%%%%%%%%%%%%%%%%%%%%

\section{\newtext{The FMT formulation and associated reductions}}
\label{sec:furini_model}

Given pseudo-polynomial time and space, an instance of the G2KP can be transformed into a bipartite directed acyclic (multi)graph; solving a flow-like problem over such graph is equivalent to solving the original G2KP instance.
The two disjoint and independent sets of vertices are (i) the enumerated plate types and (ii) the enumerated cuts over the plate types.
Each cut vertice has one incoming edge and one or two outgoing edges.
The head of the incoming edge is the plate vertice that represents the plate being cut.
The tail of each outgoing edge is a plate vertice representing a plate produced by the cut.
These are all edges that exist in the graph.
If the cut vertice has only two incident edges, it represents a trim cut, i.e., a cut that only reduces the size of an existing plate without producing a second plate.
If the cut vertice has three incident edges, it represents a plate cut into two smaller plates.
As the graph is a multigraph, it allows for parallel edges, representing a cut exactly at the middle of a plate generating two copies of the same plate type.

The aforementioned flow-like problem is as follows.
All edges only allow integer amounts to flow between vertices.
The vertice representing the original plate type is the only one to start with one flow unit (all other vertices start zeroed).
If a plate vertice receives any flow amount, it can keep any portion of the flow in the vertice and freely redistribute the remaining flow among its outgoing edges.
If a cut vertice receives any flow amount, it \emph{multiplies} the amount of flow received by the number of outgoing edges, and \emph{must} relay the exact amount of flow received to \emph{each} of the outgoing edges, e.g., if a cut vertice receives two units of flow then each outgoing edge receives two units of flow.
If a plate vertice represents a plate type of the same dimensions as a piece~\(i\), then each unit of flow kept by the vertice generates a profit~\(p_i\) constrained to a maximum of~\(u_i \times p_i\).
The problem is deciding how the plate vertices will distribute the flow they receive to maximize said profit.

The formulation proposed in~\citet{furini:2016}, henceforth referred to as the FMT formulation, generates models similar to the graph described and which are solved similarly to the flow-like problem mentioned.
As previously defined, the set \(O = \{h, v\}\) denotes the horizontal and vertical cut orientations.
The set \(Q_{jo}\) (\(\forall j \in J, o \in O\)) denotes the set of possible cuts (or cut positions) of orientation~\(o\) over plate~\(j\).
\newtext{The set \(Q_{jo}\) is not formally defined because it is a subset of~\(N_{jo}\) (formally defined in the last section) that varies based on which reductions are being applied.}

The parameter~\(a\) is a byproduct of the plate enumeration process and represents the edges of the graph.
\oldtext{The value of \(a^o_{qkj}\) is the number of plates \(j \in J\) added to the stock if a cut of orientation~\(o \in O\) is carried out at position~\(q \in Q_{jo}\) of a plate~\(k \in J\).}
\newtext{The value of~\(a^o_{qkj}\) indicates how many copies of a plate~\(j \in J\) are produced by cutting a plate~\(k \in J\) with a cut of orientation~\(o \in O\) at position~\(q \in Q_{ko}\).}
This value may be zero (no plate created by the cut has the same dimensions as \(j\), i.e., no edge exists), one (one plate created by the cut has the same dimensions as \(j\), i.e., there is an edge), or two (the parent plate was cut in half, and both halves have the same dimensions as \(j\), i.e., two parallel edges).
This parameter is needed to write the constraints that control which plates are available.
The description of this parameter in~\citet{furini:2016} has a typo, as pointed out by~\citet{martin:2020}:``[...] there is a typo in their definition of parameter~\(a^o_{qkj}\), as the indices~\(j\) and~\(k\) seem to be exchanged.''.
The original parameter description also forgets the possibility that it may have value two (instead of just zero and one).

In a valid solution, the value of \(x^o_{qj}\) is the number of times a plate~\(j \in J\) is cut with orientation~\(o \in O\) at position~\(q \in Q_{jo}\); i.e., how much flow is being transported by each edge coming from a plate vertice.
The plate~\(0 \in J\) is the original plate, and it may also be in~\(\bar{J}\), as there may exist a piece of the same size as the original plate.
The \(y_i\) variable denotes the number of times a plate~\(i\) was sold as the piece~\(i\) (as \(\bar{J} \subseteq J\), each index~\(i \in \bar{J}\) denote both a piece and the plate of the exact same dimensions).

% Formulation version without the \specialcell workaround below.
% The problem with the formulation formatting is that the first restriction is
% too long before the quantifiers, and the penultimate restriction is too
% long in the quantifiers, so making them two distinct columns with align
% breaks the layout (and send the equation reference numbers of each line
% to an empty line below the). The workaround is some dark magic that flushes
% the quantifiers to right, instead of giving them their own shared column.
%\begin{align}
%\mbox{max.} &\sum_{(i, j) \in E} p_i e_{ij} \label{eq:objfun}\\
%\mbox{s.t.} &\sum_{o \in O}\sum_{q \in Q_{jo}} x^o_{qj} + \sum_{i \in E_{*j}} e_{ij} \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qkj} x^o_{qk} \hspace*{0.05\textwidth} & \forall j \in J, j \neq 0,\label{eq:plates_conservation}\\
%	& \sum_{o \in O}\sum_{q \in Q_{0o}} x^o_{q0} + \sum_{i \in E_{*0}} e_{i0} \leq 1 &,\label{eq:just_one_original_plate}\\
%	& \sum_{j \in E_{i*}} e_{ij} \leq u_i & \forall i \in \bar{J},\label{eq:demand_limit}\\
%% TODO: fix equation below, the forall part is too long and clashes with the long equation in the first line
%	& x^o_{qj} \in \mathbb{N}^0 & \forall j \in J, o \in O, q \in Q_{jo},\label{eq:trivial_x}\\
%	& e_{ij} \in \mathbb{N}^0 & \forall (i, j) \in E.\label{eq:trivial_e}
%\end{align}

\begin{align}
\bm{max.} &\sum_{i \in \bar{J}} p_i y_i \label{eq:FMT_objfun}\\
\bm{s.t.} &\specialcell{\sum_{o \in O}\sum_{q \in Q_{jo}} x^o_{qj} \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qkj} x^o_{qk} \hspace*{0.05\textwidth} \forall j \in J\setminus\bar{J}, j \neq 0,}\label{eq:FMT_plates_conservation}\\
            &\specialcell{y_i + \sum_{o \in O}\sum_{q \in Q_{io}} x^o_{qi} \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qki} x^o_{qk} \hspace*{0.05\textwidth} \forall i \in \bar{J}, i \neq 0,}\label{eq:FMT_plates_conservation_and_sale}\\
	    &\specialcell{\sum_{i \in \{i | i \in \bar{J} \land w_i = w_0 \land l_i = l_0\}} y_i + \sum_{o \in O}\sum_{q \in Q_{0o}} x^o_{q0} \leq 1 \hspace*{\fill},}\label{eq:FMT_just_one_original_plate}\\
            & \specialcell{y_i \leq u_i \hspace*{\fill} \forall i \in \bar{J},}\label{eq:FMT_demand_limit}\\
	    & \specialcell{x^o_{qj} \in \mathbb{N}^0 \hspace*{\fill} \forall j \in J, o \in O, q \in Q_{jo},}\label{eq:FMT_trivial_x}\\
            & \specialcell{y_i \in \mathbb{N}^0 \hspace*{\fill} \forall i \in \bar{J}.}\label{eq:trivial_y}
\end{align}

The objective function maximizes the profit of the plates sold as pieces~\eqref{eq:FMT_objfun}.
Constraints~\eqref{eq:FMT_plates_conservation} and~\eqref{eq:FMT_plates_conservation_and_sale} guarantee that for every intermediary plate~\(j\) that was further cut (left-hand side), or plate/piece~\(i\) that was either sold or further cut (left-hand side), there must be a cut making available a copy of such plate (right-hand sides).
One copy of the original plate is available from the start~\eqref{eq:FMT_just_one_original_plate}, and it can be either sold as a piece of the same dimensions or further cut.
The amount of sold copies of some piece type must respect the demand for that piece type~\eqref{eq:FMT_demand_limit}.
Finally, the domain of all variables is the non-negative integers~\eqref{eq:FMT_trivial_x}-\eqref{eq:trivial_y}.

\newtext{
Besides the base formulation, \citet{furini:2016}~also employs a basic symmetry-breaking strategy and proposes two reductions for the model size.
The cut enumeration in~\citet{furini:2016} excludes one of each pair of perfectly symmetrical cuts.
Perfectly symmetrical cuts are pairs of cuts that create the same set of two child plates (i.e., just which is the first and which is the second that is reversed).
\citet{furini:2016} chose to ignore the symmetric cut in the second half of the plate during the enumeration.
The \emph{Cut-Position} reduction confines the set of cutting positions over some plates to the restricted set.
The condition for a plate to be affected by Cut-Position is that it should be able to pack \emph{at most} five pieces because, in such case, its restricted and unrestricted optimal value are the same (see \cref{fig:distinctions_restricted_unrestricted}).
The \emph{Redundant-Cut} reduction only removes a subset of the \emph{trim cuts}, i.e., cuts in which the second child plate is immediately considered waste because it is smaller than every piece.
Redundant-Cut removes a trim cut over plate~\(j\), which obtains plate~\(j^\prime\), if plate~\(j\) itself can \emph{only} be obtained by trim cuts of the same orientation from larger plates.
The rationale is that these larger plates can directly obtain~\(j^\prime\) through a single trim cut instead of using~\(j\) as an intermediary plate.
This way, the reduction removes unnecessary intermediaries and avoids the many symmetric patterns that only vary on the number of trim cuts employed to reduce a larger plate down to \(j^\prime\).
}
%%%%%%%%%%%%%%%%%%%%% SECTION BREAK %%%%%%%%%%%%%%%%%%%%%

\section{The proposed (re-)formulation}
\label{sec:enhanced_model}

\oldtext{EDIT NOTE: this section was called ``Changes to Furini's model'' before.}

The FMT is elegant: the pieces are just intermediary plates that may be sold.
The proposed changes affect both the enumeration step and the formulation mathematical description.
These changes significantly reduce the model size.
However, these changes also deepen the distinction between plates and pieces and may be regarded as sacrificing some elegance for performance.
The essentials of the formulation remain the same, but the proposed formulation has better performance in the vast majority of the problem instances.
For this reason, the author considers the formulation proposed here as an enhanced iteration of the FMT.
In general, the relaxation of the proposed model is the same as the FMT.

% TODO: should we say that this supersedes the furini original symm-breaking
% and their redundant-cut reduction?

As mentioned at the end of the last section, FMT allows for removing one of each pair of perfectly symmetrical cuts (which can be arbitrarily chosen to be the one in the second half of the plate).
Differently,~\citet{cw:1977} disregards \emph{all} cuts after the middle of the plate because of symmetry.
If FMT did the same as~\citet{cw:1977}, it could become impossible to trim a plate to the size of a piece.
For example, if there was a piece with a length larger than half the length of a plate, and such plate has no normal cut with the exact length of the needed trim, then the piece could not be extracted from the plate, even if the piece fits into the plate.
The goal of the proposed formulation is to reduce the number of cuts (i.e., model variables) by getting closer to the symmetry-breaking rule used in~\citet{cw:1977} without loss of optimality.
\newtext{Of course, disregarding every cut after the midplate also insulates against any perfectly symmetrical cuts and, therefore, supersedes a check just for this specific kind of symmetry.}
%First we present our changes to the formulation and the variable enumeration, then we prove the model correctness is not affected.

Considering the graph representation for FMT presented in the last section, it can be said that the proposed formulation throws away the possibility of piece-sized plate vertices to convert flow into profit and creates a third disjoint and independent set of vertices representing the pieces.
The vertices of this new set are all leaves/sinks responsible for converting flow to the corresponding piece profit.
The edges that reach the new vertices always come from plate vertices, but the plates do not need to have the exact dimensions of the pieces anymore.
These edges work as a shortcut to trimming, and multiple vertice plates may have an edge pointing to the same piece vertice as well as each vertice plate may have edges to multiple piece vertices.
This change alone leads to a larger model; however, as the edges to this new set of vertices allow us to obtain pieces without needing trim cuts, the symmetry-breaking strategy can be expanded to consider every cut after the middle of a plate.
This reduction leads to far fewer edges, as the cut position discretisation is often denser in the second half of the plate (compared to the first half).
It also means fewer plate vertices, as many plates were only necessary as intermediary steps to trimming a plate to the size of a piece.

%Often, there are many more normal cuts in the second half of a plate than there is in the first half. % need explanation?
%Also, if all cuts that generated some plate type are disregarded, then every cut over such plate type is also disregarded.
%Taking all of this into account, the main purpose of our revised version of Furini's formulation is to improve its symmetry breaking. % TODO: This has also the effect of superseding the Redundant-Cut reduction, which EXPLAIN SUCCINTLY THE REDUNDANT CUT.

\subsection{Changes to the formulation description}
\label{sec:enhanced}

The changes to the formulation are restricted to replacing the set of integer variables~\(y_i, i \in \bar{J},\) with a new set of variables~\(e_{ij}, (i, j) \in E, E \subseteq \bar{J} \times J\), and the necessary adaptations to accomodate this change.
In FMT, \(y_i\) denotes the number of times a plate~\(i\) was sold as the piece~\(i\) (plate~\(i\) has the exact same dimensions as piece~\(i\)).
The \emph{extraction variables}~\(e_{ij}\) denote a piece~\(i\) was extracted from plate~\(j\) which dimensions may only be the same or larger than the piece~\(i\) dimensions.
The exact definition of set~\(E\) is discussed over~\cref{sec:var_enum}; for the purpose of presenting the formulation, the intuitive definition of~\(e_{ij}\) just above is enough.
For convenience, the following are defined: \(E_{i*} = \{ j : \exists~(i, j) \in E \}\) and \(E_{*j} = \{i : \exists~(i, j) \in E \}\).
The variables \(x^o_{qj}\) and coefficients \(a^o_{qkj}\) have the same meaning as the FMT formulation (\cref{sec:furini_model}).

% Formulation version without the \specialcell workaround below.
% The problem with the formulation formatting is that the first restriction is
% too long before the quantifiers, and the penultimate restriction is too
% long in the quantifiers, so making them two distinct columns with align
% breaks the layout (and send the equation reference numbers of each line
% to an empty line below the). The workaround is some dark magic that flushes
% the quantifiers to right, instead of giving them their own shared column.
%\begin{align}
%\mbox{max.} &\sum_{(i, j) \in E} p_i e_{ij} \label{eq:objfun}\\
%\mbox{s.t.} &\sum_{o \in O}\sum_{q \in Q_{jo}} x^o_{qj} + \sum_{i \in E_{*j}} e_{ij} \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qkj} x^o_{qk} \hspace*{0.05\textwidth} & \forall j \in J, j \neq 0,\label{eq:plates_conservation}\\
%	& \sum_{o \in O}\sum_{q \in Q_{0o}} x^o_{q0} + \sum_{i \in E_{*0}} e_{i0} \leq 1 &,\label{eq:just_one_original_plate}\\
%	& \sum_{j \in E_{i*}} e_{ij} \leq u_i & \forall i \in \bar{J},\label{eq:demand_limit}\\
%% TODO: fix equation below, the forall part is too long and clashes with the long equation in the first line
%	& x^o_{qj} \in \mathbb{N}^0 & \forall j \in J, o \in O, q \in Q_{jo},\label{eq:trivial_x}\\
%	& e_{ij} \in \mathbb{N}^0 & \forall (i, j) \in E.\label{eq:trivial_e}
%\end{align}

\begin{align}
\bm{max.} &\sum_{(i, j) \in E} p_i e_{ij} \label{eq:objfun}\\
\bm{s.t.} &\specialcell{\sum_{o \in O}\sum_{q \in Q_{jo}} x^o_{qj} + \sum_{i \in E_{*j}} e_{ij} \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qkj} x^o_{qk} \hspace*{0.05\textwidth} \forall j \in J, j \neq 0,}\label{eq:plates_conservation}\\
%            & \specialcell{\sum_{o \in O}\sum_{q \in Q_{jo}} x^o_{qj} \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qkj} x^o_{qk} \hspace*{\fill} \forall j \in J\setminus\bar{J},}\label{eq:generic_plates_conservation}\\
	    & \specialcell{\sum_{o \in O}\sum_{q \in Q_{0o}} x^o_{q0} + \sum_{i \in E_{*0}} e_{i0} \leq 1 \hspace*{\fill},}\label{eq:just_one_original_plate}\\
            & \specialcell{\sum_{j \in E_{i*}} e_{ij} \leq u_i \hspace*{\fill} \forall i \in \bar{J},}\label{eq:demand_limit}\\
	    % TODO: fix equation below, the forall part is too long and clashes with the long equation in the first line
	    & \specialcell{x^o_{qj} \in \mathbb{N}^0 \hspace*{\fill} \forall j \in J, o \in O, q \in Q_{jo},}\label{eq:trivial_x}\\
            & \specialcell{e_{ij} \in \mathbb{N}^0 \hspace*{\fill} \forall (i, j) \in E.}\label{eq:trivial_e}
\end{align}

The objective function maximizes the profit of the extracted pieces~\eqref{eq:objfun}.
Constraint~\eqref{eq:plates_conservation} guarantees that for every plate~\(j\) that was further cut or had a piece extracted from it (left-hand side), there must be a cut making available a copy of such plate (right-hand side).
One copy of the original plate is available from the start~\eqref{eq:FMT_just_one_original_plate}, and it can be either have a piece directly extracted or be further cut.
The amount of extracted copies of some piece type must respect the demand for that piece type (a piece extracted is a piece sold)~\eqref{eq:demand_limit}.
Finally, the domain of all variables is the non-negative integers~\eqref{eq:trivial_x}-\eqref{eq:trivial_e}.

\subsection{Changes to the cut-and-plate enumeration}
\label{sec:var_enum}

As mentioned in~\cref{sec:furini_model}, the FMT was proposed together with some mechanisms to reduce the model size: removing perfectly symmetrical cuts, \emph{Cut-Position}, and \emph{Redundant-Cut}.
\oldtext{The two last rules are not discussed here; \citet{furini:2016}~proves their correctness, and they do not conflict with the enhanced model.}
\newtext{Cut-Position does not conflict with the proposed formulation and can be employed alongside it to further reduce the model size.
The symmetry-breaking and Redundant-Cut are superseded in the proposed formulation.
Redundant-Cut eliminates a subset of the trim cuts from the formulation; however, the proposed formulation does not have trim cuts like those removed by Redundant-Cut because the extraction variables make them unnecessary.
%If the cuts can only happen before the midplate, then either (i) there is a cut before the midplate for which the associated piece(s) would also fit into the second half, or (ii) there are no cuts before the midplate and any piece larger than half the plate dimension is obtained utilizing extraction variables.
}

The use of the \(x\)~variables does not change from the original formulation to the revised formulation -- however, the size of the enumerated set of variables changes.
The revised enumeration does not create any variable~\(x^o_{jq}\) in which \((o = h~\land~q > \lceil w_j / 2 \rceil) \lor (o = v~\land~q > \lceil l_j / 2 \rceil)\).
%given that \(D^h_j \equiv W_j\) and \(D^v_j \equiv L_j\).

The original formulation has variables~\(y_i\), \(i \in \bar{J}\), while the revised formulation replaces them with variables~\(e_{ij}\), \((i, j) \in E\), \(E \subseteq \bar{J} \times J\).
Set~\(\bar{J} \times J\) is orders of magnitude larger than~\(\bar{J}\).
Consequently, set~\(E\) must be a small subset to avoid having a revised model with more variables than the original.
A suitable subset may be obtained by a simple rule: \((i, j) \in E\) if, and only if, packing piece~\(i\) in plate~\(j\) does not allow any other piece to be packed in~\(j\).
The reason this restricted subset is enough to keep the model correctness is presented in next section.

\newtext{
For the enhanced formulation to have more variables than the original formulation, \(|E| > |\bar{J}| + |\{x^o_{jq} : j \in J \land o \in O \land q \in Q_{jo} \land (o = h \land q > \lceil w_j / 2 \rceil) \lor (o = v \land q > \lceil l_j / 2 \rceil)\}|\) must hold, this is, the number of extraction variables must be larger than the number of pieces plus the sum of the number of cuts after the middle of each enumerated plate.
Unfortunately, there is no closed formula for these sets (except \(\bar{J}\) which is given), what makes necessary to compute the full enumeration to verify the difference.
}

%If an extra piece could be packed, then there is a normal cut that creates both a plate that may be used for this extra piece and a plate that may be used to pack~\(i\).
%So the idea here is to do the extraction as late as possible: if the piece may be extracted from a descendant, then the plate may be cut until this descendant is generated to then have the piece extracted from it.

\section{The proof of correctness}
\label{sec:proof_of_correctness}

The previous section presented a detailed explanation of the changes to the formulation and variable enumeration.
This section proves such changes do not affect the correctness of the model.
\oldtext{In \citet{furini:2016} (and, consequently, in their formulation), only the perfect symmetries described below are removed}.
\newtext{The core of the proof consists in showing that any piece multiset that can be \emph{packed} in some plate by the FMT formulation can also be packed by the enhanced formulation.
So, below, what is meant by saying that \emph{a plate can pack a piece multiset} is both (i) that the FMT and the enhanced formulation can represent a valid sequence of cuts obtaining such pieces from a plate but also that (ii) there is a solution for the decision problem variant of G2KP with the plate and the piece multiset as its inputs.
Both meanings are essentially the same because both formulations are exact and will find a solution if it exists.}
The changes \newtext{to FMT} may be summarized to:

\begin{enumerate}
\item There is no variable for any cut that occurs after the middle of a plate.
\item A piece may be obtained from a plate if, and only if, the piece \oldtext{fits}\newtext{is the same size or smaller than} the plate, and the plate cannot \oldtext{fit}\newtext{pack} an extra piece (of any type). \newtext{Obtaining a piece from a plate is always regarded as an extraction and not a cut.}.
\end{enumerate}

The second change alone cannot affect the correctness of the model.
The original formulation was even more restrictive in this aspect: a piece could only be sold if a plate of the same dimensions existed.
In the revised formulation, an extraction variable will always exist in such a case because, if a piece and plate match perfectly, there is no space for any other piece, fulfilling the only criteria for the existence of extraction variables.
Consequently, what needs to be proved is that:

\begin{theorem}{Piece extractions supersede all cuts after the middle of a plate.}

Without changing the pieces obtained from a \oldtext{packing}\newtext{plate}, any normal cut after the middle of a plate may be replaced by a combination of piece extractions and cuts at the middle of a plate or before it.
\label{the:enhanced_correctness}
\end{theorem}

\newtext{
As mentioned before, normal guillotine cuts can be used to search for a solution (in contrast to considering a cut at every position) without loss of optimality.
So the proof goes in the direction of showing that everything that can be done with normal guillotine cuts can be done without the normal cuts after the midplate but by allowing piece extractions.
The theorem above and the proof below assume a plate cannot be cut twice.
If a single cut is applied to a plate, then two new plates are created, and these may be further cut.
There is no loss of generality by undertaking this assumption, which is already implicitly undertaken in the rest of this work.
This difference can be seen as the same difference between the representation of the packing with a binary tree instead of a tree with a variable number of children.
}

\begin{proof} This is a proof by exhaustion. The set of all normal cuts after the middle of a plate may be split into the following cases:
\begin{enumerate}
  \item The cut has a perfect symmetry. \label{case:perfectly_symmetric}
  \item The cut does not have a perfect symmetry.
  \begin{enumerate}
    \item Its second child can \oldtext{fit}\newtext{pack} at least one piece. \label{case:usable_second_child}
    \item Its second child cannot \oldtext{fit}\newtext{pack} a single piece.
    \begin{enumerate}
      \item Its first child packs no pieces. \label{case:no_pieces}
      \item Its first child packs a single piece. \label{case:one_piece} % call luffy to help
      \item Its first child packs two or more pieces. \label{case:many_pieces}
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

The author believes to be self-evident that the union of~\cref{case:perfectly_symmetric,case:usable_second_child,case:no_pieces,case:one_piece,case:many_pieces} is equal to the set of all normal cuts after the middle of a plate. An individual proof for each of these cases is presented below.

\begin{description}
\item[\Cref{case:perfectly_symmetric} -- \textbf{The cut has a perfect symmetry.}]
\oldtext{If two distinct cuts have the same children (with the only difference being the first child of one cut is the second child of the other cut, and vice-versa), then the cuts are perfectly symmetric.}
\newtext{Two horizontal (vertical) cuts over the same plate are considered \emph{perfectly symmetrical} if they generate the same children plate; for example, an horizontal cut at position five of a plate of length 15 creates the same two children (of lengths five and 10) than a cut at position 10.}
Whether a plate is the first or second child of a cut does not make any difference for the formulation or for the problem.
If the cut is in the second half of the plate, then its symmetry is in the first half of the plate.
Consequently, both cuts are interchangeable, and the one after the midplate may be dismissed.
\item[\Cref{case:usable_second_child} -- \textbf{Its second child can \oldtext{fit}\newtext{pack} at least one piece.}]
\Cref{pro:normalisation} allows us to replace the second child with a size-normalised plate that can pack any demand-abiding set of pieces the original second child could pack.
The second child of a cut that happens after the middle of the plate is smaller than half a plate, and its size-normalised counterpart may only be the same size or smaller.
So the size-normalised plate could be cut as the first child by a normal cut in the first half of the plate.
Moreover, the old first child (now second child) has stayed the same size or grown (because of the size-normalisation of its sibling), which guarantees this is possible.

\item[\Cref{case:no_pieces} -- \textbf{Its first child packs no piece.}]
If both children of a single cut do not pack any pieces, then the cut may be safely ignored.
\item[\Cref{case:one_piece} -- \textbf{Its first child packs a single piece.}]
First, let us ignore this cut for a moment and consider the plate being cut by it (i.e., the parent plate).
The parent plate either: can \oldtext{fit}\newtext{pack} an extra piece together with the piece the first child would pack, or cannot \oldtext{fit}\newtext{pack} any extra pieces.
If it cannot \oldtext{fit}\newtext{pack} any extra pieces, this fulfils the criteria for having an extraction variable, and the piece may be obtained through it.
The cut in question can then be disregarded (i.e., replaced by the use of such extraction variable).
However, if it is possible to \oldtext{fit}\newtext{pack} another piece, then there is a normal cut in the first half of the plate that would separate the two pieces, and such cut may be used to shorten the plate.
This kind of normal cuts may successively shorten the plate until it is impossible to pack another piece, and the single piece that was originally packed in the first child may then be obtained by employing an extraction variable.
\item[\Cref{case:many_pieces} -- \textbf{Its first child packs two or more pieces.}]
If the first child packs two or more pieces, but the second child cannot \oldtext{fit}\newtext{pack} a single piece (i.e., it is waste), then the cut separating the first and second child may be omitted and any cuts separating pieces inside the first child may still be done.
If some of the plates obtained by such cuts need the trimming that was provided by the omitted cut, then these plates will be packing a single piece each, and they are already considered in~\cref{case:one_piece}.
\end{description}

Given the cases cover every cut after the middle of a plate, and each case has a proof, then follows that \cref{the:enhanced_correctness} is correct.

\end{proof}


\begin{figure}[ht]
  \caption{\newtext{Visual help for the proof of correctness}}
  \center
  \input{diagrams/proof_examples.tex}
  \vspace{3mm}
  \legend{\justifying \newtext{\Cref{the:enhanced_correctness} case examples. \Cref{case:perfectly_symmetric,case:no_pieces} are excluded given their simplicity. In all examples, the parent plate is 15x15. In the example of~\cref{case:usable_second_child}, the cut would happen after the middle of the plate, but then the pieces of the second child can be packed in the first child instead. In the example of~\cref{case:one_piece}, both cuts happen after the middle of the plate, and there are no other pieces; however, as no piece may be extracted from the leftovers, then there is an extraction variable available. In the example of~\cref{case:one_piece}, a 2x7 piece exists, but it is not extracted from the plate (the demand for it may be exhausted, for example); therefore, the extraction variable from the previous case does not exist; however, the 2x7 piece allows us to make a cut just to reduce the plate length and, for the size of the second child, an extraction variable is available. Finally, in the example of~\cref{case:many_pieces}, which cut that happens first may be changed, as there is no piece packed in the subplate that would originally become the second child.}}
  \label{fig:proof_examples}
\end{figure}

\newtext{
\Cref{fig:proof_examples} may help the reader to visualize the more complicated parts of the proof.
From the cases above, the FMT formulation (from \citet{furini:2016}) only treats specially the pairs of cuts that are perfectly symmetrical to each other (by removing one of them).
}

\section{\newtext{Adaptation to the rotation variant}}
\label{sec:adaptation_for_rotation}

The adaptation of both FMT and the proposed formulation to the rotation-allowed variant of G2KP are very similar, and the author chose to employ only the proposed formulation to illustrate the process.
The changes needed are:

\begin{enumerate}
\item change the piece set~\(\bar{J}\) before the call to the enumeration procedure;\label{item:J_change}
\item create a new set~\(P\), which binds the two rotations of every piece;\label{item:P_creation}
\item change the constraint~\eqref{eq:demand_limit} to take into account this new set~\(P\).\label{item:demand_con_change}
\end{enumerate}

The changes mentioned in \cref{item:J_change} consist of adding to \(\bar{J}\) a new piece~\(i^\prime\) for each piece~\(i\) for which~\(\nexists k \in \bar{J} : l_k = w_i~\land~w_k = l_i\), piece~\(i^\prime\) have~\(l_{i^\prime} = w_i\), \(w_{i^\prime} = l_i\), and~\(u_{i^\prime} = u_i\); differently, for each piece~\(i\) for which~\(\exists k \in \bar{J} : l_k = w_i~\land~w_k = l_i\) (i.e., for each piece~\(i\) that has its rotation as an already existing piece~\(k\)), both \(u_i\) and \(u_k\) become the sum of their original values (given by the instance).

The set~\(P\) mentioned in~\cref{item:P_creation} may be defined as \(P = \{ \{i, k\} \in P : i \in \bar{J}, k \in \bar{J},  l_k = w_i~\land~w_k = l_i\}\). Each element of~\(P\) is a set of two pieces.

Finally, as mentioned in~\cref{item:demand_con_change}, the following change is made:

\begin{flalign*}
&& \sum_{j \in E_{i*}} e_{ij} \leq u_i && \forall i \in \bar{J}\tag{\ref{eq:demand_limit}}
%\specialcell{\sum_{j \in E_{i*}} e_{ij} \leq u_i \hspace*{\fill} \forall i \in \bar{J},}\tag{\ref{eq:demand_limit}}
\end{flalign*}

to

\begin{flalign}
&& \sum_{j \in E_{i*}} e_{ij} + \sum_{j \in E_{k*}} e_{kj} \leq u_i && \forall \{i, k\} \in P\label{eq:rotation_demand}
\end{flalign}

\section{\newtext{The rotation-specific mirror plate enhancement}}
\label{sec:mirror_plate}

The previous section describes only the minimal changes necessary to adapt the proposed formulation for the rotation-allowed variant.
This section describes a reduction for the formulation that is only possible if rotation is allowed.
This change is also compatible with the FMT formulation.

The core idea of the enhancement is that, if all pieces can rotate, then any two plates~\(j\) and~\(j^\prime\), for which \(l_j = w_{j^\prime} \land w_j = l_{j^\prime}\) holds, are equivalent to each other.
The rationale is simple: if a set of pieces is extracted from plate~\(j\) through a guillotine cutting pattern, then there exists an equivalent guillotine pattern in which every piece is rotated and which can be extracted from plate~\(j^\prime\) which is the rotation of plate~\(j\).
This way, if the cut and plate enumeration generates such plates~\(j\) and \(j^\prime\), only one of them needs to be kept (but every cut that generated the removed plate needs now to generate the kept rotation instead).
Finally, the opposite statement is also true (yet very inefficient): if all \emph{plates} could somehow rotate, then the pieces themselves would not need to be able to rotate, and distinct piece types that are rotations of each other could be considered the same.

The modifications necessary for the reduction are restricted to the cut-and-plate enumeration.
Only plates in which the length is smaller than the width are allowed.
If a cut would generate a plate with its length greater than its width, then the cut instead creates the rotated version of the plate, in which the length is the smallest dimension.
This change can potentially reduce the number of plates (constraints) to half their original amount.
Consequently, the number of variables (cuts and extractions over specific plate types) may also reduce up to half their original amount.

The only downside of mirroring the plates is that the Redundant-Cut reduction needs to be either disabled or adapted.
If plate mirroring is enabled, then Redundant-Cut needs to keep track if either (or both) child plates of a cut were rotated or not.
This tracking usually is not necessary to implement just the plate mirroring itself.
The experiments employing rotation focus mostly on the proposed formulation (in which Redundant-Cut is already disabled because it is superseded); the few experiments with rotation-enabled FMT have Redundant-Cut disabled.

\section{\newtext{The pricing phase}}
\label{sec:pricing}

The pricing procedure described in~\citet{furini:2016,dimitri_thesis} was reimplemented by us.
]No significant changes were made to the procedure.
As the experiments include multiple comparisons involving this procedure, a summary of the procedure is presented below.
For simplicity, the procedure takes an already built model (from either the original formulation or the enhanced version), and any previous reductions mentioned were already applied.
\citet{clautiaux:2018} refers to a similar procedure (that they apply to their own formulation) as \emph{lagrangian filtering}; however, this term is not employed by~\citet{furini:2016,dimitri_thesis}.

\begin{enumerate}
\item Fix to zero all variables representing horizontal (vertical) cuts that do not match a piece length (width).
\item Remove all integrality constraints and solve the relaxed model to obtain an upper bound for the position-only restricted problem.
\item Obtain a lower bound from an \emph{inexact 2-staged} heuristic~\citep{furini:2016,dolatabadi:2012}.\label{it:heuristic}
\item Employ the reduced costs of the model variables, the position-only restricted upper bound, and the heuristic lower bound to price-out variables (more details below) by fixing them to zero.\label{it:restricted_final_pricing}
\item Restore the integrality constraints, warm-start with the heuristic solution from (step \autoref{it:heuristic}), solve the model (currently, a reduced MILP model for the position-only restricted variant of the problem) and obtain a probably better lower bound. While unlikely, the heuristic may have already provided an optimal solution for the position-only restricted problem.\label{it:restricted_lb}
\item Remove all integrality constraints again.
\item \textbf{DO} solve the relaxed model, compute the reduced cost of the fixed variables, and unfix a subset of the variables with positive reduced cost \textbf{WHILE} variables with positive reduced cost exist. This loop is responsible for reintroducing any variables representing unrestricted cuts needed to solve the unrestricted variant back to the model. More details on the subset of the variables selected are below.\label{it:loop}
\item Employ the reduced costs and the upper bound, both obtained from the last solve in the loop, as well as the lower bound from the MILP solve of the position-only restricted model (\autoref{it:restricted_lb}), to price-out variables (similarly to what was done in~\autoref{it:restricted_final_pricing}).\label{it:final_pricing}
\item Warm-start the model with the solution from~\autoref{it:restricted_lb}.
\item Restore integrality constraints, remove all variables yet fixed to zero, and return the model.
\end{enumerate}

In~\autoref{it:restricted_final_pricing} and \autoref{it:final_pricing}, a variable is \emph{priced out} if \(\lfloor reduced\_cost(var) + ub \rfloor \leq lb\), where the upper and lower bounds are the ones available at the corresponding step.
The rationale behind this requirement is straightforward.
If forcing \(var\) to assume value \(1\) is enough to reduce the upper bound from the relaxation to less than the lower bound, then that variable (guillotine cut) cannot be used to provide a solution better than the current lower bound.
Any variables necessary to produce the current lower bound are kept.

The criteria for choosing the subset of variables in each iteration of~\autoref{it:loop} takes into account two parameters: \(n_{max}\) and \(\bar{p}\).
If any variables have reduced cost above~\(\bar{p}\), they define the subset; otherwise, the first \(n_{max}\) variables with positive reduced cost define the subset.
The original description of the procedure does mention an ordering of the variable pool, so what constitutes the \emph{first} \(n_{max}\) \emph{variables} is not well-defined.
The author chose to interpret that the \(n_{max}\) variables of \emph{largest reduced cost} are selected.
Both parameters are automatically computed for each instance: \(n_{max}\) is one-fifth of the sum of the demand vector~\(u\), and~\(\bar{p}\) is one-fourth of the sum of the profits for every piece (taking demand into account).

The original description of the procedure does not indicate if, during the process, the variables are fixed and unfixed, or removed and added back.
Preliminary tests indicated that the fix-and-unfix approach had better performance, so it was used in the experiments of this work.
In the last step, all variables yet fixed to zero are removed.

\chapter{Empirical analysis of the proposed enhancements}
\label{sec:experimental_results}
\label{sec:furini_vs_enhanced_comparison}

There are three formulation implementations that provide data used in the comparisons of this chapter:
\emph{original} refers to the implementation presented in~\citet{furini:2016} and in \citet{dimitri_thesis};
\emph{faithful} refers to the reimplementation of \emph{original} employed in this thesis;
\emph{enhanced} refers to the enhanced formulation presented in~\cref{sec:enhanced_model}.
The \emph{original} implementation was not available\footnote{
	The author os this work asked the authors of~\citet{furini:2016} for the \emph{original} implementation and Dimitri Thomopulos informed us it was not available.
}.
Consequently, all data relative to \emph{original} presented in this work comes from~\citet{dimitri_thesis}.
\newtext{As both \emph{original} and \emph{faithful} refer to implementations of the FMT, the term `FMT' is avoided in this chapter.}
For the sake of brevity and consistency, in this section, if a reference could be made to both \citet{dimitri_thesis} and \citet{furini:2016}, or to either of them, then only the former is cited.
Both \emph{faithful} and \emph{enhanced} data were obtained by runs using the setup described in~\cref{sec:setup}.

Each formulation may be modified by applying any combination of the following optional procedures:
\emph{priced} -- refer to the pricing procedure described in\newtext{~\cref{sec:pricing} (originally from \citet{dimitri_thesis})}\oldtext{\citet{dimitri_thesis}};
\emph{normalised} -- the plate-size normalisation procedure described in~\cref{sec:psn};
\emph{warmed} -- the MILP models solved were warm-started with a solution found by a previous step;
\emph{Cut-Position} and \emph{Redundant-Cut} -- are reduction procedures described in~\citet{furini:2016} and in \citet{dimitri_thesis}, that may be enabled and disabled individually.
For each experiment described in the next sections, if a procedure is not mentioned, then it is disabled.
The term \emph{restricted priced} refers to the model for the restricted version of the problem that is solved inside the pricing procedure mentioned above.
Consequently, for each run of a \emph{priced} variant, there will be a \emph{restricted priced} run with the same combination of optional procedures.
The differences between the \emph{restricted priced} and the (unrestricted) \emph{priced} models are mainly that:
(i) the \emph{restricted priced} model never has an horizontal (vertical) cut that does not match the \oldtext{width}\newtext{length} (\oldtext{length}\newtext{width}) of a piece;
(ii) the \emph{restricted priced} model is MIP-started with the solution of an heuristic (described in~\citet{dimitri_thesis}) while the \emph{priced} model is MIP-started with the solution of the \emph{restricted priced} model;
(iii) the distinct solutions used to MIP-start the respective models are also used as the lower bound for the pricing procedure (details in~\citet{dimitri_thesis}).

\newtext{
Without the set of model variables (guillotine cuts) removed by the pricing, plates of some dimensions may become impossible to obtain.
These plates are not necessary to obtain an optimal solution; otherwise, the pricing could not have removed all variables that led to them.
Most of these plates could be further cut, but the value of the variables associated with such cuts can only be zero now, and, therefore, these variables can be removed too.
This thinning effect may be recursive, as each newly removed variable may render some plate sizes unobtainable, similarly to what is described in~\cref{sec:psn}.
Hence, the pricing phase uncovers a set of unnecessary variables larger than the set it directly removes.
The effort to remove such unnecessary variables and constraints is negligible.
The algorithm to select which variables and constraints are kept is similar to finding the connected subgraph (starting from the original plate) in the graph representation of the formulation described at~\cref{sec:furini_model}.
In \emph{priced} variants of \emph{faithful} and \emph{enhanced} this \emph{purge} procedure is done unless stated otherwise. The experiments will show that this \emph{purge} drastically reduces the number of variables and constraints but has almost no effect on the running times. Consequently, the author believes the solver can detect and remove such variables by itself.
Nonetheless, the author encourages future comparisons to implement this \emph{purge} procedure, as it helps determine the real size of the solved models.
}

\oldtext{The goal of the pricing procedure is to remove unneeded variables from the model. However, the priced model often ends up with unneeded constraints and variables due to pricing. This effect is similar to the one described by items (ii)--(iv) in~\autoref{sec:psn}: if some variables (i.e., cuts) are removed, then some plates are never produced (i.e., some constraints just fix their variables to zero), consequently other variables/cuts become impossible, recursiverly. The effort to remove such unnecessary variables and constraints is negligible. The algorithm used is similar to finding the connected subgraph in the directed hypergraph defined by the variables/cuts (edges) and constraints/plates (nodes) starting from the original plate. In \emph{priced} variants of \emph{faithful} and \emph{enhanced} this \emph{purge} procedure is done unless stated otherwise. The experiments will show that this \emph{purge} drastically reduces the number of variables and constraints, but has almost no effect on the running times. Nonetheless, we encourage future comparisons to implement this \emph{purge} procedure, as it helps determine the real size of the solved models.}

Each experiment \oldtext{fills a gap for the next experiments}\newtext{helps to substantiate choices taken in the subsequent experiments}:
\cref{sec:lp_method} explains the choice of LP algorithms made in all remaining experiments;
\cref{sec:faithful_reimplementation} provides evidence that \emph{faithful} is on par with \emph{original}, allowing us to use it as a replacement;
\cref{sec:comparison} compares \emph{faithful} to \emph{enhanced} and shows the value of some of this thesis contributions (namely, the \emph{normalise} procedure and the \emph{enhanced} formulation);
\cref{sec:new_results} applies the methods with best results in the last experiment to prove new optimal values and bounds for harder instances.

\section{Setup}
\label{sec:setup}

Every experiment in this work uses the following setup unless stated otherwise.
The CPU was an AMD\textsuperscript{\textregistered} Ryzen\textsuperscript{TM} 9 3900X 12-Core Processor (3.8GHz, cache: L1 -- 768KiB, L2 -- 6 MiB, L3 -- 64 MiB) and 32GiB of RAM were available (2 x Crucial Ballistix Sport Red DDR4 16GB 2.4GHz).
The operating system used was Ubuntu 20.04 LTS (Linux 5.4.0-42-generic).
Hyper-Threading was disabled.
Each run executed on a single thread, and no runs executed simultaneously.
The computer did not run any other CPU-bound tasks during the experiments.
The exact version of the code used is available online (\url{https://github.com/henriquebecker91/GuillotineModels.jl/tree/0.2.4}), and it was run using Julia 1.4.2~\citep{julia} with JuMP 0.20.1~\citep{JuMP} and Gurobi 9.0.2~\citep{gurobi}.
The following Gurobi parameters had non-default values: \verb+Threads+~\(= 1\); \verb+Seed+~\(= 1\); \verb+MIPGap+~\(= 10^{-6}\) (to guarantee optimality); and \verb+TimeLimit+~\(= 10800\) (i.e., three hours).
The next section explains the rationale for using \verb+Method+~\(= 2\) (i.e., barrier) to solve the root node relaxation of the final built model; and \verb+Method+~\(= 1\) (i.e., dual simplex) inside pricing (if pricing is enabled).

\section{The choice of LP algorithm}
\label{sec:lp_method}

\citet{dimitri_thesis} do not specify the algorithm used for solving the MILP root node relaxation and, if pricing is enabled, for solving some LP models (upper bound computation) and the MILP root node relaxation of the \emph{restricted priced} model.
As Gurobi is used here, the \verb+Method+ parameter (for LP models and MILP root node relaxations) is being discussed, and not the \verb+NodeMethod+ parameter (for non-root nodes).
The choice of the algorithm can drastically impact running times.
A preliminary experiment included all LP algorithms available in Gurobi.
\Cref{tab:lp_method_comparison} presents the data of the two algorithms selected for use.
They are the \emph{Dual Simplex} and the \emph{Barrier}.

The runs use the \emph{faithful} implementation, with \emph{Cut-Position} and \emph{Redundant-Cut} enabled, in its \emph{priced} (Priced PP-G2KP in~\citet{dimitri_thesis}) and \emph{not priced} (PP-G2KP in~\citet{dimitri_thesis}) variants.
For convenience, the experiment is limited to a few instances.
This subset consists of all instances for which the \emph{Complete PP-G2KP Model} finds the optimal solution within the time limit in~\citet{furini:2016} (Table 2).
If pricing is disabled, the root node relaxation contributes to most of the running time.
This characteristic makes them a good choice for this experiment.

\begin{table}
\centering
\caption{Comparison of LP-solving algorithms used inside solving procedure}
\begin{tabular}{@{\extracolsep{4pt}}lrrrrrrr@{}}
\hline\hline
Instance & \multicolumn{3}{c}{Dual Simplex} & \multicolumn{3}{c}{Barrier} & DS + B \\\cline{2-4}\cline{5-7}
& N. P. & R. \% & Priced & N. P. & R. \% & Priced & Priced \\\hline
CU1 & 27.37 & 92.11 & 3.79 & 24.18 & 94.68 & 3040.82 & \textbf{3.58} \\
STS4 & 93.49 & 89.88 & 48.80 & 49.94 & 77.32 & 7851.30 & \textbf{47.75} \\
STS4s & 103.20 & 94.92 & 39.29 & 43.74 & 86.34 & 8470.41 & \textbf{38.36} \\
gcut9 & 226.68 & 72.29 & \textbf{3.92} & 51.48 & 85.77 & 2060.04 & 4.01 \\
okp1 & 51.95 & 84.18 & 38.89 & \textbf{32.41} & 67.78 & -- & 38.79 \\
okp4 & 98.25 & 93.35 & 144.30 & \textbf{72.09} & 92.31 & -- & 141.53 \\
okp5 & 178.13 & 89.89 & 252.09 & \textbf{96.38} & 67.24 & -- & 239.44 \\\hline\hline
\end{tabular}
\legend{
\justifying \emph{Dual Simplex} and \emph{Barrier} indicate the respective algorithm was used for all LPs and root node relaxations, \emph{DS + B} means that \emph{Dual Simplex} was used to solve all LPs inside the pricing phase, and \emph{Barrier} was used to solve the root node relaxation of the final model.
The columns \emph{N.P.} (\emph{Not Priced}) and \emph{Priced} display the time to solve (in seconds) using the aforementioned variant.
The columns \emph{R.\%} refer to the per cent of the time spent by \emph{Not Priced} in the root node relaxation of the final model.
Source: the author.}
\label{tab:lp_method_comparison}
\end{table}

The following conclusions can be derived from \cref{tab:lp_method_comparison}.
Using the \emph{Barrier} algorithm in the pricing phase is not viable.
This impracticality happens because the pricing phase includes an iterative variable pricing phase.
This iterative phase repeatedly adds variables to one LP model and solves it again.
The \emph{Barrier} algorithm solves every LP from scratch; the \emph{Dual Simplex} reuses the previous basis and saves considerable effort.
However, \emph{Barrier} performs better if there is no previous base to reuse.
Consequently, the configuration chosen was \emph{Dual Simplex} for the pricing phase and \emph{Barrier} for the root relaxation of the final model.

\section{Comparison of \emph{faithful} against \emph{original}}
\label{sec:faithful_reimplementation}

Without a reimplementation of \emph{original}, any comparison would need to be made directly against the data in~\citet{dimitri_thesis}.
However, such a comparison would hardly be fair, as it compares across machines, solvers, and programming languages.
Also, for example, it does not allow us to assess the benefits of applying the \emph{plate-size normalisation} procedure to the \emph{original} formulation.
The purpose of this section is to show that \emph{faithful} may be fairly used in place of \emph{original}.
For this purpose, \cref{tab:faithful_reimplementation} compares the number of model variables and number of plates of the diverse model variants presented in~\citet{dimitri_thesis}\oldtext{~(using the same 59 instances)}.
\newtext{The chosen dataset is, therefore, the same as the one used in these works for the comparison to be possible.
The dataset aggregates 59 instances of the previous literature from many distinct sources, and all instances are either artificially generated or of undisclosed origin.
A detailed entry about this dataset and all of its constituting instances can be found in~\cref{sec:datasets} under the FMT59 (which is the name adopted for this dataset in this work).}
The number of enumerated plates strongly correlates to the number of constraints in the model.
\citet{dimitri_thesis} presents the number of plates, not the number of constraints.
To simplify the comparison, the same is done here.

The \emph{Priced PP-G2KP} runs in~\citet{dimitri_thesis} had three time limits of one hour to solve: the restricted model (i.e., obtaining a lower bound); the iterative variable pricing (i.e., obtaining an upper bound); the final model.
Such configuration always generates a final model.
However, it also has two drawbacks:(i) the computer performance may define the answer given in the first two phases, affecting the size of the final model (and making it harder to make a fair comparison);(ii) if the restricted model, or the iterated variable pricing, cannot be done in one hour, then the final model will probably hit the time limit too -- in~\citet{dimitri_thesis}, every run that hits one of the two first time limits also hits the third time limit.
The author chose to use a single three-hour time limit \newtext{for the experiments of this chapter. In the other chapters, either the more common one-hour time limit is employed or, for relatively short experiments, no time limit is employed.}

\Cref{tab:faithful_reimplementation} references the names used in~\citet{dimitri_thesis}.
The \emph{Complete PP-G2KP} is the formulation with all optional procedures disabled, while the \emph{PP-G2KP} mean both \emph{Cut-Position} and \emph{Redundant-Cut} are enabled.
\emph{Restricted PP-G2KP} and its priced version are solved inside \emph{Priced PP-G2KP} runs.
If the lower and upper bounds found during pricing are the same, then the optimal solution was found before generating the final model.
The instances in which this happened for an unrestricted solution are 3s, A1s, CU1, CU2, W, cgcut1, and wang20.
The instance A1s presented this behaviour already in the pricing of the restricted model.

The \emph{original} had no \emph{purge} phase after pricing.
Consequently, for the columns that refer to \emph{original}, the last row just repeats the data of the row above.

\begin{table}
\centering
\caption{Comparison of \emph{faithful} against \emph{original}}
\begin{tabular}{lccrrrr}
\hline\hline
Variant & T. L. & E. R. & O. \#v & F. \%v & O. \#p & F. \%p\\\hline
Complete PP-G2KP & 0 & 0 & 156,553,107 & 100.00 & 1,882,693 & 100.00\\
Complete +Cut-Position & 0 & 0 & 103,503,930 & 99.99 & 1,738,263 & 100.01\\
Complete +Redundant-Cut & 0 & 0 & 121,009,381 & 109.94 & 1,882,693 & 100.00\\
PP-G2KP (CP + RC) & 0 & 0 & 74,052,541 & 120.05 & 1,738,263 & 100.01\\
Restricted PP-G2KP & 0 & 0 & 5,335,976 & 99.28 & 306,673 & 99.99\\
Priced Restricted PP-G2KP & 0 & 1 & 3,904,683 & 102.20 & 305,690 & 99.99\\
(no purge) Priced PP-G2KP & 3 & 7 & 14,619,460 & 93.74 & 1,642,382 & 100.01\\
Priced PP-G2KP & 3 & 7 & 14,619,460 & 31.92 & 1,642,382 & 25.55\\\hline\hline
\end{tabular}
\legend{
\justifying
The sum of columns \emph{T. L.} (Time Limit) and \emph{E. R.} (Early Return) gives the number of instances excluded from consideration in the respective row.
Column \emph{T. L.} has the number of instances for which \emph{faithful} reached the time limit without generating the respective model variant -- these instances are: Hchl7s, okp2, and okp3.
The column \emph{E. R.} has the number of instances for which this thesis reimplementation found an optimal solution before generating the respective model variant.
Columns \emph{O. \#v} and \emph{O. \#\oldtext{v}\newtext{p}} refer to \emph{original}.Column \emph{O. \#v} (\emph{O. \#p}) presents the sum of variables (plates) for the instances in which \emph{faithful} generated a model.
Columns \emph{F. \%v} and \emph{F. \%p} refer to \emph{faithful}
Column \emph{R. \%v} (\emph{R. \%p}) has the sum of variables (plates) in the generated models as a percentage of the quantity obtained by the original implementation.
Source: the author.}
\label{tab:faithful_reimplementation}
\end{table}

The following conclusions can be derived from \cref{tab:faithful_reimplementation}.
All variants, except \emph{Priced PP-G2KP}, are within \(\pm0.01\)\% of the expected number of plates (and, consequently, of constraints).
The \emph{Complete PP-G2KP}, \emph{Complete +Cut-Position}, and \emph{Restricted PP-G2KP} are within \(\pm1\)\% of the expected number of variables.
The number of variables in both \emph{Complete +Redundant-Cut} and \emph{PP-G2KP (CP + RC)} is \(10\sim20\)\% larger than expected.
\oldtext{Our reimplementation of \emph{Redundant-cut} reduction seems responsible for both deviations.}\newtext{Given the experiments isolate such divergence to cases in which Redundant-Cut is enabled, the author believes there is some disagreement between the original implementation of Redundant-Cut and its reimplementation.}
However, \oldtext{it}\newtext{the reimplementation} follows closely the description given in~\citet{dimitri_thesis}.
The number of variables and plates in \emph{Priced} variants is not entirely deterministic.
The number of variables of \emph{Priced} variants is either slightly above (\(+2\)\%) or lower (\emph{\(-6\sim68\)\%}).

For all non-\emph{priced} variants, the fraction of the running time spent in the model generation is negligible.
Consequently, the comparison presented in~\cref{tab:faithful_reimplementation} is sufficient.
The author cannot say the same for the \emph{priced} variants.
\citet{dimitri_thesis} does not report the size of the multiple LP models solved inside the iterative pricing (a phase of the pricing).
For instances in which \emph{original} and \emph{faithful} executed all phases of pricing and solved the final model, the \emph{original} spent 34.35\% of its time in the iterative pricing phase, while \emph{faithful} spent 61.69\%.
It is hard to pinpoint the source of this discrepancy.
One possible explanation is that, in \emph{original}, other phases took more time than they took in \emph{faithful}.
For example, \emph{faithful} uses the \emph{barrier} algorithm for the root node relaxation of the final model, which reduces the percentage of time spent in this phase.
Nevertheless, for the subset of the instances aforementioned, the total time spent by \emph{faithful} was about 13\% of the time spent by \emph{original}.
While the difference between machines and solvers does not allow us to infer much from that figure, the author believes that the magnitude of the difference guarantees that \emph{faithful} is not a gross misrepresentation.

\section{Comparison of \emph{faithful} against \emph{enhanced}}
\label{sec:comparison}

The primary purpose of this section is to evaluate the impact of the proposed enhancements to the state of the art.
The contributions evaluated here are the \emph{normalise} reduction (i.e., the plate-size normalisation presented in~\cref{sec:psn}) and the \emph{enhanced} formulation (presented in \cref{sec:enhanced}).
The state of the art consists in a formulation (\emph{Complete PP-G2KP}), two reductions (\emph{Cut-Position} and \emph{Redundant-Cut}), and a pricing procedure presented in~\citet{furini:2016,dimitri_thesis}.
In this section, the reimplementation of \emph{Complete PP-G2KP} named \emph{faithful} (to distinguish from the data of the \emph{original}) is employed.
The author also reimplemented the reductions and the pricing procedure, but as \emph{enhanced} may also enable these optional procedures, the text avoids labelling them as \emph{faithful} to minimise confusion.

The \emph{faithful} and \emph{enhanced} formulations cannot be combined.
However, both allow enabling any combination of the optional procedures.
The only exception is \emph{Redundant-Cut}, which is unnecessary for \emph{enhanced} and, therefore, never applied to it.
Outside of this exception, in this section, \emph{Redundant-Cut} and \emph{Cut-Position} are always enabled.
These reductions never increase the number of variables (or constraints), cost negligible computational effort, and were already discussed in~\citet{furini:2016,dimitri_thesis}.

This section also discusses the effects of the \emph{purge} procedure and warm-starting the non-\emph{priced} model.
The deterministic heuristic used to MIP-start the non-\emph{priced} models is the same used in the \emph{restricted priced} model solved inside the pricing procedure.

\begin{table}
\centering
\rowcolors{1}{white}{gray-table-row}
\caption{Comparison of \emph{faithful} vs. \emph{enhanced} over the 59 instances used in~\citet{dimitri_thesis}}
\begin{tabular}{lrrrrrrrr}
\hline\hline
Variant & T. T. & \#e & \#m & \#s & \#b & S. T. T. & \#variables & \#plates \\\hline
Faithful & 106,057 & -- & 59 & 53 & 0 & 41,257 & 88,901,964 & 1,738,366 \\
Enhanced & 25,538 & -- & 59 & 58 & 2 & 14,738 & 3,216,774 & 231,836 \\
F. +Normalizing & 60,078 & -- & 59 & 56 & 0 & 27,678 & 60,316,964 & 610,402 \\
E. +Normalizing & 14,169 & -- & 59 & 59 & 52 & 14,169 & 2,733,125 & 145,157 \\
F. +N. +Warming & 60,542 & -- & 59 & 56 & 0 & 28,142 & 60,316,964 & 610,402 \\
E. +N. +Warming & 9,778 & -- & 59 & 59 & 4 & 9,778 & 2,733,125 & 145,157 \\
Priced F. +N. +W. & 49,919 & 8 & 50 & 55 & 0 & 6,719 & 3,210,857 & 174,214 \\
Priced E. +N. +W. & 9,108 & 8 & 51 & 59 & 1 & 9,108 & 600,778 & 64,904 \\
P. F. +N. +W. -Purge & 50,054 & 8 & 50 & 55 & 0 & 6,854 & 8,072,810 & 544,892 \\
P. E. +N. +W. -Purge & 9,209 & 8 & 51 & 59 & 0 & 9,209 & 1,021,526 & 134,102 \\\hline\hline
\end{tabular}

\legend{
\justifying
The meaning of each column follows: \emph{T. T.} (Total Time) -- sum of the time spent in all instances including timeouts, in seconds;
\emph{\#e} (early) -- number of instances in which pricing found an optimal solution (and, consequently, did not generate a final model);
\emph{\#m} (modeled) -- number of instances that generated a final model;
\emph{\#s} (solved) -- number of solved instances;
\emph{\#b} (best) -- number of instances that the respective variant solved faster than any other variant;
\emph{S. T. T.} (Solved Total Time) -- same as Total Time but excluding runs ended by time or memory limit;
\emph{\#variables} (\emph{\#plates}) -- sum of the variables (plates) in all generated final models (see column~\emph{\#m}).
The first row (Faithful) has two runs that ended in memory exhaustion.
The time of these runs is accounted for as they were timeouts.
Source: the author.}
\label{tab:contribution}
\end{table}

Considering the data from~\cref{tab:contribution}, the following statements can be made:
\begin{enumerate}
\item \emph{enhanced} solves more instances than \emph{faithful} (using at most 24\% of its time);
\item the number of variables of `Enhanced' is almost the same as `Priced F. +N. +W.';
\item between `Enhanced' and `Priced F. +N. +W.' the former has better results;
\item \emph{normalise} further reduces variables by \(14\sim32\)\% and plates by \(37\sim65\)\%;
\item MIP-starting \emph{enhanced} makes it slightly slower in 52 instances;
\item MIP-starting \emph{enhanced} saves more than one hour in the other 7 instances;
\item any benefit from MIP-start in `F. +N. +Warming' was negated by its timeouts;
\item \emph{purge} greatly reduces the model size but has almost no effect on running time;
\item the effects of applying \emph{pricing} to \emph{enhanced} are not much better than \emph{purge};
\item applying \emph{pricing} to \emph{faithful} is positive overall but loses one solved instance.
\end{enumerate}

\newtext{
Both the number of variables (cuts) and plates (constraints) are reduced by~\emph{enhanced}.
The reduction in the number of variables (cuts) is a direct consequence of the \emph{enhanced} differential: making unnecessary any cuts after the middle of each plate.
However, the reduction in the number of plates (constraints) is an \emph{indirect} consequence of the same differential.
}

\newtext{
One of the ways the \emph{enhanced} reduces the number of constraints is by innately avoiding the creation of some size-normalised plate types.
The length (width) of an horizontal (vertical) cut is always normalised, i.e., a demand-abiding combination of piece lengths (widths), and so is the length (width) of the first child, but there is no guarantee about the length (width) of the second child.
If the cut happens after the middle of the plate and the length (width) of the second child is not normalised (or the second child cannot pack any piece and is discarded as waste), then there is no perfectly symmetrical cut in the first half of the plate.
Horizontal (vertical) cuts in the first half of the same plate (the only ones available to the \emph{enhanced} formulation) cannot create a second child with the same normalised length (width) of the previously mentioned first child.
Therefore, plates with the non-normalised length (width) are obtained by both formulations, but the normalised counterparts are obtained only by the \emph{original} formulation.
The fact that a plate is normalised (or not) is irrelevant in itself (at least for the considered formulations).
The fact that the \emph{enhanced} formulation has the non-normalised plate instead of its normalised counterpart is not relevant to the performance of the formulation (much less to its correctness).
However, having both the non-normalised plate and the normalised counterpart increases the number of constraints without any clear advantage and, therefore, negatively impacts the performance of the \emph{original} formulation.
}

\begin{table}
\centering
\rowcolors{1}{white}{gray-table-row}
\caption{Fraction of the total time spent in each step (only runs that executed all steps)}
\begin{tabular}{lrrrrrrrrr}
\hline\hline
Variant & Time & E~\% & H~\% & RP~\% & IP~\% & FP~\% & LP~\% & BB~\% \\\hline
Priced Faithful +N. +W. & 6,632 & 0.12 & 0.38 & 26.16 & 57.36 & 2.91 & 4.56 & 8.29 \\
Priced Enhanced +N. +W. & 1,178 & 0.03 & 2.18 & 50.89 & 23.66 & 0.46 & 2.70 & 19.95 \\
P. F. +N. +W. -Purge & 6,766 & 0.11 & 0.37 & 26.00 & 57.03 & 2.81 & 5.12 & 8.45 \\
P. E. +N. +W. -Purge & 1,185 & 0.03 & 2.18 & 50.70 & 23.64 & 0.46 & 2.83 & 20.09 \\\hline\hline
\end{tabular}
\legend{
\justifying
\emph{Time} is the sum of all time (in seconds) spent in the 47 instances that finished all phases in all four variants considered.
These are the same 47 indicated in row \emph{Priced F. +N. +W.} of \cref{tab:contribution}.
From the 59 instances dataset, 4 had timeout (Hchl4s, Hchl7s, okp2, and okp3), and 8 found an optimal solution inside pricing (3s, A1s, CU1, CU2, W, cgcut1, okp4, and wang20).
All remaining columns present percentages of the time spent in a specific phase:
\emph{E} -- enumeration of cuts and plates (and all reductions);
\emph{H} -- restricted heuristic used to warm-start the restricted priced model;
\emph{RP} -- restricted pricing (not including the heuristic time);
\emph{IP} -- iterative pricing;
\emph{FP} -- final pricing;
\emph{LP} -- root node relaxation of the final model;
\emph{BB} -- branch-and-bound over the final model.
Source: the author.}
\label{tab:time_fractions}
\end{table}

Considering the data from~\cref{tab:time_fractions}, the following statements can be made:
\begin{enumerate}
\item both \emph{BB} and \emph{LP} phases are slightly faster with \emph{purge} as expected;
\item both \emph{E} and \emph{H} phases are almost negligible (at most 2\% with \emph{H} in \emph{enhanced});
\item together the \emph{RP} and \emph{IP} phases account for \(74.5\sim83.5\)\%;
\item \emph{RP} and \emph{IP} swap percentages between \emph{enhanced} and \emph{faithful};
\item \emph{faithful} shows some overhead in all phases strongly affected by model size.
\end{enumerate}

\section{Evaluating \emph{enhanced} against harder \newtext{G2KP} instances}
\label{sec:new_results}

The purposes of the experiment described in this section are:
(i) to show the limitations of the \emph{enhanced} formulation against more challenging instances;
(ii) to provide better bounds and new proven optimal values for such instances.

\citet{velasco:2019} proposes a set of 80 hard instances to test the limitations of their bounding procedures; these instances are employed in this section.
\newtext{The instances were artificially generated and are divided into four classes of 20 instances each.
The dataset focuses on two characteristics: (i) the area of the pieces is small compared to the area of the original plate (the average ratio varies between 1.6\% and 5\%); (ii) each class is defined by the shape of the original plate, and the likely shape of the randomly generated pieces.
The original plates of the first two classes have one dimension two or four times larger than the other dimension.
In the first class, the pieces are likely to be larger in the same dimension the original plate is larger; in the second class, the pieces are likely to be larger in the dimension the original plate is shorter.
The original plates of the last two classes are squares.
The pieces of the third class have, on average, the same dimension with double the size of the other; in the fourth class, half of the pieces follow the previous distribution, and the other half invert the favoured dimension.
More details can be found in the Appendix.}

Only two variants were executed for this experiment, the \emph{priced} and non-\emph{priced} versions of \emph{enhanced} with \emph{Cut-Position}, \emph{normalise}, and \emph{MIP-start} enabled.
The results for the \emph{restricted priced} variant are also presented because this variant is solved inside a step of the \emph{priced} variant (the same reductions apply to it).
\Cref{tab:velasco_summary} presents a summary of all runs, and \cref{tab:velasco_new_results_I,tab:velasco_new_results_II,tab:velasco_new_results_III,tab:velasco_new_results_VI} presents the improved bounds and solved instances.

For this experiment, Gurobi was allowed to use the 12 physical cores of the employed machine.
Gurobi distributes the effort of the branch-and-bound (B\&B) phase equally among all cores.
\oldtext{However, s}\newtext{S}olving an LP (as a root node relaxation, or not) calls barrier, primal simplex, and dual simplex.
Each of \newtext{the simplex methods} uses a single thread, \newtext{while barrier uses all remaining cores}, and Gurobi stops when the first of them finish\newtext{es}.

\begin{table}
\centering
\caption{Summary table for the instances proposed in~\citet{velasco:2019}}
\begin{tabular}{lrrrrrrr}
\hline\hline
C. & Variant & \#m & Avg. \#v & Avg. \#p & T. T. & \#s & Avg. S. T. \\\hline
\multirow{3}{*}{1} & Not Priced & 20 & 1,787,864.55 & 22,316.50 & 172,574 & 5 & 2,114.85 \\
                   & Restricted Priced & 13 & 467,692.15 & 17,139.00 & 180,051 & 5 & 3,610.29 \\
\vspace{1.5mm}     & Priced & 5 & 264,315.80 & 11,978.40 & 196,733 & 3 & 4,377.77 \\
\multirow{3}{*}{2} & Not Priced & 20 & 1,533,490.70 & 18,638.50 & 167,973 & 5 & 1,194.68 \\
                   & Restricted Priced & 20 & 453,159.70 & 18,638.30 & 155,184 & 8 & 3,198.11 \\
\vspace{1.5mm}     & Priced & 8 & 394,613.88 & 9,735.50 & 178,812 & 4 & 1,503.01 \\
\multirow{3}{*}{3} & Not Priced & 20 & 2,895,300.75 & 33,249.40 & 171,155 & 5 & 1,831.11 \\
                   & Restricted Priced & 10 & 431,913.00 & 15,895.80 & 174,569 & 5 & 2,513.80 \\
\vspace{1.5mm}     & Priced & 5 & 372,597.00 & 13,287.80 & 179,712 & 4 & 1,728.08 \\
\multirow{3}{*}{4} & Not Priced & 20 & 3,201,374.45 & 35,197.10 & 167,776 & 7 & 3,910.89 \\
                   & Restricted Priced & 10 & 497,802.20 & 17,011.00 & 197,047 & 2 & 1,323.65 \\
                   & Priced & 2 & 211,093.00 & 14,227.00 & 199,477 & 2 & 2,538.79 \\\hline\hline
\end{tabular}
\legend{
\justifying
Summary table for the instances proposed in~\cite{velasco:2019}.
The columns are:
\emph{C.} -- instance class (described in~\cite{velasco:2019}, 20 instances each);
\emph{Variant} -- the solving method employed;
\emph{\#m} (modeled) -- number of instances in which the model was built before timeout;
\emph{Avg. \#v} and \emph{Avg. \#p} -- the average number of variables and plates in the \emph{\#m} instances that generated a final model for the respective variant;
\emph{T. T.} (Total Time) -- sum of the time spent in all instances in seconds, including timeouts;
\emph{\#s} (solved) -- number of instances solved;
\emph{Avg. S. T.} (Avg. Solved Time) -- as total time but excludes timeouts and divides by \emph{\#s}.
Averages were used instead of simple sums because the very different number of generated and solved models made the sums misleading.
Source: the author.}
\label{tab:velasco_summary}
\end{table}

Concerning the data from~\cref{tab:velasco_summary}, the author wants to highlight some unexpected results:
(i) the total number of instances solved by the \emph{restricted priced} was slightly smaller than non-\emph{priced}, even with non-\emph{priced} solving the harder \emph{unrestricted} problem;
(ii) many runs reached time limit without solving the continuous relaxation of the \emph{restricted} model (necessary for creating \emph{restricted priced} model);
(iii) non-\emph{priced} solved more instances than \emph{priced} in all cases.
\newtext{It is worth noting that the \emph{priced} variant could have been considered the best configuration in the previous dataset, as its total time was shorter than non-\emph{priced} (both solved all instances).}
Ideally, the pricing procedure would significantly reduce the size of the model and, consequently, the root node relaxation and B\&B phases would take much less time to solve.
However, the gain in decreasing the size of the (already reduced) \emph{enhanced} model further does not seem to compensate for the cost of solving hard LPs more than once.
Also, previous sections have shown that reducing the model size does not guarantee that the running time will be reduced by the same magnitude.

\begin{table}[!ht]
\centering
\let\mc\multicolumn
\rowcolors{3}{white}{gray-table-row}
\caption{V\&U instances either solved (restricted or unrestricted) or with improved bounds. (PART I)}
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrrrrr}
\hline\hline
\hiderowcolors
Instance & \mc4c{Lower Bounds for Unrestricted} & RP UB & \mc3c{Upper Bounds for Unr.} \\\cline{2-5}\cline{7-9}
 & \mc1c{RP} & \mc1c{P} & \mc1c{NP} & \mc1c{V\&U} & & \mc1c{P} & \mc1c{NP} & \mc1c{V\&U} \\\hline
\showrowcolors
P1\_100\_200\_25\_1 & \underline{\textbf{27,251}} & \underline{\textbf{27,251}} & \underline{\textbf{27,251}} & \textbf{27,251} & \underline{27,251} & \underline{\textbf{27,251}} & \underline{\textbf{27,251}} & 27,340 \\
P1\_100\_200\_25\_2 & \underline{\textbf{25,090}} & \textbf{25,090} & \textbf{25,090} & 24,870 & \underline{25,090} & 25,403 & \textbf{25,389} & 25,522 \\
P1\_100\_200\_25\_3 & \underline{\textbf{25,730}} & \textbf{25,730} & \textbf{25,730} & \textbf{25,730} & \underline{25,730} & 25,974 & \textbf{25,909} & 26,088 \\
P1\_100\_200\_25\_4 & \underline{26,732} & \underline{\textbf{26,896}} & \underline{\textbf{26,896}} & 26,769 & \underline{26,732} & \underline{\textbf{26,896}} & \underline{\textbf{26,896}} & 27,051 \\
P1\_100\_200\_25\_5 & \textbf{26,152} & -- & \textbf{26,152} & 25,772 & 26,565 & -- & \textbf{26,617} & 26,857 \\
P1\_100\_200\_50\_1 & 28,388 & -- & \underline{\textbf{28,440}} & 28,388 & 28,504 & -- & \underline{\textbf{28,440}} & 28,558 \\
P1\_100\_200\_50\_2 & \underline{\textbf{26,276}} & \underline{\textbf{26,276}} & \underline{\textbf{26,276}} & \textbf{26,276} & \underline{26,276} & \underline{\textbf{26,276}} & \underline{\textbf{26,276}} & 26,326 \\
P1\_100\_200\_50\_3 & \textbf{27,192} & -- & \textbf{27,192} & 27,165 & 27,536 & -- & \textbf{27,483} & 27,679 \\
P1\_100\_200\_50\_4 & 28,058 & -- & \textbf{28,095} & 27,977 & 28,345 & -- & \textbf{28,340} & 28,388 \\
P1\_100\_200\_50\_5 & \textbf{27,722} & -- & \underline{\textbf{27,722}} & 27,603 & 27,930 & -- & \underline{\textbf{27,722}} & 28,009 \\
P1\_100\_400\_25\_1 & 53,247 & -- & 53,008 & \textbf{53,904} & 54,540 & -- & \textbf{54,707} & 55,038 \\
P1\_100\_400\_25\_2 & -- & -- & 41,275 & \textbf{44,581} & -- & -- & \textbf{47,091} & 47,097 \\
P1\_100\_400\_25\_3 & 42,748 & -- & 46,222 & \textbf{47,455} & \textbf{\large \textasteriskcentered} & -- & \textbf{49,371} & 49,473 \\
P1\_100\_400\_25\_4 & -- & -- & 38,567 & \textbf{40,517} & -- & -- & \textbf{46,069} & 46,078 \\
P1\_100\_400\_25\_5 & 44,482 & -- & \textbf{53,220} & 53,205 & \textbf{\large \textasteriskcentered} & -- & 54,120 & \textbf{54,063} \\
P1\_100\_400\_50\_1 & -- & -- & 53,831 & \textbf{55,856} & -- & -- & \textbf{56,897} & 57,074 \\
P1\_100\_400\_50\_2 & -- & -- & 40,440 & \textbf{48,373} & -- & -- & \textbf{51,754} & 51,893 \\
P1\_100\_400\_50\_4 & -- & -- & \textbf{55,107} & 52,708 & -- & -- & \textbf{55,654} & 55,661 \\
P1\_100\_400\_50\_5 & -- & -- & \textbf{53,749} & 53,502 & -- & -- & \textbf{55,005} & 55,454 \\\hline\hline
\end{tabular}
} % resizebox
\legend{
\justifying
Instances solved (\newtext{position-only} restricted or unrestricted) or with improved bounds.
Lower and upper bounds that are valid for the unrestricted problem are grouped.
Column \emph{RP UB} (restricted priced upper bound) is kept separate as it is not a valid bound for the unrestricted problem.
Bold indicates the best unrestricted bounds for the instance.
If the LB and the UB are the same for the same instance and variant, both values are underlined.
\newtext{The instance names follow the pattern \texttt{Class_L_W_n_seed}.}
The sub-headers mean:
\emph{RP} -- Restricted Priced (solved inside \emph{P} runs);
\emph{P} -- Priced;
\emph{NP} -- Not Priced;
\emph{V\&U} -- obtained by Velasco and Uchoa in~\cite{velasco:2019}.
\newline\textbf{\large \textasteriskcentered} These runs hit the time limit at the very start of the upper bound computation and, consequently, they produced only large and irrelevant upper bounds, which the author chose to omit to keep the table formatting.}
\label{tab:velasco_new_results_I}
\end{table}

The purpose of \cref{tab:velasco_new_results_I,tab:velasco_new_results_II,tab:velasco_new_results_III,tab:velasco_new_results_VI} is to allow querying the exact values for specific instances.
Even so, there are some gaps to fill.
For the instances presented in \cref{tab:velasco_new_results_I,tab:velasco_new_results_II,tab:velasco_new_results_III,tab:velasco_new_results_VI},
the min / mean / max gap between the heuristic lower bound and the final lower bound were: 0.38 / 18.08 / 37.03 (non-\emph{priced}); 0.68 / 20.62 / 37.29 (\emph{restricted priced}); 9.17 / 19.38 / 32.24 (\emph{priced}).
In other words, no solution, or best bound, was given by the heuristic, and most of the time, its solution was considerably improved.
For the reader convenience, it can be said that this experiment has:
proved 22 unrestricted optimal values (5 already proven by~\citet{velasco:2019}, confirming their results);
proved 22 \emph{position-only} restricted optimal values (in an overlapping but distinct subset of the instances);
improved lower bounds for 25 instances;
improved upper bounds for 58 instances.

\begin{table}[!ht]
\centering
\let\mc\multicolumn
\rowcolors{3}{white}{gray-table-row}
\caption{V\&U instances either solved (restricted or unrestricted) or with improved bounds. (PART II)}
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrrrrr}
\hline\hline
\hiderowcolors
Instance & \mc4c{Lower Bounds for Unrestricted} & RP UB & \mc3c{Upper Bounds for Unr.} \\\cline{2-5}\cline{7-9}
 & \mc1c{RP} & \mc1c{P} & \mc1c{NP} & \mc1c{V\&U} & & \mc1c{P} & \mc1c{NP} & \mc1c{V\&U} \\\hline
\showrowcolors
P2\_200\_100\_25\_1 & \underline{\textbf{21,494}} & \underline{\textbf{21,494}} & \underline{\textbf{21,494}} & \underline{\textbf{21,494}} & \underline{21,494} & \underline{\textbf{21,494}} & \underline{\textbf{21,494}} & \underline{\textbf{21,494}} \\
P2\_200\_100\_25\_2 & \underline{25,244} & \underline{\textbf{25,413}} & \underline{\textbf{25,413}} & \textbf{25,413} & \underline{25,244} & \underline{\textbf{25,413}} & \underline{\textbf{25,413}} & 25,648 \\
P2\_200\_100\_25\_3 & \underline{25,282} & \textbf{25,397} & \textbf{25,397} & \textbf{25,397} & \underline{25,282} & \textbf{25,640} & 25,647 & 25,723 \\
P2\_200\_100\_25\_4 & 25,729 & -- & \textbf{25,734} & 25,437 & 26,181 & -- & \textbf{26,239} & 26,898 \\
P2\_200\_100\_25\_5 & \underline{26,211} & \textbf{26,413} & \underline{\textbf{26,413}} & 26,220 & \underline{26,211} & 26,728 & \underline{\textbf{26,413}} & 26,898 \\
P2\_200\_100\_50\_1 & \textbf{25,679} & -- & 25,626 & 25,627 & 26,233 & -- & \textbf{26,282} & 26,447 \\
P2\_200\_100\_50\_2 & \underline{\textbf{27,801}} & \underline{\textbf{27,801}} & \underline{\textbf{27,801}} & 27,789 & \underline{27,801} & \underline{\textbf{27,801}} & \underline{\textbf{27,801}} & 27,943 \\
P2\_200\_100\_50\_3 & \underline{27,435} & \textbf{27,453} & \textbf{27,453} & \textbf{27,453} & \underline{27,435} & 27,584 & \textbf{27,579} & 27,596 \\
P2\_200\_100\_50\_4 & 27,395 & -- & \textbf{27,439} & 27,362 & 27,668 & -- & \textbf{27,704} & 27,718 \\
P2\_200\_100\_50\_5 & \underline{\textbf{29,386}} & \underline{\textbf{29,386}} & \underline{\textbf{29,386}} & \underline{\textbf{29,386}} & \underline{29,386} & \underline{\textbf{29,386}} & \underline{\textbf{29,386}} & \underline{\textbf{29,386}} \\
P2\_400\_100\_25\_1 & 49,327 & -- & \textbf{49,947} & 49,026 & 50,218 & -- & \textbf{50,365} & 51,006 \\
P2\_400\_100\_25\_2 & 48,312 & -- & \textbf{48,542} & 47,773 & 49,268 & -- & \textbf{49,315} & 49,908 \\
P2\_400\_100\_25\_3 & \textbf{46,970} & -- & 46,860 & 45,406 & 47,113 & -- & \textbf{47,204} & 48,938 \\
P2\_400\_100\_25\_4 & \textbf{51,051} & -- & 49,847 & 49,521 & 51,526 & -- & \textbf{51,600} & 52,229 \\
P2\_400\_100\_25\_5 & \textbf{49,620} & -- & 48,832 & 47,403 & 50,440 & -- & \textbf{50,580} & 54,248 \\
P2\_400\_100\_50\_1 & \underline{54,550} & 54,550 & \textbf{54,679} & 52,890 & \underline{54,550} & 54,981 & \textbf{54,916} & 55,629 \\
P2\_400\_100\_50\_2 & \textbf{54,821} & -- & 54,768 & 53,492 & 55,183 & -- & \textbf{55,181} & 55,543 \\
P2\_400\_100\_50\_3 & 54,141 & -- & \textbf{54,747} & 54,216 & 55,537 & -- & \textbf{55,709} & 56,065 \\
P2\_400\_100\_50\_4 & 53,375 & -- & \textbf{54,240} & 48,649 & 54,857 & -- & \textbf{54,987} & 55,604 \\
P2\_400\_100\_50\_5 & \textbf{53,763} & -- & 53,541 & 50,047 & 54,893 & -- & \textbf{54,918} & 55,471 \\\hline\hline
\end{tabular}
} % resizebox
\legend{Table organization is the same as~\cref{tab:velasco_new_results_I}. Source: the author.}
\label{tab:velasco_new_results_II}
\end{table}

\begin{table}[!ht]
\centering
\let\mc\multicolumn
\rowcolors{3}{white}{gray-table-row}
\caption{V\&U instances either solved (restricted or unrestricted) or with improved bounds. (PART III)}
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrrrrr}
\hline\hline
\hiderowcolors
Instance & \mc4c{Lower Bounds for Unrestricted} & RP UB & \mc3c{Upper Bounds for Unr.} \\\cline{2-5}\cline{7-9}
 & \mc1c{RP} & \mc1c{P} & \mc1c{NP} & \mc1c{V\&U} & & \mc1c{P} & \mc1c{NP} & \mc1c{V\&U} \\\hline
\showrowcolors
P3\_150\_150\_25\_1 & \underline{29,896} & \underline{\textbf{29,989}} & \underline{\textbf{29,989}} & 29,896 & \underline{29,896} & \underline{\textbf{29,989}} & \underline{\textbf{29,989}} & 30,005 \\
P3\_150\_150\_25\_2 & \textbf{29,345} & -- & 29,196 & 29,101 & 29,906 & -- & 29,965 & \textbf{29,961} \\
P3\_150\_150\_25\_3 & \underline{\textbf{30,286}} & \underline{\textbf{30,286}} & \underline{\textbf{30,286}} & \textbf{30,286} & \underline{30,286} & \underline{\textbf{30,286}} & \underline{\textbf{30,286}} & 30,327 \\
P3\_150\_150\_25\_5 & \underline{\textbf{31,332}} & \textbf{31,332} & \textbf{31,332} & 30,924 & \underline{31,332} & 31,715 & \textbf{31,682} & 31,839 \\
P3\_150\_150\_50\_1 & \underline{31,377} & \underline{\textbf{31,701}} & \underline{\textbf{31,701}} & \textbf{31,701} & \underline{31,377} & \underline{\textbf{31,701}} & \underline{\textbf{31,701}} & 31,892 \\
P3\_150\_150\_50\_2 & 30,846 & -- & \textbf{30,884} & \textbf{30,884} & 31,110 & -- & \textbf{31,008} & 31,115 \\
P3\_150\_150\_50\_3 & \underline{32,037} & \underline{\textbf{32,121}} & \underline{\textbf{32,121}} & 32,050 & \underline{32,037} & \underline{\textbf{32,121}} & \underline{\textbf{32,121}} & 32,240 \\
P3\_150\_150\_50\_4 & \textbf{31,925} & -- & \underline{\textbf{31,925}} & \textbf{31,925} & 32,210 & -- & \underline{\textbf{31,925}} & 32,070 \\
P3\_150\_150\_50\_5 & \textbf{31,631} & -- & 31,521 & 31,448 & 31,857 & -- & \textbf{31,896} & 31,901 \\
P3\_250\_250\_25\_1 & -- & -- & 51,027 & \textbf{58,480} & -- & -- & \textbf{60,548} & 60,611 \\
P3\_250\_250\_25\_2 & -- & -- & 63,646 & \textbf{68,070} & -- & -- & \textbf{73,316} & 73,339 \\
P3\_250\_250\_50\_1 & -- & -- & 59,072 & \textbf{67,603} & -- & -- & \textbf{76,117} & 76,341 \\
P3\_250\_250\_50\_2 & -- & -- & 62,772 & \textbf{75,569} & -- & -- & \textbf{82,644} & 82,666 \\\hline\hline
\end{tabular}
} % resizebox
\legend{Table organization is the same as~\cref{tab:velasco_new_results_I}. Source: the author.}
\label{tab:velasco_new_results_III}
\end{table}

\begin{table}[!ht]
\centering
\let\mc\multicolumn
\rowcolors{3}{white}{gray-table-row}
\caption{V\&U instances either solved (restricted or unrestricted) or with improved bounds. (PART IV)}
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrrrrr}
\hline\hline
\hiderowcolors
Instance & \mc4c{Lower Bounds for Unrestricted} & RP UB & \mc3c{Upper Bounds for Unr.} \\\cline{2-5}\cline{7-9}
 & \mc1c{RP} & \mc1c{P} & \mc1c{NP} & \mc1c{V\&U} & & \mc1c{P} & \mc1c{NP} & \mc1c{V\&U} \\\hline
\showrowcolors
P4\_150\_150\_25\_1 & 30,870 & -- & \underline{\textbf{30,923}} & \textbf{30,923} & 31,094 & -- & \underline{\textbf{30,923}} & 31,130 \\
P4\_150\_150\_25\_2 & 30,576 & -- & \underline{\textbf{30,687}} & 30,460 & 30,786 & -- & \underline{\textbf{30,687}} & 30,931 \\
P4\_150\_150\_25\_3 & 30,257 & -- & \underline{\textbf{30,352}} & \underline{\textbf{30,352}} & 30,501 & -- & \underline{\textbf{30,352}} & \underline{\textbf{30,352}} \\
P4\_150\_150\_25\_4 & \underline{30,055} & \underline{\textbf{30,106}} & \underline{\textbf{30,106}} & \underline{\textbf{30,106}} & \underline{30,055} & \underline{\textbf{30,106}} & \underline{\textbf{30,106}} & \underline{\textbf{30,106}} \\
P4\_150\_150\_25\_5 & \textbf{30,582} & -- & 30,102 & \textbf{30,582} & 30,952 & -- & \textbf{31,228} & 31,286 \\
P4\_150\_150\_50\_1 & \underline{\textbf{31,673}} & \underline{\textbf{31,673}} & \underline{\textbf{31,673}} & \underline{\textbf{31,673}} & \underline{31,673} & \underline{\textbf{31,673}} & \underline{\textbf{31,673}} & \underline{\textbf{31,673}} \\
P4\_150\_150\_50\_2 & 32,302 & -- & \underline{\textbf{32,317}} & \textbf{32,317} & 32,434 & -- & \underline{\textbf{32,317}} & 32,423 \\
P4\_150\_150\_50\_3 & 30,906 & -- & \textbf{30,913} & 30,882 & 31,500 & -- & \textbf{31,519} & 31,756 \\
P4\_150\_150\_50\_4 & 31,912 & -- & \underline{\textbf{31,961}} & 31,912 & 32,206 & -- & \underline{\textbf{31,961}} & 32,140 \\
P4\_150\_150\_50\_5 & \textbf{32,027} & -- & 31,845 & 31,864 & 32,331 & -- & \textbf{32,308} & 32,484 \\
P4\_250\_250\_25\_4 & -- & -- & 69,530 & \textbf{79,476} & -- & -- & \textbf{81,634} & 81,839 \\
P4\_250\_250\_50\_2 & -- & -- & 67,675 & \textbf{77,206} & -- & -- & \textbf{87,314} & 87,331 \\
P4\_250\_250\_50\_4 & -- & -- & 69,063 & \textbf{78,359} & -- & -- & \textbf{86,941} & 87,069 \\\hline\hline
\end{tabular}
} % resizebox
\legend{Table organization is the same as~\cref{tab:velasco_new_results_I}. Source: the author.}
\label{tab:velasco_new_results_VI}
\end{table}

\input{comparison_to_martin.tex}

\chapter{\protect\newtext{Hybridisation with the restricted formulation}}
\label{sec:hybridisation}

This chapter proposes another symmetry-breaking change compatible with the formulations considered in~\cref{sec:enhanced_model}, this is, FMT and the proposed formulation (BBA).
This change further complicates the formulation, and the empirical results did not reveal an improvement as large as the previously discussed enhancements.
Therefore, the author chose to keep this change self-contained in this chapter.
The author is unaware of any previous application of the proposed change to unrestricted 2D guillotine problems.
The Cut-Position enhancement from~\citet{furini:2016} draws inspiration from the same broad idea: to get closer to a formulation for the (simpler) restricted problem while keeping optimality for the unrestricted problem.
However, the proposed change and the Cut-Position both approach this goal in distinct and complementary ways.

\section{The restricted problem and piece-outlining cuts}

A guillotine cutting problem is said to be \emph{restricted} if (i) each horizontal (vertical) guillotine cut must match the length (width) of a piece that fits into the plate, i.e., it happens at a \emph{restricted cut position}, and (ii) a piece of that length (width) is guaranteed to be obtained from the first child plate.
The concept of a \emph{restricted} variant appears first in the context of the three-staged guillotine cutting problem.
The two-staged problem is inherently restricted: a cut that does not match the outline of a piece, or a cut that does not guarantee a piece extraction because it is not paired with a cut from the only other stage, is a cut that will not help to obtain any pieces before the two stages are over.
Only when the number of stages is three or more that an optimal solution for the unrestricted problem may require cuts without such immediate purposes.
Applying the concept of \emph{restricted} to unlimited stages is not new, \citet{furini:2016} already does it.
\citet{furini:2016} also presents an intermediary variant which respects (i) but not (ii), this variant can be referred to as \emph{position-only} restricted problem.
The \emph{position-only restricted problem} is the one solved by the \emph{restricted priced} in~\cref{sec:furini_vs_enhanced_comparison}.

% The (position-only) restricted problem may be required (or prefered) because of precision problems dependent on available cutting technology~\citep{furini:2016}, however it is also used an as high-cost high-quality heuristic for the unrestricted problem.

The restricted problem has at least two performance advantages over the unrestricted problem.
The first advantage is related to the number of restricted cut positions: the number of cuts positions in any plate is bounded by the number of pieces (i.e., linear on the input) and not pseudo-polynomial (i.e., bounded by plate dimensions), even if the number of plates themselves is still pseudo-polynomial.
The second advantage is related to the piece extraction requirement.
There is no optimality loss if, after a cut at a restricted position related to a single piece, it is immediately determined that, if necessary, the first child plate will be cut again in the next stage to obtain the respective piece.
The possibility of joining two decision variables together has led previous prior on the restricted problem, as \citet{silva:2010}, to redefine \emph{cut} to mean \emph{one or two guillotine cuts associated a priori to a piece type and which outline and obtain a piece-sized plate that cannot be further cut}.
The guillotine cuts considered until now may incidentally outline and obtain a piece-sized plate as their child plates.
However, they are not a priori associated with a single piece type, nor do they guarantee their first child plate (if piece-sized) cannot be further cut.
In this chapter, the text distinguishes between these two kinds of cuts to avoid confusion.
The single and unassociated cuts considered until now will be referred to as \emph{basic guillotine cuts} (or BGCs for short), and this new definition of cut will be referred to as \emph{piece-outlining cuts} (or POCs for short).
\Cref{fig:piece_outlining_cut} may help to visualise the \emph{piece-outlining cuts}

\begin{figure}[h]
  \caption{Piece-outlining cuts}
  \center
  \input{diagrams/piece_outlining_cut.tex}
  \legend{Souce: the author.}
  \label{fig:piece_outlining_cut}
\end{figure}

While a POC constituted by two BGCs may be considered a single decision by a solving method and may be seen as happening in succession, in practice, stage restrictions may change the order a cutting machine performs them.
However, these real-world details do not impact the modelling and will not be discussed in this chapter.
Essentially, each piece type that fits into a plate has two POCs associated with it.
One POC that does the horizontal guillotine cut first and then obtains the piece from the first child plate through a vertical cut (if necessary).
This POC always leaves a \emph{top residual plate} (second child plate of the first cut) and often a \emph{right residual plate} (second child plate of the second cut).
The other POC is the same, except that the vertical cut is done first (i.e., always leaving a right residual and often a top residual plate).
Finally, the piece-sized plate obtained by a POC is the first child plate of the second cut if the second cut exists; otherwise, just the first child plate of the only cut.
The piece-sized plate is either immediately regarded as an obtained piece (already enforcing a rule of the restricted problem) or may be considered waste (e.g., the cutting stock problem often allows piece overproduction).
However, the piece-sized plate is \emph{never} treated as an intermediary plate that could be further cut.

A caveat of the coupled representation mentioned above is that, for some instances of the restricted problem, the number of POCs may be larger than the number of restricted cut positions.
In general, each piece type that fits into a plate has two POCs\footnote{The exception happens when the piece type shares the length or the width with the plate and, consequently, both POCs are equivalent and can be considered the same.} (vertical-first and horizontal-first).
An horizontal (vertical) BGC at a restricted position is shared by all piece types with the same length (width).
However, the main advantage of the coupled representation comes from breaking symmetries, not reducing the number of variables.
%For example, if a stripe of width~\(10\) is obtained by a BGC in the restricted problem, it may be used to obtain a single piece of width~\(10\) and four pieces of width~\(8\), and every permutation in the order of the pieces are obtained from the strip is a symmetry, the POC enforce the width~\(10\) piece is the first to be obtained, and that no other mechanism is necessary to guarantee that the piece will be obtained from such plate.

The POCs are a natural choice for the \emph{restricted} problem but not for the \emph{unrestricted} problem for mostly two main reasons.
The first reason is that, in the restricted problem, each horizontal (vertical) cutting position shares length (width) with at least one piece.
However, in the unrestricted problem, some cutting positions can only be reached by combining many pieces.
The second reason is that the definition of the \emph{restricted} problem guarantees that employing only POCs cannot lead to optimality loss; the same is not true for the unrestricted problem (see \cref{fig:distinctions_restricted_unrestricted}).

\begin{figure}[h]
  \caption{Distinctions between, restricted, position-only restricted, and unrestricted problems.}
  \center
  \input{diagrams/distinctions_restricted_unrestricted.tex}
  \legend{
\justifying
The restricted problem cannot obtain the unrestricted optimal solution. If the first cut happens at a restricted position, the child plates cannot fit the six pieces of the optimal solution, regardless of the piece chosen to be obtained first from the original plate and the orientation of the first cut employed. The position-only restricted problem can obtain the unrestricted optimal solution if, by chance, there is an unpacked piece with a width that matches the necessary vertical cut; otherwise, the solution is also out of reach. Source: the author.
}
  \label{fig:distinctions_restricted_unrestricted}
\end{figure}

\citet{silva:2010}~proposes a mathematical formulation for the two-stage and three-stage restricted cutting stock problems.
The formulation was not named by its authors; hence, in this text, it will be referred to as SAV (from the author's surname initials: Silva, Alvelos, and Valério).
The SAV is very similar to the FMT, which is examined in~\cref{sec:furini_model}.
In fact, the SAV may be seen as an FMT variant that uses POCs instead of BGCs.
The limitation to two- and three-stage problems comes from the cut-and-plate enumeration.
If the enumeration is not stopped at a specific stage, the SAV immediately supports unlimited stages.
Essentially, the proposed change is to: hybridise the FMT with the SAV, replacing BGCs with POCs \emph{only} when doing so cannot lead to loss of optimality for the unrestricted problem.
%Next section presents the implementation details for this change.

\section{Implementation details}

As seen in the last section, a POC (\emph{piece-oulining cut}) is prefered over a BGC (\emph{basic guillotine cut}) if it is guaranteed that replacing the latter by the former will not cause loss of optimality.
For the restricted problem, the typical set of horizontal (vertical) cutting positions is just the set of unique values in~\(l_i\) (\(w_i\)) for every piece type~\(i\) that fits into the plate.
Besides one corner case, each single guillotine cut at such positions may be replaced by the corresponding POC.
The corner case arises in cutting positions that come from a length (or width) value shared by two or more pieces.
In this case, a single guillotine cut needs to be replaced by two or more POCs, depending on how many pieces share the corresponding cutting position; otherwise, the model would lose the capability to produce that piece type.

For the unrestricted problem, the exact set of cutting positions often varies between different solving methods.
There are many discretisation procedures (see \cref{sec:enhanced_model}) and reductions to be applied either after or during such discretisations.
The author will focus on the discretisations and reductions procedures employed by the formulations of~\cref{sec:furini_vs_enhanced_comparison} (this is, the FMT and BBA formulations).
The base discretisation employed by both FMT and BBA is straightforward: \(q\) is an horizontal (vertical) cutting position if, and only if, there is a demand-abiding linear combination of lengths (widths) from pieces that fit into the respective plate.
This cutting position set is a superset of the restricted set (from the last paragraph) and will be referred to as the \emph{base unrestricted set}.
Suppose a cutting position allows for the associated BGC to be replaced by (one or more) POCs without loss of optimality for the \emph{unrestricted problem}.
In that case, the cutting position (and, by extension, the BGC) is said to be \emph{replaceable}.
%The set of cutting positions for which the associated single guillotine cuts may be replaced by POCs, without loss of optimality in the context of the \emph{unrestricted problem}, will be referred to as the \emph{replaceable set} henceforth.

A cutting position must meet two conditions to be deemed replaceable.
The first condition is that a cutting position of the same orientation for the same plate exists in the restricted set.
This first condition is necessary because, otherwise, the cut is not outlining a piece, i.e., there is no corresponding piece type to be extracted from the first child plate.
The second condition is that such horizontal (vertical) cutting positions cannot be obtainable by a demand-abiding linear combination of two or more piece lengths (widths), considering only the pieces that fit into the respective plate.
This second condition is necessary because, otherwise, the replaced cut could be necessary for the only optimal cutting pattern of an instance of the unrestricted problem.
An example of this situation can be seen in~\cref{fig:distinctions_restricted_unrestricted} (the middle pattern, i.e., Position-only Restricted).
The middle vertical cut matches a piece width (i.e., it satisfied the first condition); however, if it were replaced by a POC associated with the square piece, it would be impossible to obtain the unrestricted optimal solution (that needs a BGC at the same position).
%the sum of lengths (widths) for a piece multiset in which (a) each piece type multiplity respects the demand, (b) each piece type fits into the first child plate, and (c) the multiset has a cardinality of two or more.
% NOTE: probably we need an image with three diagrams, two of them equal to the introduction ones and a third image showing the possibility of the cut if there is an out-of-the-pattern piece type that has the same size as two other summed.

The two reductions proposed in~\citet{furini:2016}, \emph{Cut-Position} and \emph{Redundant-Cut}, cause little change to the replaceable cutting positions.
Both reductions are briefly described at the start of~\cref{sec:var_enum}.
The only cutting positions removed by \emph{Cut-Position} are the ones not in the restricted set and, therefore, not replaceable.
Moreover, if the cutting position set of a plate is reduced by \emph{Cut-Position} and the kept positions are all replaced with POCs, then that plate and any plate strictly smaller than it will, in fact, be solved by the SAV formulation instead of the FMT formulation.
\emph{Redundant-Cut} may remove a replaceable cutting position.
However, the predicted alternative cutting position from a larger plate will always be replaceable too, and replacing it with one or more POCs never requires adding back the cuts removed by Redundant-Cut.
Also, the BBA formulation never has trim cuts like those removed by Redundant-Cut (see \cref{sec:enhanced_model}), so this enhancement is superseded by it.

BBA adds extraction variables and reduces the base unrestricted set to only the cutting positions up to the midplate.
The extraction variables can be seen as POCs in which both top and right residual plates are guaranteed to be waste; therefore, extractions are not subject to be replaced by POCs.
BBA requires us to differentiate between \emph{binding} and \emph{non-binding} POCs.
A POC is \emph{non-binding} if the piece-sized plate it obtains may be regarded as waste; conversely, if the piece-sized plate must be sold as a piece, then the POC is \emph{binding}.
A \emph{binding} POC cannot be employed if an extra copy of the associated piece type would lead to disrespecting the demand constraint.
If replaceable cuts in the BBA formulation are replaced by \emph{binding} POCs, then there are cases in which loss of optimality occurs.
The cause of this loss of optimality is that, in BBA, a replaceable cut may be required by an optimal solution even if there is no demand for the associated piece.
These seemingly unnecessary cuts aim to reduce the plate size until a large piece can be obtained from the plate through an extraction variable.
A complete example follows.

\begin{example}{Hybridised BBA with binding cuts loses optimality.}
Consider the following G2KP instance: \(L = 100\), \(W = 100\), \(l = [100, 100]\), \(w = [1, 51]\), \(u = [1, 1]\), and \(p = [1, 1]\).
The optimal solution clearly must contain the only available copy of each of the two piece types.
In BBA, there is no cut after the midplate; consequently, a vertical cut at position~\(51\) is ruled out.
The only possibility is a vertical cut at position~\(1\) for which the first child plate could be immediately sold as the single copy of the first piece type.
The second child plate (\(100\)x\(99\)) also does not have an extraction variable for the immediate extraction of the second piece type (\(100\)x\(51\)).
The BBA determines that for an extraction variable to exist ``[...] the plate cannot fit an extra piece (of any type).'' and the first piece type fits together with the second in the \(100\)x\(99\) plate.
Again, a vertical cut at position~\(51\) is unavailable because it happens after midplate.
Consequently, BBA forces the optimal solution to create~\(50\) plates of size~\(100\)x\(1\), one of which will be sold as a piece, and the rest considered waste.
The second child of the~\(50\)th (and last) cut has size~\(100\)x\(50\), and it can be sold as the second piece type because an extraction variable is now available (i.e., the previously quoted condition does not apply anymore).
The adoption of \emph{binding} POCs makes it impossible for BBA to obtain an optimal solution for this example.
The reason is that there are not~\(50\) copies of the first piece type, but these would be needed by the~\(50\) binding piece-outlining cuts necessary to obtain an optimal solution.
The same problem does not arise if the POCs are not binding.
\end{example}

The corner case of two or more pieces sharing the same length/width needs to be considered in the unrestricted problem too, but with a subtle distinction.
In the restricted problem, replacing every single guillotine cut by POCs also brings the advantage of not needing an additional mechanism to enforce the problem definition (i.e., to guarantee piece extractions from the first child plates).
However, in the unrestricted problem, the choice between replacing a single guillotine cut by multiple POCs, or keeping it as a guillotine cut, is just a trade-off between model size and model symmetry.
Therefore, this work further distinguishes between two implementations of hybridisation.
The \emph{conservative} hybridisation substitutes each replaceable horizontal (vertical) BGC with one horizontal-first (vertical-first) POC that is associated with the single piece type that matches the length (width) of the cutting position (and that fits into the respective plate).
If two or more fitting piece types match the cutting position, the conservative hybridisation leaves the BGC unchanged.
The \emph{aggressive} hybridisation substitutes each replaceable horizontal (vertical) BGC with one horizontal-first (vertical-first) POC \emph{for each piece type} that matches its length (width) (and that fits into the respective plate).
%enhancement based on how they deal with the corner case of multiple pieces sharing length or width.
%The distinction between aggressive and conservative is made mostly during cut and plate enumeration, and depending on implementation details, the model formulation may be kept blind to it, i.e., only taking into account which cuts are BGCs and which are POCs.

The author believes it is excessive to present the full formulation and implementation details for every combination of the FMT/BBA formulation with conservative/aggressive hybridisation and binding/non-binding POCs.
The experiments in the next section only consider the BBA with conservative/aggressive hybridisation and non-binding POCs.
The distinction between conservative and aggressive hybridisation is mostly made at the cut and plate enumeration; however, because of an unfortunate notation detail explained further, it is less troublesome to present an accurate formulation of the conservative hybridisation than the aggressive hybridisation.
In light of this, the author chose to fully present the conservative hybridised BBA formulation with non-binding POCs.
The main differences in implementing other combinations are briefly discussed shortly after.

The \emph{conservative hybridised BBA formulation with non-binding} POCs requires a new set of variables, a new set of constraints, a new parameter, and some minor changes to the objective function and some of the existing constraints.
Both the new set of variables and the new set of constraints are bounded by \(|\bar{J}|\) and, therefore, cause only a small relative increase to the model size of a non-trivial instance.
The notation for the new variable and parameter set follows:

\begin{description}
\item [\(s_i\)] \(\forall i \in \bar{J}\) -- Integer variable. Indicates how many piece-sized plates obtained by POCs associated with piece type~\(i\) were sold as pieces of type~\(i\). By \emph{sold} the author means they contributed to the objective function and were accounted for by the demand constraint.
\item [\(h^o_{qji}\)] \(\forall o \in O, j \in J, q \in Q_{jo}, i \in \bar{J}\) -- Binary parameter. Byproduct of the cut and plate enumeration. It has value one if cut~\(x^o_{qj}\) is a POC that produces a piece-sized plate corresponding to piece~\(i\); zero otherwise.%For a cut~\(x^o_{qj}\), \(\sum_{i\in\bar{J}} h^o_{qji}\) is zero if the cut is a BGC, and one if the cut is a POC.
\end{description}

Some variables, parameters, and constraints need just a little reinterpretation or no change at all.
The already established parameter \(a^o_{qkj}\) is exactly the same for BGCs and has a slightly different meaning for POCs.
The difference is that the \(j\) (obtained child plate) is always either the top or right residual (i.e., the POC version of the first and second child) and that both \(o\) (orientation) and \(q\) (cutting position) refer only to the first constituting cut of a POC; the meaning of \(k\) (parent plate) is left unchanged.
The set of variables representing cuts (\(x^o_{qj}\)) also does not need change, as \(h^o_{qji}\) fills the need to identify POCs and their associated piece types. Consequently, the constraints~\eqref{eq:plates_conservation} and~\eqref{eq:just_one_original_plate} presented below are the same as the non-hybridised formulation.

The constraint~\eqref{eq:piece_sized_plates} guarantees each piece-sized plate available~(\(s_i\)) comes from an actual POC.
The remaining changes consist into adding \(s_i\) to the demand constraint~\eqref{eq:hyb_demand} (which avoids overproduction without prohibiting the POCs themselves) and to the objective function~\eqref{eq:hyb_obj} (which allows piece-sized plates to be sold).

% TODO: check if labels are correct

\begin{align}
\bm{max.} &\sum_{(i, j) \in E} p_i e_{ij} + \sum_{i \in \bar{J}} p_i s_i \label{eq:hyb_obj}\\
\bm{s.t.} &\specialcell{\sum_{o \in O}\sum_{q \in Q_{jo}} x^o_{qj} + \sum_{i \in E_{*j}} e_{ij} \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qkj} x^o_{qk} \hspace*{0.05\textwidth} \forall j \in J, j \neq 0,}\tag{\ref{eq:plates_conservation}}\\
	    & \specialcell{\sum_{o \in O}\sum_{q \in Q_{0o}} x^o_{q0} + \sum_{i \in E_{*0}} e_{i0} \leq 1 \hspace*{\fill},}\tag{\ref{eq:just_one_original_plate}}\\
            & \specialcell{s_i \leq \sum_{j \in J}\sum_{o \in O}\sum_{q \in Q_{jo}} h^o_{qji} x^o_{qj} \hspace*{\fill} \forall i \in \bar{J},}\label{eq:piece_sized_plates}\\%\tag{\ref{eq:piece_sized_plates}}\\
            & \specialcell{s_i + \sum_{j \in E_{i*}} e_{ij} \leq u_i \hspace*{\fill} \forall i \in \bar{J},}\label{eq:hyb_demand}\\
	    & \specialcell{x^o_{qj} \in \mathbb{N}^0 \hspace*{\fill} \forall j \in J, o \in O, q \in Q_{jo},}\tag{\ref{eq:trivial_x}}\\
            & \specialcell{e_{ij} \in \mathbb{N}^0 \hspace*{\fill} \forall (i, j) \in E}\tag{\ref{eq:trivial_e}}\\
            & \specialcell{s_{i} \in \mathbb{N}^0 \hspace*{\fill} \forall i \in \bar{J}.}\label{eq:trivial_s}
\end{align}

The aforementioned unfortunate notation detail is the incapability of denoting two or more different cuts~\(x^o_{qj}\) with the same orientation~\(o\) and the same cutting position~\(q\) over the same plate~\(j\).
Therefore, if the aggressive hybridisation replaces a BGC with two or more POCs, then the notation does not allow us to differentiate between them.The~\(a^o_{qkj}\) parameter also needs to change, as it suffers from the same problem.
The aggressive hybridisation code deals with this problem by having unique single indexes for each cut and reverse indexes from each cut property (like orientation or cutting position) to the cuts themselves; this way, the cuts are not limited to the uniqueness of some property combination.

A trivial way to change the presented formulation to use \emph{binding cuts} is to change the constraint set~\eqref{eq:piece_sized_plates} to require equality.
However, the binding cuts can also be implemented without the new variable and constraint sets.
The term \(s_i\) could just be replaced by \(\sum_{j \in J}\sum_{o \in O}\sum_{q \in Q_{jo}} h^o_{qji} x^o_{qj}\) in both the objective function and the demand constraint.
Both mentioned ways to implement binding cuts work on the FMT formulation, which does not have the same loss of optimality problem as the BBA.

% FROM FURINI 2016 PAGE 8: "As an example, if there are three items with widths 21 31 5, the Restricted PP-G2KP Model would allow us to cut at position q = 5 and then perform a further cut at position q = 2 on the obtained plate. Hence, the width of the strip obtained by cutting at position 5 would not correspond to the width of one of the obtained items. For this reason, the Restricted PP-G2KP Model can produce solutions that do not satisfy the definition of restricted guillo- tine cuts given in Section 1."

\section{Experimental results}

In these experiments, for reasons explained further ahead, each instance was solved ten times with ten distinct solver seeds.
The BBA configuration included all applicable reductions previously discussed (i.e., Cut-Position and Plate-Size Normalisation) but excluded initialisation with a primal heuristic and pricing.
The barrier algorithm was used to solve the root node as usual.
Only the Gurobi solver is used in these experiments.
No runs ended in timeout.
The computer setup, as well as the Julia and Gurobi versions/parameters, are the same as described in~\cref{sec:setup_other_formulations}, but the model was built and solved in the same process (i.e., there was no writing and reading from MPS file), and no time limit was enforced.
Three variants are scrutinised: no hybridisation (N. H.), conservative hybridisation (C. H., avoids increasing model size), and aggressive hybridisation (A. H., always hybridise, even if it leads to an increase of the model size).
The first dataset considered is FMT59 (solved as G2KP), and the second is CJCM (solved as G2OPP); more details on these datasets can be found in~\cref{sec:datasets}.

\Cref{tab:g2kp_hyb_summary} shows that both C. H. and A. H. had a similar impact on the total solving time (i.e., a reduction of \(\approx\)20\%).
A. H. had slightly better timings despite the considerable increase in the number of cuts.
C. H. slightly reduces the number of cuts.
Both C. H. and A. H. have almost no effect on the number of plates (or extractions variables).
The percentage of hybridised cuts (\emph{h \%}) and hybridised cuts with just one residual (\emph{k \%}) show that the new reductions changed a very significant part of the models.
The number of instances with the lowest averages shows that N. H. is the best option for most instances.

\begin{table}
\caption{Summary of hybridisation impact over BBA formulation and FMT59 dataset.}
\label{tab:g2kp_hyb_summary}
\begin{center}

\begin{tabular}{lrrrrrrrr}
\hline\hline
\textbf{Variant} & \textbf{T. T.} & \textbf{\(\Delta\) B. T.} & \textbf{\#b} & \textbf{\#extr.} & \textbf{\#cuts} & \textbf{h \%} & \textbf{k \%} & \textbf{\#plates} \\\hline
N. H. & 6,681 & 1,703 & 27 & 186,536 & 2,498,801 & -- & -- & 113,822 \\
C. H. & 5,468 & 489 & 22 & 184,067 & 2,496,421 & 41 & 18 & 113,373 \\
A. H. & 5,447 & 469 & 10 & 184,050 & 3,021,911 & 67 & 28 & 113,366 \\\hline\hline
\end{tabular}
\end{center}

\legend{
\justifying
\emph{T. T.} (Total Time) -- sum of the mean time of all instances, in seconds; \emph{\(\Delta\) B. T.} (Distance from the Best Time) -- sum of the difference in mean time between the respective variant and the variant with the lowest mean time for the same instance, in seconds, i.e., if all variants ran in parallel and had average time, how much time the runs of the respective variant would spend after another thread already finished; \emph{\#b} (best) -- number of instances in which the respective variant had the lowest (best) average time among the variants; \emph{\#extr.} -- total number of extraction variables (considering one model per instance); \emph{\#cuts.} -- total number of cut variables (considering one model per instance); \emph{h \%} -- percentage of \#cuts that were hybridised; \emph{k \%} -- percentage of \#cuts that were not only hybridised but also discarded the second child of the second constituting cut as waste, i.e., the POC resulted in the piece-sized plate and \emph{one} other plate; \emph{\#plates} -- total number of plates (considering one model per instance). Source: the author.
}
\end{table}

A closer look into the data, see \Cref{tab:g2kp_hyb_selected_instances}, reveals that most time difference comes from a few hard instances.
In fact, the instance Hchl4s alone is responsible for most of the difference, with okp2 having about half its relevance and the rest of the instances considerably less impact.
The number of variables hybridised (\emph{H} columns) does not seem a good indicator of how impacted the solving times will be.
However, if N. H. spends most of the time solving the root node (low Non-Root \%), C. H. and A. H. generally do not bring great time improvements.
As the most significant reductions often occur in instances that spend less than 1\% of the time in the root node, the time distribution does not change significantly.
An exception is CHL1s which shows that C. H. seems to impact not the time at the root node but the time at the B\&B, as expected from a symmetry breaking-enhancement.

\begin{table}[!ht]
\caption{Impact of BBA hybridisation in FMT59 instances taking more than 10s.}
\label{tab:g2kp_hyb_selected_instances}
\begin{center}
%\resizebox{!}{.77\height}{%
\begin{tabular}{lrrrrrrrrrrr}
\hline\hline
& \multicolumn{2}{c}{H (\%)} & \multicolumn{3}{c}{Mean Time (s/\%)} & \multicolumn{3}{c}{CV (\%)} & \multicolumn{3}{c}{Non-Root (\%)} \\\cmidrule(lr){2-3}\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}
Inst. & C & A & N (s) & C (\%) & A (\%) & N & C & A & N & C & A \\\hline\hline
Hchl4s & 46 & 57 & 3,657 & 80 & \bestcolumnemph{69} & 76 & 35 & 25 & >99 & >99 & >99 \\
okp2 & 22 & 22 & 1,844 & \bestcolumnemph{77} & 88 & 21 & 19 & 44 & >99 & >99 & >99 \\
Hchl7s & 50 & 77 & 428 & \bestcolumnemph{100} & 134 & 18 & 26 & 19 & 25 & 36 & 44 \\
okp3 & 33 & 49 & \bestcolumnemph{209} & 113 & 122 & 29 & 38 & 35 & >99 & >99 & >99 \\
Hchl8s & 17 & 35 & 253 & 68 & \bestcolumnemph{48} & 73 & 45 & 44 & >99 & >99 & >99 \\
Hchl3s & 46 & 57 & 39 & \bestcolumnemph{93} & 130 & 11 & 21 & 85 & 80 & 82 & 87 \\
Hchl2 & 25 & 77 & 45 & \bestcolumnemph{89} & 144 & 5 & 8 & 18 & 49 & 50 & 65 \\
CHL6 & 45 & 68 & 39 & \bestcolumnemph{91} & 98 & 13 & 15 & 14 & 48 & 46 & 44 \\
CHL7 & 23 & 78 & \bestcolumnemph{30} & 105 & 116 & 8 & 8 & 5 & 26 & 27 & 44 \\
Hchl6s & 51 & 80 & \bestcolumnemph{36} & 103 & 103 & 3 & 5 & 3 & 14 & 18 & 27 \\
CHL1 & 32 & 64 & \bestcolumnemph{26} & 115 & 120 & 14 & 16 & 14 & 68 & 71 & 72 \\
CHL1s & 32 & 64 & 21 & \bestcolumnemph{75} & 121 & 14 & 13 & 6 & 62 & 39 & 61 \\
okp5 & 11 & 12 & 12 & \bestcolumnemph{97} & 99 & 1 & 2 & 2 & 19 & 21 & 21 \\\hline\hline
\end{tabular}
%} % resizebox
\legend{
\justifying
\emph{H (\%)} -- the percentage of all variables (i.e., cut and extraction) that were hybridised for C. H. and A. H.; \emph{Mean Time (s/\%)} -- mean time spent to solve the instance, in seconds for N. H., and in a percentage relative to N. H. for both C. H. and A. H.; \emph{CV} -- coefficient of variation (also known as relative standard deviation) is the standard deviation for N. H., C. H., and A. H., divided by their respective means (CV is always a percentage); \emph{Non-Root (\%)} -- the percentage of the total time which was \emph{not} spent solving the root node. Source: the author.
}
\end{center}
\end{table}

The coefficient of variation of the analysed instances reveals the reason for multiple runs with distinct seeds: the difference between two runs of the same variant but distinct seeds is often larger than the difference between the means of two distinct variants.
Intuitively, breaking symmetries should reduce the variance of the timings.
By cutting symmetric branches, there is less opportunity for a solver seed to traverse multiple equivalent branches with a good relaxation (but bad primal) before finding a primal solution that cuts all such branches.
In fact, when C. H. and A. H. achieve a considerable (20\% or more) reduction of the mean time, the coefficient of variation (which is relative to the mean time) generally shows a reduction.
However, a more general effect, i.e., a higher percentage of model hybridisation (H\%) leading to lower CV (or mean time), is not observed.
Exactly \emph{which} variables were hybridised probably have more impact than \emph{how many} variables.
Finally, the reduction of variance, while positive if the objective is to compare solution methods, may be unwanted when solving the same problem in parallel.
For example, if two methods have similar mean times, the method with the most variance will probably have a thread find the optimal solution first.

The Clautiaux42 dataset has two traits that make it a worst-case scenario for both C. H. and A. H.: most instances are small instances, and the number of pieces sharing the same length, or width, is high.
For the sake of comparison, let us define \(rr_l\) (\(rr_w\)) as the \emph{repeat ratio} of the length (width) values of pieces.
For a given set of piece types, the \(rr_l\) (\(rr_w\)) is a fraction with the difference between the set cardinality and the number of distinct length (width) values as the numerator, and the set cardinality minus one as the denominator.
\emph{Zero} means there is no repetition, and \emph{one} means that all pieces share the same value in the respective dimension.
The FMT59 dataset has \(rr_l \approx 0.178\) and \(rr_w \approx 0.155\), while the Clautiaux42 has \(rr_l \approx 0.465\) and \(rr_w \approx 0.402\).
The metric is a good baseline, even if it does not account for some important details.
For example, small piece types sharing a small length, or width, cause more hybridisation than large pieces sharing a large length (or width).
This last observation is especially true for the BBA formulation, which has no cuts after the midplate.

\Cref{tab:g2opp_hyb_summary} shows that C. H. hybridises only \(\approx\)5\% of the variables and has minimal impact, while A. H. hybridises \(\approx\)55\% of the variables but causes a large increase in both time to solve and the number of cut variables.
\Cref{tab:g2opp_hyb_summary} reveals that for many instances, C. H. has less than 0.5\% of hybridised variables, and the mean and CV show they behave basically the same as N. H. for most instances.
In fact, the slight advantage of C. H. comes from the fact that the two hardest instances have no hybridisation and, therefore, the same mean time as N. H. But the third-hardest instance reaps a few seconds from the hybridisation of 9\% of the variables.
The extra variables from the A. H. have a strong negative effect on the mean time for all instances.
For some instances (such as E05F18 and E00X23), the mean time is more than ten times longer than N. H.
Finally, if rotation is allowed, then \(rr_l\) and \(rr_w\) become a single metric that can only be greater than or equal to both previous values; consequently, the gap in behaviour between C. H. and A. H. can only increase by allowing rotation.

\begin{table}[!ht]
\caption{Summary of hybridisation impact over BBA formulation and Clautiaux42 dataset.}
\label{tab:g2opp_hyb_summary}
\begin{center}
\begin{tabular}{lrrrrrrrr}
\hline\hline
\textbf{Variant} & \textbf{T. T.} & \textbf{\(\Delta\) B. T.} & \textbf{\#b} & \textbf{\#extr.} &\textbf{\#cuts} & \textbf{h \%} & \textbf{k \%} & \textbf{\#plates} \\\hline
N. H. & 255.10 & 11.30 & 32 & 1,205 & 109,369 & 0.00 & 0.00 & 12,642 \\
C. H. & 251.58 & 7.78 & 10 & 1,205 & 109,369 & 5.50 & 2.11 & 12,642 \\
A. H. & 858.30 & 614.50 & 0 & 1,205 & 160,650 & 55.23 & 10.78 & 12,642 \\\hline\hline
\end{tabular}
\end{center}
\legend{The description of the columns can be found in~\Cref{tab:g2kp_hyb_summary}. Source: the author.}
\end{table}

In general, for both datasets, C. H. either had a negligible difference from N. H. or provided some considerable benefit (especially for instances with longer running times).
For solving mostly small instances, or instances with high \(rr_l\) or \(rr_w\), the extra complexity brought to the formulation may not be worthwhile, but the change does not bring much risk of worsening the results.
The A. H. has the best reduction of mean time and CV for both Hchl4s and Hchl8s (FMT59 dataset), but it has a consistently bad performance for small instances with high \(rr_l\) and \(rr_w\).
There is no clear class of instances for which it can consistently outperform C. H. (or N. H.).

\begin{table}[!ht]
\caption{Details of hybridisation impact over BBA formulation and Clautiaux42 dataset.}
\label{tab:g2opp_multiple_seeds_full_CV_hyb}
\begin{center}
%\resizebox{!}{.77\height}{%
\begin{tabular}{lrrrrrrrrrrr}
\hline\hline
& \multicolumn{2}{c}{H (\%)} & \multicolumn{3}{c}{Mean Time (s/\%)} & \multicolumn{3}{c}{CV (\%)} & \multicolumn{3}{c}{Non-Root (\%)} \\\cmidrule(lr){2-3}\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}
Inst. & C & A & N (s) & C (\%) & A (\%) & N & C & A & N & C & A \\\hline\hline
E02F20 & 0 & 58 & \bestcolumnemph{83.70} & 100 & 350 & 62 & 62 & 70 & >99 & >99 & >99 \\
E20F15 & 0 & 54 & \bestcolumnemph{68.26} & 101 & 212 & 51 & 49 & 67 & >99 & >99 & >99 \\
E04F19 & 9 & 57 & 26.59 & \bestcolumnemph{64} & 183 & 117 & 118 & 156 & >99 & >99 & >99 \\
E05F18 & 0 & 57 & \bestcolumnemph{8.52} & 111 & 1,172 & 21 & 17 & 51 & >99 & >99 & >99 \\
E10X15 & 15 & 55 & \bestcolumnemph{7.62} & 124 & 292 & 64 & 28 & 96 & 99 & >99 & >99 \\
E08F15 & 0 & 51 & \bestcolumnemph{5.05} & 115 & 424 & 34 & 33 & 63 & 99 & 99 & >99 \\
E04F17 & 0 & 54 & \bestcolumnemph{4.50} & 100 & 730 & 16 & 16 & 7 & 99 & 99 & >99 \\
E02N20 & 0 & 55 & \bestcolumnemph{4.23} & 100 & 721 & 17 & 17 & 45 & 99 & 99 & >99 \\
E05X15 & 9 & 51 & 3.46 & \bestcolumnemph{92} & 213 & 25 & 25 & 31 & 99 & 99 & 99 \\
E02F17 & 2 & 54 & \bestcolumnemph{3.43} & 111 & 239 & 26 & 17 & 20 & 99 & 99 & >99 \\
E07F15 & 2 & 59 & \bestcolumnemph{3.27} & 121 & 418 & 23 & 28 & 26 & 99 & 99 & >99 \\
E04F20 & 2 & 53 & 2.95 & \bestcolumnemph{76} & 469 & 53 & 43 & 145 & 99 & 98 & >99 \\
E15N15 & 12 & 55 & \bestcolumnemph{2.78} & 123 & 159 & 83 & 77 & 87 & 99 & 99 & 99 \\
E07X15 & 0 & 51 & 2.55 & \bestcolumnemph{99} & 300 & 17 & 18 & 45 & 99 & 98 & 99 \\
E00X23 & 0 & 55 & \bestcolumnemph{2.26} & 100 & 1,237 & 11 & 11 & 100 & 97 & 97 & >99 \\
E03X18 & 0 & 56 & 2.26 & \bestcolumnemph{86} & 310 & 28 & 19 & 40 & 98 & 97 & 99 \\
E04N17 & 9 & 49 & \bestcolumnemph{1.99} & 128 & 234 & 40 & 35 & 31 & 98 & 98 & 99 \\
E03N17 & 2 & 57 & \bestcolumnemph{1.98} & 115 & 236 & 13 & 10 & 18 & 98 & 98 & 99 \\
E04F15 & 1 & 49 & \bestcolumnemph{1.94} & 101 & 182 & 14 & 9 & 13 & 98 & 98 & 99 \\
E03N16 & 1 & 55 & \bestcolumnemph{1.78} & 110 & 337 & 21 & 12 & 19 & 98 & 98 & 99 \\
E02F22 & 4 & 54 & \bestcolumnemph{1.72} & 101 & 321 & 41 & 38 & 55 & 98 & 98 & 99 \\
E04N18 & 0 & 55 & 1.62 & \bestcolumnemph{86} & 743 & 32 & 25 & 48 & 98 & 98 & >99 \\
E05F20 & 10 & 55 & \bestcolumnemph{1.36} & 113 & 243 & 33 & 41 & 45 & 97 & 97 & 98 \\
E00N23 & 0 & 49 & 1.33 & \bestcolumnemph{96} & 215 & 12 & 15 & 22 & 96 & 96 & 98 \\
E08N15 & 8 & 61 & \bestcolumnemph{1.19} & 106 & 436 & 18 & 15 & 29 & 97 & 98 & 99 \\
E05N17 & 0 & 53 & \bestcolumnemph{1.17} & 112 & 364 & 18 & 15 & 52 & 97 & 97 & 99 \\
E05F15 & 11 & 53 & \bestcolumnemph{1.10} & 107 & 386 & 9 & 15 & 24 & 96 & 97 & 99 \\
E15N10 & 13 & 53 & 1.09 & \bestcolumnemph{88} & 236 & 20 & 30 & 24 & 97 & 97 & 98 \\
E03N15 & 9 & 61 & \bestcolumnemph{1.09} & 109 & 249 & 10 & 10 & 22 & 96 & 97 & 98 \\
E05N15 & 1 & 55 & \bestcolumnemph{0.72} & 113 & 200 & 33 & 15 & 21 & 96 & 95 & 97 \\
E20X15 & 0 & 55 & \bestcolumnemph{0.71} & 105 & 531 & 25 & 25 & 45 & 94 & 95 & 99 \\
E07N15 & 8 & 63 & \bestcolumnemph{0.58} & 116 & 313 & 44 & 46 & 105 & 97 & 97 & 98 \\
E04N15 & 19 & 57 & \bestcolumnemph{0.48} & 111 & 350 & 12 & 13 & 20 & 93 & 94 & 97 \\
E13X15 & 0 & 57 & 0.48 & \bestcolumnemph{93} & 205 & 19 & 18 & 19 & 94 & 91 & 96 \\
E13N10 & 13 & 52 & \bestcolumnemph{0.37} & 112 & 164 & 13 & 18 & 14 & 92 & 93 & 95 \\
E00N15 & 1 & 53 & \bestcolumnemph{0.25} & 104 & 134 & 8 & 8 & 11 & 76 & 77 & 83 \\
E13N15 & 5 & 56 & \bestcolumnemph{0.24} & 113 & 787 & 8 & 10 & 25 & 84 & 85 & 97 \\
E07N10 & 32 & 59 & \bestcolumnemph{0.16} & 119 & 158 & 37 & 22 & 13 & 87 & 89 & 88 \\
E10N10 & 21 & 62 & \bestcolumnemph{0.14} & 117 & 218 & 44 & 38 & 15 & 86 & 83 & 90 \\
E10N15 & 29 & 52 & \bestcolumnemph{0.08} & 117 & 148 & 11 & 9 & 8 & 66 & 67 & 73 \\
E03N10 & 11 & 51 & 0.08 & \bestcolumnemph{95} & 166 & 9 & 6 & 8 & 34 & 28 & 76 \\
E00N10 & 31 & 61 & \bestcolumnemph{0.04} & 132 & 199 & 4 & 2 & 109 & 15 & 22 & 61 \\\hline\hline
\end{tabular}
%} % resizebox
\end{center}
\legend{The description of the columns can be found in~\Cref{tab:g2kp_hyb_selected_instances}. Source: the author.}
\end{table}

\input{related_problems.tex}

\chapter{Conclusions}
\label{sec:conclusions}

% Mention rotation (and its improvement)
% A win of symmetry breaking (or more preprocessing)over postprocessing
% Minor things: gurobi slightly advantadge in our context.

% Proof of T instances?

The present work advances the state of the art on MILP formulations for the G2KP \newtext{and related problems}.
\oldtext{This work improved the performance of one of the most competitive MILP formulations for the G2KP by at least one order of magnitude.}
\newtext{This work proposes a (re-)formulation that improves the performance of one of the most competitive MILP formulations for the G2KP by at least one order of magnitude.}
The enhanced formulation dominates the original formulation in the instance set selected by the original formulation.
\oldtext{Concerning other competitive MILP formulations in the literature, the proposed formulation kept the advantage of tighter bounds the original formulation had over them, and greatly reduced the model size and running times for instances that these other formulations had the advantage.}
\newtext{
Concerning other formulations for the problem in the literature, the proposed formulation has shorter run times, and it proves the optimality of more instances (in all datasets in which any of the considered formulations can prove the optimality of at least one instance).
The weakness of the proposed formulation is that in harder datasets (like APT) it will either be unable to solve the root node under the time limit or fail by memory exhaustion.
The other formulations cannot prove the optimality of instances of this dataset either, but they are able to return good solutions at least.
}

\newtext{
The proposed formulation and the plate-size normalisation are enhancements practically exempt from drawbacks except for the extra complexity of implementation, which is still lower than the pricing procedure they mostly supersede. The proposed hybridisation also does not add too much complexity to the implementation. However, it achieves moderate success only in runs where most time is spent after solving the root node, and its aggressive variant risks a large increase to run time for easier instances.
}

\newtext{
The flexibility advantage of formulations allows for easy adaptation for the rotation variant and changing the problem to G2MKP, G2OPP, and G2CSP.
G2MKP is not deeply studied by the literature, and this thesis gives a starting point for future practitioners.
For the G2CSP, as it is common in many CSP variants, a simple formulation has difficulty keeping with the state of the art.
The relaxation upper bound is often optimal, but good upper and lower bounds methods can often prove optimality without needing a systematic framework for exploring the search space.
The formulation can be a tool for scanning through the search space when these bounds cannot close the gap by themselves, but it is not competitive by itself.
For the G2OPP, the proposed formulation seems competitive against a similar approach (a constraint programming model), but it is orders of magnitude slower than the state-of-the-art bottom-up pattern enumeration (which has the disadvantage of needing even more memory than the proposed formulation).
}

\newtext{
In the experiments, some elementary inferences were already discussed, such as the impact on the performance caused by the LP-solving algorithm, the specific changes made, MIP-starting the models, some procedures proposed together with the original model (i.e., pricing and some preprocessing reductions), allowing rotation, and choice of the solver.
The author deemed these too specific to be discussed here and better contextualised in their respective experiment sections.
}
\oldtext{
In the experiments, some elementary inferences were already discussed, for example: the limitations (and partial success) of the improved formulation against the most recent and challenging instances in the literature; and the impact on the performance caused by the LP-solving algorithm, by the specific changes made, by MIP-starting the models, and by some procedures proposed together with the original model (i.e., pricing and some preprocessing reductions).
Here more general conclusions from a broader perspective are presented.
}

\emph{The author believes symmetry-breaking plays a significant part in the success of the proposed formulation.}
In the experiments, the text focuses on the significant reduction of the model size because it is easier to measure.
However, in~\cref{sec:comparison}, by comparing formulations with and without the \emph{purge} procedure, it can be seen that a significant reduction of the model size does not always lead to a significant reduction in running times.
In the case of the variables removed by the \emph{purge} procedure (which could never assume a nonzero value), it seems clear the solver was able to disregard them without the need for the explicit removal by \emph{purge}.
The same does not apply to the variables removed by the enhanced model \newtext{, the plate-size normalisation, the hybridisation, or even the (rotation-specific) mirror-plate enhancement)}, which could assume nonzero values and compose symmetric solutions.
\oldtext{A single extraction variable may replace many distinct sequences of cuts that would extract the same piece from the same slightly-larger plate.}
\newtext{Each of these enhancements either removes redundant variables which could assume non-zero values or change the variables to further restrict the search space (without losing the guarantee of optimality).}
\newtext{The enhanced formulation did not present consistent gains in the LP relaxation for them to be responsible for the observed improvement in performance.}
The author also believes the results suggest that clever dominance rules may considerably improve pseudo-polynomial models (which often have tight bounds but large formulations) before resorting to more complicated techniques (as the pricing procedure proposed in~\citet{furini:2016}\newtext{, and described in~\cref{sec:pricing}}, or column generation techniques).

\oldtext{
\emph{Limited parallelisation of solving LP models is becoming a bottleneck.}
Obtaining tighter bounds, even at the cost of larger model size, is often valuable.
Some recent examples of this trade-off are pseudo-polynomial models like ours, but exponential-sized models solved by column generation are a pervasive and older example of the same trade-off.
In the experiment focusing on finding new optimal solutions for hard instances, it became clear that this approach shifts computational effort from the massively parallelisable B\&B phase to the almost serial root node relaxation phase.
This effect postpones finding the first primal solution and diminishes the value in massive computer clusters.
}

\newtext{
The author believes it would be fruitful to pursue the following subjects in future research: applying the hybridisation to G2OPP (especially infeasible instances) as these are negatively impacted by the symmetries that hybridisation removes; any further reductions that may help to tackle large instances of the literature (as \texttt{gcut13}); and matheuristics that work without adaptation over any problems the formulation can solve and help to avoid the worst-case of the pseudo-polynomial formulations (i.e., providing no primal solution before the time limit if the root relaxation is hard to solve).
}

%Our suggestions for future works follow: adapt the formulation for closely related problem variants and compare to their state-of-the-art solving procedure; expand on the symmetry-breaking; \oldtext{search for more parallelisable ways of solving LPs;} consider other frameworks besides the pricing framework of~\citet{furini:2016}.

% TODO: say that one rotation-specific enhancement that was not tested is to take into consideration which length and width are unique to a single piece, so if the width was already used in the discretisation for the same dimension the length cannot be, and vice-versa.

%\input{future_works.tex}

\bibliographystyle{./infufrgs/inputs/abntex2-alf}
\bibliography{thesis}

\appendix

\chapter{Details on mentioned datasets}
\label{sec:datasets}

The following is a list of every dataset that provided at least one of the instances used in this thesis experiments, as well as the N and T datasets, discussed in~\cref{sec:about_T_instances}.
The list is sorted by the year of the work that proposed the first (if not all) instances of the dataset.

% What to describe about the datasets:
% Order the datasets by the date they were proposed.
% * The paper that proposed the dataset.
% * The oldest paper we know that referred to the instance by their current name.
% * Other names by what the instances were called, if any.
% * If they are artificially generated, how they were generated.
% * If they are not artificially generated, basic info about them (magnitude).
% * For which specific problem variant they were created.
% * Related with the specific variant, if they specify demand or profit.
% * Repositories with it.

\begin{description}
\item [HH] \emph{Proposed in:} \citet{herz:1972} and \citet{hifi:1997} (see details below) \emph{for the} unweighted unconstrained (and, after, constrained) G2KP, and the \emph{first known reference to the name adopted in this work is} \citet{cung:2000}. \emph{Other names:} the proposing paper does not name the unconstrained version and \citet{hifi:1997} call the constrained version of H. \emph{Characteristics: } ``[...], we have considered another instance (denoted H), derived from the instance of \citet{herz:1972}, by adding an upper bound for each piece. The instance is described by \((L, W) = (127, 98)\), \(n = 5\), \(b = (5, 4, 2, 1, 6)\), \(c_i = l_i \times w_i\) and \((l_i, w_i)\), for \(i = 1 \dots 5\), are given by \((21, 13)\), \((36, 17)\), \((54, 20)\), \((24, 27)\) and \((18, 65)\), respectively.'' \citep{hifi:1997}. Their \(b\) is referred to as~\(u\) in this thesis (i.e., piece demand), the analogue is valid for \(c\) and \(p\) (i.e., piece profit).
\item [cgcut1 to cgcut3] \emph{Proposed in} \citet{cw:1977} \emph{for the} G2KP, and the \emph{first known reference to the name adopted in this work here is} \citet{martello:1998}. \emph{Other names:} the instances were numbered as 1--3 by the proposing paper, which is a common approach but often not considered a name, in this case, however, other papers (e.g., \citet{hifi:1997}) and instance repositories (e.g., \url{ftp://cermsem.univ-paris1.fr/pub/CERMSEM/hifi/2Dcutting}) adopted the numbers as names; also, \citet{fayard:1998} refers to cgcut1--2 as CHW1--2, \citet{tschoke:1995} proposes a four-instance unnamed dataset in which instances 1 and 3 are cgcut1 and cgcut3, consequently, cgcut1 and cgcut3 are also called STS1 and STS3 by PackLib\textsuperscript{2} (\url{https://www.ibr.cs.tu-bs.de/alg/packlib/xml/b-autdg-85-xml.shtml}); \citet{velasco:2019} mention a CW4 instance from \citet{cw:1977}, however, the author believes they wanted to refer to the CW4 instance from \citet{fayard:1998} instead. \emph{Characteristics:} The cgcut1--3 are part of a larger semi-artificially generated dataset, but they were the only ones fully described in the body of the original paper and the most commonly adopted by later works. The authors selected the number of pieces (7, 10, and 20, respectively) and the original plate dimensions (15x10, 40x70, and 40x70, respectively). The piece dimensions were obtained by defining selecting a random value in \([1, 0.25\times L \times W]\) to be the piece area, then selecting a random integer value between one and the piece area to be the piece length and, finally, determined the piece width in base of the already defined piece length and the provisional area (rounding the width up, if necessary, i.e., allowing the area to grow instead of shrink). The profit values were obtained by multiplying the area by a random real number between 1 and 3. Finally, the demand vector was handpicked by the authors to best suit their purposes.
\item [wang20] \emph{Proposed in:} \citet{wang:1983} \emph{for the} unweighted G2KP, i.e., waste minization variant, the paper also cover the G2CSP but the instance does not seem to be used for this purpose, and the \emph{first known reference to the name adopted in this work is}~\citet{fekete:1997}. \emph{Other names:} the instance is also referred as W by~\citet{fayard:1998}, however, in \url{ftp://cermsem.univ-paris1.fr/pub/CERMSEM/hifi/2Dcutting/}, W has a different demand vector (which does not seem to affect the optimal objective value). \emph{Characteristics:} The instance is, according to the author, ``a variation of an example presented by \citet{cw:1977}''. The instance is fully described in the proposing paper and reproduced here: \(L = 70\), \(W = 40\), \(l = [11,\) \(12,\) \(14,\) \(17,\) \(18,\) \(21,\) \(23,\) \(24,\) \(24,\) \(25,\) \(27,\) \(32,\) \(34,\) \(35,\) \(36,\) \(37,\) \(38,\) \(39,\) \(41,\) \(43]\), \(w = [19,\) \(21,\) \(23,\) \(9,\) \(29,\) \(31,\) \(33,\) \(15,\) \(15,\) \(16,\) \(17,\) \(22,\) \(24,\) \(25,\) \(26,\) \(27,\) \(28,\) \(29,\) \(30,\) \(31]\), \(u = [4,\) \(3,\) \(4,\) \(1,\) \(3,\) \(3,\) \(3,\) \(1,\) \(2,\) \(4,\) \(2,\) \(2,\) \(2,\) \(2,\) \(1,\) \(1,\) \(1,\) \(1,\) \(1,\) \(1]\).
\item [gcut1 to gcut13] \emph{Proposed in} \citet{beasley:1985:guillotine} \emph{for the} unconstrained G2KP with both a limited and an unlimited number of stages, and the \emph{first known reference to the name adopted in this work here is} \citet{martello:1998}. \emph{Other names:} gcut13 is also referred to as B by \citet{fekete:1997}. \emph{Characteristics: } The profit of each piece is set to their area, as in the unweighted variant, and the demand of each piece was left undefined, as they were developed for an unconstrained variant. In their experiments, \citet{furini:2016} set the demand of each piece to one, and the same is done here (i.e., in the experiments of this work), to allow the comparison with their results. The gcut1--12 instances are artificially generated: the number of sampled pieces is \([10, 20, 30, 50]\) for the first four, middle four, and last four instances; \(L = W\) and they are \(250\) for the first four, \(500\) for the middle four, and \(1000\) for the last four; both piece length and width are sampled from an integer uniform distribution~\([L/4, 3L/4]\). The gcut13 instance is a real-world instance of \(32\) pieces and an original plate of size \(3000\)x\(3000\), fully described in the Table 2 of the original paper. This assimetry is probably the cause many papers select only the gcut1--12 for their experiments. \emph{Online repositories: } PackLib\textsuperscript{2} \url{https://www.ibr.cs.tu-bs.de/alg/packlib/xml/b-autdg-85-xml.shtml}, and ESICUP \url{https://www.euro-online.org/websites/esicup/data-sets/}.
\item[cl\_* (a.k.a. CLASS)] \emph{Proposed in:} \citet{berkey:1987} (first 6 classes with 50 instances each) and \citet{martello:1998} (last 4 classes with 50 instances each) \emph{for the} two-dimensional BPP (non-guillotine) and the \emph{first known reference to the name adopted in this work is} hard to pinpoint, \citet{martello:1998} refer to each new set of 50 instances they propose as classes, and~\citet{boschetti:2003} echoes this when they merge some instance sets from the two previous works together into a single dataset divided into 10 classes, but the name \emph{CLASS} (also adopted by 2DPackLib) seems clearly accidental. \emph{Other names:} in~\cite{alvelos:2009}, the initials of the authors are used for each part of the dataset (BW for the first six classes, and MV for the four last classes), but other names are also possible, as the instance groups were just numbered (using roman numerals) and the individual instances are referred just by their attributes (i.e., cl\_\emph{class}\_\emph{n}\_\emph{seed}, where \(n\) is the total number of pieces). \emph{Characteristics:} each class has 50 instances, these instances can be further divided into five groups, each group has 10 instances sharing the same~\(n \in \{20, 40, 60, 80, 100\}\), all instances inside a group have exactly the same generation parameters except by the random seed. For the first six classes, both the length and the width of the pieces is sampled from an integer uniform distribution~\([1, 10]\) (classes I and II), \([1, 35]\) (classes III and IV), and \([1, 100]\) (classes V and VI), and the original plates are squares of, respectively, 10, 30, 40, 100, 100 (again), and 300 units; for the last 4 classes all original plates are 100x100, and the piece dimensions are drawn from four \emph{types} of piece size distribution. Each piece from the class (originally referred to as)~\(k \in \{I, II, III, V\}\) has 70\% chance of being sampled from the distribution of \emph{type}~\(k\), and 10\% chance from being sampled from each of the other three types. The four \emph{types} of distribution are uniformily random integers. The length (width) range for the types of distribution I to IV are, respectively, \([1, 50]\) (\([66, 100]\)), \([66, 100]\) (\([1, 50]\)), \([50, 100]\) (\([50, 100]\)), and \([1, 50]\) (\([1, 50]\)). \emph{Possible sources of confusion:} In~\citet{berkey:1987}, the term `two-dimensional bin-packing problem' is used to describe what is now commonly referred to as Strip Packing Problem (i.e., packing into an strip that is open ended in one dimension), the term is then adopted for `a special case of packing into finite bins' which is the current meaning of the two-dimensional BPP. Also, in \citet{martello:1998}, \emph{seven} new classes are proposed, however, for this combined dataset only the first four classes (referred to as I, II, III, and IV in that work) are taken and they are referred to as classes VII, VIII, IX, and X in the context of this dataset of 10 classes (because the first six come from~\citet{berkey:1987}). The author believes this can be a source of confusion, as class VII in~\citet{martello:1998} is \emph{not} the class VII in the CLASS dataset.
\item [OF1 and OF2] \emph{Proposed in:} \citet{oliveira:1990} \emph{for the} unweighted G2KP (i.e., waste minization variant), and the \emph{first known reference to the name adopted in this work is} \citet{hifi:2001}. \emph{Other names:} not known, but possible, as the instances were just numbered in the proposing paper. \emph{Characteristics: } The OF1--2 are part of a larger artificially generated dataset, but they were the only ones fully described in the body of the original paper and, as far as the author knows, the only ones adopted by later works. The generation procedure is clever but more complex than usual. Considering this thesis only employs two instances, the author do believe it is better just reproduce them than to describe the whole generation process. Both instances have \(L = 40\), \(W = 70\), and \(n = 10\). The profit of each piece is set to their area. OF1 have \(l = [5, 39, 9, 15, 16, 21, 14, 19, 36, 4]\), \(w = [29, 9, 55, 31, 11, 23, 29, 16, 9, 22]\), and \(u = [1, 4, 1, 1, 2, 3, 4, 3, 2, 2]\). OF2 have \(l = [18, 10, 27, 18, 8, 4, 9, 19, 16, 16]\), \(w = [22, 40, 13, 23, 29, 16, 47, 19, 13, 36]\), and \(u = [2, 1, 3, 2, 4, 1, 1, 4, 2, 4]\).
\item [STS1 to STS4] \emph{Proposed in:} \citet{tschoke:1995} \emph{for the} no-rotation and rotation G2KP, and the \emph{first known reference to the name adopted in this work is} \citet{alvarez:2002:tabu} (STS2 and STS4) and PackLib\textsuperscript{2} (STS1 and STS3). \emph{Other names:} the instances STS1 and STS3 are, in fact, cgcut1 and cgcut2 from \citet{cw:1977}, the instances STS2 and STS4 (which were proposed by \citet{tschoke:1995}) are also called TH1 and TH2 by \citet{fayard:1998}. \emph{Characteristics: } As STS1 and STS3 are cgcut1 and cgcut3 (both already described), this entry will focus exclusively on STS2 and STS4. \citet{tschoke:1995} mention the instances are artifical and fully specified in their appendix, but give no details of the generation procedure. The \(L\), \(W\), and \(n\) of the STS2 and STS4 are, respectively, \([55, 99]\), \([85, 99]\), and \([30, 20]\). \emph{Online repositories:} PackLib\textsuperscript{2} \url{https://www.ibr.cs.tu-bs.de/alg/packlib/instances_problem_type.shtml}.
\item [okp1 to okp5] \emph{Proposed in:} \citet{fekete:1997} \emph{for the} 2KP (i.e., non-guillotine G2KP), and the \emph{first known reference to the name adopted in this work is} the proposing paper. \emph{Other names:} not known and improbable (the instances were named by the proposing paper). \emph{Characteristics: } the instances were artificially generated using the same schema than \citet{beasley:1985:nonguillotine} ``after applying initial reduction''. The ``initial reduction'' seems to consist on a set of rules for reducing the pieces demand to the smallest value which does not affect the optimal objective value. The instances were fully described in the proposing paper. The original plate of all instances is 100x100, and the number of piece types in the five instances are 15, 30, 30, 33, and 29, respectively. The schema is the same as the one described in this list for the cgcut1--3 instances except that (i) the length is picked from~\([1, L]\) (instead of the provisory piece area) and (ii) the demand vector was not handpicked but a random integer among 1, 2, and 3 (which may be changed by the ``initial reduction'', as mentioned above).
\item [A1 to A5] \emph{Proposed in:} \citet{hifi:1997} \emph{for the} weighted and unweighted G2KP (which is referred to as `constrained two-dimensional cutting stock problem' in the paper), and the \emph{first known reference to the name adopted in this work is} the proposing paper. \emph{Other names:} not known and improbable (the instances were named by the proposing papers; but note the similarly named instances A-1 to A-43 from \citet{macedo:2010} have no relation to these instances. \emph{Characteristics: } The instances are fully described in the paper, and their origin (real-world or artificial) is not mentioned. The instances A1 and A2 have arbitrary profits associated to the pieces, the A3, A4, and A5 do not (i.e., the piece area is used). The original plates range from 50x60 to 132x100, the piece demands range from 1 to 4, the average piece area of the first four instances is 699 (i.e., 26x27) and, for the fifth instance, 1107 (33x33).
\item [CW1 to CW11 and CU1 to CU11] \emph{Proposed in:} \citet{fayard:1998} \emph{for the} weighted and unweighted G2KP (CW means \emph{constrained weighted} and CU mean \emph{constrained unweighted}), and the \emph{first known reference to the name adopted in this work is} the proposing paper. \emph{Other names:} not known and improbable (the instances were named by the proposing paper). \emph{Characteristics: } The CW\(i\) and CU\(i\), for \(i = 1, \dots, 11\), share \(L\), \(W\), \(l\), \(w\), and \(u\); only \(p\) is distinct: in the CU instances \(p_i = l_i \times w_i\), and in the CW instances \(p_i\) is a random integer in~\([100, 1000]\); ``the dimensions of the initial plate, the dimensions of the pieces and the number of pieces to cut \(m\) are uniformly taken in the integer intervals \([100, 1000]\), \([0.1 \times L, 0.7 \times W]\) and \([25, 60]\) respectively.'' \citep{fayard:1998}. The pieces demand \(u_i\) were sampled using~\(max\{1, min\{10, random(0, \lfloor L/l_i \rfloor \times \lfloor W/w_i \rfloor)\}\}\). \emph{Online repositories:} 2DPackLIB \url{http://or.dei.unibo.it/library/2dpacklib-2-dimensional-packing-problems-library}, \url{ftp://cermsem.univ-paris1.fr/pub/CERMSEM/hifi/2Dcutting/}.
\item [CHL1 to CHL7] \emph{Proposed in:} \citet{cung:2000} \emph{for the} G2KP, and the \emph{first known reference to the name adopted in this work is} the proposing paper. \emph{Other names: } not known and improbable (the instances were named by the proposing paper). \emph{Characteristics: } ``[\dots] the dimensions \(l_i\) and \(w_i\) of pieces to cut are taken uniformly from the intervals \([0.1L, 0.75L]\) and \([0.1W, 0.75W]\) respectively. The weight associated to a piece \(i\) is computed by \(c_i = \lceil\rho l_i p_i\rceil\), where \(\rho = 1\) for the unweighted case and \(\rho \in [0.25, 0.75]\) for the weighted case. The constraints \(b_i\), for \(i = 1, \dots, n\), have been chosen such that \(b_i = min\{\rho_1, \rho_2\}\), where \(\rho_1 = \lfloor L/l_i \rfloor\lfloor W/w_i \rfloor \) and \(\rho_2\) is a number randomly generated in the interval \([1, 10]\)'' \citep{cung:2000}. Their \(c_i\) is what is referred to as \(p_i\) in this thesis (i.e., piece profit), the same can be said for \(b_i\) and \(u_i\) (i.e., piece demand). The \(L\), \(W\), and \(n\) must be provided, and for CHL1--7 they are \([132,\) \(62,\) \(157,\) \(207,\) \(20,\) \(130,\) \(130]\), \([100,\) \(55,\) \(121,\) \(231,\) \(20,\) \(130,\) \(130]\) and \([30,\) \(10,\) \(15,\) \(15,\) \(10,\) \(30,\) \(35]\), respectively. \emph{Online repositories:} PackLib\textsuperscript{2} \url{https://www.ibr.cs.tu-bs.de/alg/packlib/instances_problem_type.shtml}, \url{ftp://cermsem.univ-paris1.fr/pub/CERMSEM/hifi/2Dcutting/}.
\item [Hchl1, Hchl2, Hchl9, and Hchl3s to Hchl8s] \emph{Proposed in:} \citet{cung:2000} \emph{for the} weighted and unweighted G2KP, and the \emph{first known reference to the name adopted in this work is} the proposing paper. \emph{Other names:} not known and improbable (the instances were named by the proposing paper). \emph{Characteristics: } The generation procedure is the same described in item CHL1--CHL7. The suffix `s' is used to indicate that the pieces have a profit value equal to their area (as in A1s, A2s, 2s, \dots). However, differently of the other suffixed instances, which had an earlier version without the suffix and with an arbitrary profit vector, there does not seem to exist instances Hchl3--Hchl8. The \(L\), \(W\), \(n\) for the instances (in the order they are numbered) are \([130,\) \(130,\) \(127,\) \(127,\) \(205,\) \(253,\) \(263,\) \(49,\) \(65]\), \([130,\) \(130,\) \(98,\) \(98,\) \(223,\) \(244,\) \(241,\) \(20,\) \(76]\), and \([30,\) \(35,\) \(10,\) \(10,\) \(25,\) \(22,\) \(40,\) \(10,\) \(35]\), respectively. \emph{Online repositories:} \url{ftp://cermsem.univ-paris1.fr/pub/CERMSEM/hifi/2Dcutting/}.
\item [A1s, A2s, 2s, 3s, STS2s, STS4s, and CHL1s to CHL4s] \emph{Proposed in:} \citet{cung:2000} \emph{for the} unweighted G2KP, and the \emph{first known reference to the name adopted in this work is} the proposing paper. \emph{Other names:} not known and improbable (the instances were named by the proposing papers), however, in the text pf the original paper there was a single space separating the instance name from the `s' which was supressed by all subsequent works. \emph{Characteristics: } ``The instances 2 s--3 s, A1 s--A2 s, STS2 s--STS4 s and CHL1 s--CHL4 s represent exactly the instances 2--3 , A1--A2, STS2--STS4 and CHL1--CHL4, respectively for which the profit of each piece is represented by its area.''. \citep{cung:2000} The original version of each instance (i.e., without the `s') is described by other items of this list. The instances 2\oldtext{s} and 3\oldtext{s} refer to the alternative name to for the cgcut2 and cgcut3.
\item [T1a to T7e and N1a to N7e] \emph{Proposed in:} \citet{hopper_thesis} \emph{for the} Strip Packing Problem with the guillotine constraint (T instances, but see below) and without the guillotine constraint (N instances), and the \emph{first known reference to the name adopted in this work is} the proposing thesis. \emph{Other names:} not known and improbable (the instances were named by the proposing paper). \emph{Characteristics: } All instances of both datasets have a strip width of 200, and a (supposedly) known optimal height of 200 (with no waste). Each dataset has 35 instances which are divided into categories 1 to 7, each category has five distinct instances indicated by the last character of the instance name: a, b, c, d, e; and these five instances differ from each other only because the seed given to the random number generator. Each category define the same total number of pieces in both datasets, respectively: 17, 25, 29, 49 73, 97, 199. All instances are fully described in the appendix of \citet{hopper_thesis}. The generation procedure is also described by the proposing thesis and consists mostly of starting with a 200x200 plate, and recursively making guillotine cuts to the plates (T instances), or recursively dividing the plates into the five plates of the basic non-guillotinable pattern (N instances). In~\cref{sec:about_T_instances}, a proof by exhaustion shows that T1a is impossible to pack into a guillotine pattern of 200x200, and there is strong evidence empirical evidence of this impossibility for other T instances. The author believes that both dataset T and N were created using the procedure to create non-guillotinable patterns.
\item[C1-p1 to C7-p3] \emph{Proposed in:} \citet{hopper:2001} \emph{for the} Strip Packing Problem (without the guillotine constraint and allowing piece rotation) and the \emph{first known reference to the name adopted in this work is} the proposing paper. \emph{Other names:} while the instances were named by the proposing paper, some papers refer to them by `HT' followed by a number or another identifier\cite{stephane:sat:2010,fleszar:2016}. \emph{Characteristics:} The dataset of 21 instances is divided into categories 1 to 7 (the \emph{C} comes from category) each with problems 1 to 3 (the \emph{p} comes from problem). Each category defines the total number of pieces, a strip width, and the optimal height (known at generation). The number of pieces are 16 or 17 (C1), 25, 28 or 29, 49, 72 or 73, 97, and 196 or 197 (C7). The strip widths are: 20, 40, 60, 60, 60, 80 and 160. The optimal heights are: 20, 15, 30, 60, 90, 120, and 240. Each of the 21 instances is described in its entirety inside the proposing paper. The optimal solution is always a pattern with no waste, and it `` is achieved by packing the rectangles in the order they are stated in the tables using the BLF routine.'' \citep{hopper:2001}. The proposing paper does not give details of the generation process. In this thesis, the experiments show that, for some instances, it is impossible to obtain the known optimal solution while enforcing guillotine cuts and disallowing rotation; however, when rotation is allowed, the optimal solution was found for every instance which the runs did not timeout.
\item [\newtext{APT10 to APT49}] \newtext{\emph{Proposed in:} \citet{alvarez:2002:tabu} \emph{for the} G2KP (unconstrained unweighted and weighted, and constrained weighted and unweighted) and the \emph{Other names:} not known and improbable (the instances were named by the proposing paper). \emph{Characteristics:} The instances APT10 to APT29 are unconstrained, \(L\) and \(W\) are sampled from~\([1500, 3000]\) and the number of pieces types is sampled from~\([30, 60]\). The instances APT30 to APT49 are constrained, \(L\) and \(W\) are sampled from~\([100, 1000]\), the number of pieces types is sampled from~\([25, 60]\), and the demand of each piece~\(i\) comes from~\(min{u^1_i, u^2_i}\) where \(u^1_i = \lfloor L / l_i \rfloor \times \lfloor W / w_i \rfloor\) and \(u^2_i\) is sampled from~\([1, 10]\). For all intances, the piece lengths are sampled from~\([0.05L, 0.4L]\), and the piece widths are sampled from~\([0.05W, 0.4W]\). The instances APT10 to APT19 (unconstrained), as well as APT30 to APT39 (constrained), are unweighted, this is, the profit of each piece is equal to its area. The instances APT20 to APT29, as well as APT40 to APT49, are weighted, and the profit of each piece~\(i\) is sampled from \([0.25l_iw_i, 0.75l_iw_i]\).}
\item [CJCM] \emph{Proposed in:} \citet{clautiaux:2007}, but sometimes misatributted to~\citet{clautiaux:2008}, \emph{for the} ortogonal packing problem (this is, the non-guillotined G2OPP) and the \emph{first known reference to the name adopted in this work is} \citet{cote:2018} which explicitly refer to the dataset as CJCM, some citation styles abbreviate the authors as `CJCM', so there is a previous work that referred to the instances as `the instances from [CJCM08]' \citep{belov2009branch}. \emph{Other names:} the author is also aware of the name `CCM' employed by \citep{fleszar:2016}; as the dataset was not named by its authors and the instances were identified by their three unique attributes (see below) it is possible other names exist. \emph{Characteristics:} Unfortunately, \citet{clautiaux:2007} points to \citet{hopper2002problem} for details on the instance generation method and the author was not able to find a copy of the latter. Besides that reference, the proposing paper tell us that ``The idea is to obtain both feasible and non-feasible problem instances. The first step of the algorithm consists in randomly generating a set of values whose sum is equal to \((1 - \epsilon)WH\). These values are the areas of the items in the created instance. Then the values are factorized to get the width and the height of the items.'' \citep{clautiaux:2007}. In this thesis, the name of individual instances follow the pattern E\(\epsilon f n\), in which \(\epsilon\) are two digits indicating \((LW - \sum_{i \in \bar{J}} l_i w_i u_i)/100\) (i.e., guaranteed waste percentage if feasible), \(f\) is the feasibility status as determined by the methods employed in the proposing paper (`F' for feasible, `N' for unfeasible, and `X' for not solved by any of the three methods employed within the time limit of 15 minutes for each), and \(n\) is the total number of pieces (\(\sum_{i \in \bar{J}} u_i\)). ). The selection criteria for the 42 instances of the dataset is not entirely clear: the distribution of \(n\) in the range \([10, 23]\) seems arbitrary, and instances with the same \(\epsilon\) and \(n\) only exist if \(f\) (assessed by the experiments) is distinct. Consequently, it seems like a larger dataset was first generated, and then 42 instances were selected from it based on the results of the experiments. The original plate dimensions are always 20x20, and the piece types are heterogeneous (\(u_i = 1\) for the vast majority of the piece types, going up to \(u_i = 4\) for five piece types in four distinct instances).
\item[A-1 to A-43] \emph{Proposed in:} \citet{macedo:2010} \emph{for the} two-staged G2CSP and the \emph{first known reference to the name adopted in this work is} the proposing paper. \emph{Other names:} not known and improbable (the instances were named by the proposing paper); but note the similarly named instances A1 to A5 from \citet{hifi:1997} have no relation to these instances. \emph{Characteristics:} ``[...],  two sets of real instances from the furniture industry, set A and [...]'' \citep[p. 7]{macedo:2010}. The instances are highly heterogeneous, and their numbering seem to have no relation with any of their characteristics. The most consistent characteristic are the dimensions of the original plates, which are either 2550x2100 (31 instances), 2750x1220 (11 instances), or 2470x2080 (1 instance). A table summarising their characteristics can be found in the proposing paper. \emph{Online repositories:} 2DPackLib \url{http://or.dei.unibo.it/library/2dpacklib}.
\item [P1\_*, P2\_*, P3\_*, and P4\_*] \emph{Proposed in:} \citet{velasco:2019} \emph{for the} rotation and no-rotation G2KP, and the \emph{first known reference to the name adopted in this work is} the proposing paper. \emph{Other names:} not known and improbable (the instances were named by the proposing papers). \emph{Characteristics: } There is a total of 80 instances, each combination of \(i \in \{1, 2, \dots, 5\}\) \newtext{(random number generator seed)} and \(n \in \{25, 50\}\) for each of the following 8 triples of \emph{class}, \(L\), and \(W\): \((1, 100, 200)\), \((1, 100, 400)\), \((2, 200, 100)\), \((2, 400, 100)\), \((3, 150, 150)\), \((3, 250, 250)\), \((4, 150, 150)\), \((4, 250, 250)\). The instance names follow the pattern P\emph{class}\_\(L\)\_\(W\)\_\(n\)\_\(i\). The pieces of instances with \(n = 25\) are a subset of the pieces in instances with \(n = 50\) (i.e., the `first half'). The piece demands are randomly picked from the integer uniform distribution~\([1, 9]\), and the piece profits are the piece area multiplied by a real number randomly picked from the continuous distribution between \(0.5\) and \(1.5\). The classes 1--3 have piece lenghts randomly picked from the integer uniform distribution~\([5, 40]\), and piece widths from \([10, 80]\). In class 4, half the pieces have their length and width defined in the same way as classes 1--3, and the other half uses \([10, 80]\) for length, and \([5, 40]\) for width, i.e., the distributions are switched between dimensions. \emph{Online repositories:} 2DPackLIB \url{http://or.dei.unibo.it/library/2dpacklib-2-dimensional-packing-problems-library}.
\item [CW*\_M{2, 4, 8}] \emph{Proposed in:} this thesis (but it is a direct adaptation of the CW1--CW11 instances from \citet{fayard:1998}) \emph{for the} G2MKP and the \emph{first known reference to the name adopted in this work is} this thesis (the dataset as a whole may be referred to as CW\_M). \emph{Other names:} none. \emph{Characteristics:} each instance CW\(k\)\_M\(m\) is exactly the same as the original CW\(k\) instance except it has \(m\) original plates available instead of just one. To retain the selection aspect of the problem (i.e., \emph{which} pieces will be packed, not only \emph{how} they will be packed), the author chose to only have a CW\(k\)\_M\(m\) instance if \((\sum_{i \in \bar{J}} l_i w_i u_i) \geq 2 m L W\). This is the reason CW1\_M8 to CW5\_M8 do not exist: each instance from CW1 to CW5 has a summed area of their pieces lower than the summed area of \(2 \times 8\) copies of their respective original plates.
\item [A-*\_M2, A-*\_M4, A-\_M8] \emph{Proposed in:} this thesis (but it is a direct adaptation of the A-1 to A-43 instances from \citet{macedo:2010}) \emph{for the} G2MKP and the \emph{first known reference to the name adopted in this work is} this thesis. \emph{Other names:} none. \emph{Characteristics:} each instance A-\(k\)\_M\(m\) is exactly the same as the original A-\(k\) instance except it has \(m\) original plates available instead of just one. To retain the selection aspect of the problem (i.e., \emph{which} pieces will be packed, not only \emph{how} they will be packed), the author chose to only have a A-\(k\)\_M\(m\) instance if \((\sum_{i \in \bar{J}} l_i w_i u_i) \geq 2 m L W\). The set of instances A-1 to A-43 is very heterogeneous, and the numbering seem to be arbitrary, so this leads to an equally arbitrary subset of them respecting the criteria for \(m \in \{2, 4, 8\}\). For the sake of completeness, the numbers of the original instances that have \(m = 2\) counterparts are: 2, 3, 5, 7, 9, 11--21, 23--29, 31--38, and 40--43 (i.e., 35 of the original 43 instances); of these the following 26 original instances have \(m = 4\) counterparts, they are: 2, 5, 7, 9, 11--16, 18--21, 23--25, 27--29, 32, 33, 35, 38, 40, and 41; and, finally, of these the following 16 original instances have \(m = 8\) counterparts: 2, 9, 11, 14--16, 18--21, 24, 27, 29, 32, 33, and 38. This totalises \(35 + 26 + 16 = 77\) instances in this new dataset.
\item [\newtext{FMT59}] \newtext{\emph{Proposed in:} \citet{furini:2016} (no instance is new so it was not proposed but grouped) \emph{for the} weighted, unweighted, constrained, and unconstrained G2KP and the \emph{first known reference to the name adopted in this work is} this thesis. \emph{Other names:} none. \emph{Characteristics:} the dataset contains 37 unweighted instances and 22 weighted instances (59 in total) and comprises the entirety of some literature datasets as well as subsets of others. Every instance come from another dataset described in this list. The unweighted instances are: W, wang20 \citep{wang:1983},
gcut1 to gcut12 \citep{beasley:1985:guillotine},
OF1, OF2 \citep{oliveira:1990},
A3 to A5 \citep{hifi:1997},
CU1, CU2 \citep{fayard:1998},
2s, 3s, A1s, A2s, STS2s, STS4s, CHL1s, CHL2s, CHL5, CHL6, CHL7, Hchl3s, Hchl4s, Hchl6s, Hchl7s, Hchl8s\citep{cung:2000}.
The weighted instances are:
cgcut1 to cgcut3 \citep{cw:1977},
okp1 to okp5 \citep{fekete:1997},
HH \citep{hifi:1997},
2, 3 \citep{cw:1977},
A1, A2 \citep{hifi:1997},
STS2, STS4, CHL1, CHL2, CW1 to CW3, Hchl2 and Hchl9 \citep{cung:2000}.
The dataset was assembled from an assortment of datasets from the OR library (wang20, gcut1 to gcut12, cgcut1 to cgcut3, okp1 to okp5) and the already mixed dataset employed in~\citet{hifi:2001} (all the remaining instances).
The author do not suggest employing this dataset as it is in future works, unless a comparison with a work that has already employed it is needed.
The reason for this recommendation is that, because the literature adopted different names for the same instances, some instances of the dataset are just duplicates with different names.
This is the case for instances 2 and 3 which are the same as cgcut2 and cgcut3.
Taking into account the literature wang20 and W should be the same instance but the instances obtained by the author have distinct demand values for some of the pieces (all the remaining instance characteristics are the same for all pieces).
The weighted instances A1, A2, 2, 3, STS2, STS4, CHL1, and CHL2 have an unweighted version of themselves in the same dataset; the unweighted alternatives have the same name but are suffixed with an `s'.
}
\item [\newtext{Easy18}] \newtext{A subset of the dataset FMT59 defined in this work. Its purpose is to reduce the number of runs needed before discarding a formulation from further consideration. The dataset contains: cgcut1 to cgcut3, gcut1 to gcut12, OF1, OF2, and wang20. See FMT59 for the origin of each instance, and the rest of the list for a description of their characteristics.}
\end{description}

\chapter{Resumo expandido}

O problema principal desse trabalho é o Problema da Mochila 2D Guilhotinada com cortes ortogonais (e irrestritos), demanda limitada, estágios ilimitados, e sem rotação.
Essa variante específica é referida doravante como PM2G.
O PM2G é um problema fortemente NP-difícil~\citep{korf:initial:2003,dolatabadi:2012}.
Esse trabalho também examina um tipo específico de corte restrito, três problemas distintos relacionados ao PM2G, e a variante que permite a rotação das peças (em todos problemas estudados).
Os três problemas distintos mencionados acima são o Problema das Múltiplas Mochilas (PMM), o Problema de Empacotamento Ortogonal (PEO), e o Problema de Corte de Estoque (PCE).
Esse trabalho foca na obtenção de soluções ótimas para esse problema através de Programação Linear Inteira Mista (PLIM).

Uma instância do PM2G consiste de: um retângulo de comprimento~\(L\) e largura~\(W\) (aqui chamada de \emph{placa original}); um conjunto de retângulos~\(\bar{J}\) (doravante chamados de \emph{peças}) onde cada retângulo~\(i \in \bar{J}\) tem um comprimento~\(l_i\), uma largura~\(w_i\), um lucro~\(p_i\), e uma demand~\(u_i\).
O PM2G procura maximizar o lucro das peças obtidas pelo corte da placa original.
O termo \emph{guillotinado} significa que cada corte sempre vai de um lado da placa até o lado oposto da mesma; um corte nunca para ou começa no meio de uma placa.

Problemas de corte guilhotinado são de interesse da indústria, especialmente da indústria da madeira~\cite{yanasse:linear:2008,morabito:hardboard:2007} e do vidro~\cite{clautiaux:2019,parreno:2020}, em geral devido a limitações do maquinário.
Existe uma literatura vasta e em crescimento no assunto como é evidenciado por~\citet{iori:2020} e por~\citet{russo:2020}.

Esse trabalho foca em PLIM como método de solução (ao invés de métodos \emph{ad hoc}) porque a adaptabilidade amplifica o valor de quaisquer melhoramentos descobertos.
Uma formulação PLIM melhor significa: um método de solução melhor para os vários (já mencionados) problemas relacionados; uma melhor relaxação contínua para computar um chute otimista do valor da função objetivo de todas essas variantes (alguns algoritmos \emph{ad hoc} usam solucionadores PLIM para computar esses limites); não somente um melhor método exato mas também uma base melhor para heurísticas e procedimentos que suportam interrupção a qualquer momento; a capacidade automática de se beneficiar de paralelização, decomposição automática de problemas, e heurísticas do solucionador; e, finalmente, melhor envelhecimento do método através dos anos por meio da tendência atual de processadores de múltiplos núcles e o constante avanço na performance dos solucionadores.

\newtext{
As contribuições dessa tese incluem:
\begin{itemize}
\item uma formulação PLIM baseada em uma formulação prévia do estado da arte, a prova da sua corretude, e evidência empírica do melhoramento da performance;
\item uma nova forma de empregar uma propriedade já conhecida (normalização do tamanho da placa) para tanto a formulação original quanto a formulação melhorada, e evidência empírica do impacto positivo deste uso original na performance de ambas formulações;
\item novos limitantes inferiores e superiores, assim como novos valores ótimos, para várias das instâncias difíceis propostas recentemente~\citet{velasco:2019};
\item uma comparação direta com formulações recentes da literatura enfatizando os pontos fracos e fortes de cada;
\item uma adaptação da formulação proposta para três problemas (PMM2G, PEO, e PCE2G) e resultados empíricos sobre conjuntos de dados da literatura para servir como uma base para futuras comparações entre formulações;
\item a hibridização da formulação proposta (com a formulação de~\citet{silva:2010}) que tem sucesso moderado em reduzir ainda mais o tempo de execução para instâncias em que a maior parte to tempo é dispendida na fase de B\&B (\emph{branch and bound}) por meio da proibição de certas simetrias.
\end{itemize}
Para tal, o autor reimplementou uma formulação do estado da arte e um procedimento de precificação usado por ela.
}

A primeira formulação PLIM feita especificamente para o PM2G foi proposta por~\citet{furini:2016}.
A formulação é classificada como uma extensão da formulação \emph{one-cut} de~\citet{dyckhoff:1981} para o Problema de Corte de Estoque unidimensional.
Entretanto, a formulação de~\citet{silva:2010} pode ser vista como uma etapa intermediária entre essas duas: esta já havia estendido a formulação \emph{one-cut} para duas dimensões mas não havia alterado o problema do PCE2G para o PM2G e era limitada a dois estágios ou três estágios restritos.
Uma versão estendida de~\citet{furini:2016} aparece em~\citet{dimitri_thesis} (uma tese de doutorado), e um prelúdio a ela aparece em~\citet{furini:conference:2016}.
A formulação tem tamanho pseudo-polinomial, \(O((L + W) \times L \times W)\)~variáveis e~\(O(L \times W\) restrições, além da sua relaxação prover limitantes melhores que a formulação em~\citet{messaoud:2008}.

Nesse trabalhi, o autor propõe uma formulação melhorada baseada na formulação de~\citet{furini:2016} mencionada acima.
Uma vantagem significativa desse melhoramento é evitar a enumeração de quaisquer cortes depois da metade de uma placa.
Essa vantagem aparece em muitos trabalhos desde~\citet{herz:1972}.

Considerando as 59 instâncias usadas nos experimentos da formulação em que o autor se inspirou, e somando os valores para todos os modelos gerados, a formulação proposta tem apenas uma pequena fração das variáveis e restrições do modelo original (respectivamente, 3.07\% e 8.35\%).
A formulação melhorada soluciona todas as 59 instâncias em cerca de 4 horas enquanto a formulação original soluciona 53 em 12 horas (as outras 6 instâncias não são solucionadas dentro do limite de 3 horas por instância).
Nós integramos, em ambas formulações, uma estrutura de precificação proposta para a formulação original; a formulação melhorada mantém uma vantagem significativa nessa situação.
Em um conjunto de 80 instâncias difíceis recentemente proposto, a formulação melhorada (com e sem a estrutura de precificação) encontrou: 22 soluções ótimas para o problema com cortes irrestritos (5 já conhecidas, 17 novas); 22 soluções ótimas para o problema com cortes restritos (todas novas para o problema e nenhuma é a mesma que do problema de cortes irrestritos); melhores limitantes inferiores para 25 instâncias; melhores limitantes superiores para 58 instâncias.
Considerando outras formulações para o problema na literatura, a formulação proposta apresenta tempos de execução menores, e prova a otimalidade para mais instâncias.
Somente nos conjuntos de instâncias em que nenhuma formulação solucionou instância alguma é que a formulação proposta falhou em encontrar boas soluções primais enquanto outras formulações obtiveram êxito.
A formulação proposta somente falhou em obter soluções de boa qualidade nos conjuntos de instâncias em que nenhuma formulação conseguiu solucionar instância alguma.
Nesses conjuntos de dados, outras formulações obtiveram boas soluções primais mesmo não sendo capazes de solucionar instância alguma.

\end{document}

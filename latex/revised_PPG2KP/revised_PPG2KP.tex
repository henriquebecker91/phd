\RequirePackage{fix-cm} % Added by the template.

% smallextended below is the one used by MPC,  see:
% https://www.springer.com/journal/12532/submission-guidelines
\documentclass[smallextended]{svjour3}       % onecolumn (second format)

% FIX cleverref + svjour3 bug, see:
% https://tex.stackexchange.com/questions/499497/
\makeatletter
\let\cl@chapter\undefined
\makeatletter

\smartqed  % flush right qed marks, e.g. at end of proof

% insert here the call for the packages your document requires
\usepackage{mathptmx} % use the same font the Journal will use if possible
% packages for diagrams and colored links
\usepackage{graphicx} % already present in template, do not remove
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{color}
\usepackage[table]{xcolor}
\definecolor{gray-table-row}{gray}{0.90}
% One specific table uses three shades.
%\definecolor{gray0}{gray}{1.0} % white, but called gray to automatize simpler
%\definecolor{gray1}{gray}{0.95}
%\definecolor{gray2}{gray}{0.90}
% Packages for the orcid logo (hyperref is also needed but included above)
% Disabled for now because only seem to work with xetex or luatex
%\usepackage{academicons}
% Packages for computer code
\usepackage{algorithm}
\usepackage{algpseudocode}
% Package for multiline comments
\usepackage{verbatim}
% Packages for formatting the mathematical formulation
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[nameinlink]{cleveref}

% please place your own definitions here and don't use \def but
% \newcommand{}{}
%\newsavebox\CBox
%\def\textBF#1{\sbox\CBox{#1}\resizebox{\wd\CBox}{\ht\CBox}{\textbf{#1}}}
%\parindent=0pt
%\renewcommand\UrlFont{\color{blue}\rmfamily}
% Command that justifies the rest of a math equation to right.
% Used to format the formulations, which are broken using just align, as
% there are some lines where the middle column is big and the last column
% is small (and vice-versa), and the align cannot avoid overlap between
% the large middle and the large last without breaking the layout.
% With this command, the middle and last columns are merged as one, and the
% inequation is separated from the forall with the \pushright
%\newcommand{\pushright}[0]{\hskip \textwidth minus \textwidth}
%\newcommand{\specialcell}[1]{\ifmeasuring@#1\else\omit$\displaystyle#1$\ignorespaces\fi}
% Defines the ORCID command
%\newcommand{\orcid}[1]{\hspace{2mm}\href{https://orcid.org/#1}{\textcolor[HTML]{A6CE39}{\aiOrcid}}}

% Insert the name of "your journal" with
\journalname{Mathematical Programming Computation}

\begin{document}

\title{Enhanced ILP formulation for 2D guillotine knapsack packing\thanks{
PUT CAPES/CNPq ACKNOWLEDGEMENT HERE
}}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

%\author{Henrique Becker\orcid{0000-0003-3879-2691} \and Olinto Araujo\orcid{0000-0003-1136-5032} \and Luciana S. Buriol\orcid{0000-0002-9598-5732}}
\author{Henrique Becker \and Olinto Araujo \and Luciana S. Buriol}

\authorrunning{H. Becker et al.} % if too long for running head

\institute{
	Henrique Becker \at
	Federal University of Rio Grande do Sul (UFRGS), Postgraduate on Computer Science Program, Av. Bento Gonçalves, 9500, Porto Alegre, RS, Brazil\\
	%Tel.: +123-45-678910\\
	%Fax: +123-45-678910\\
	\email{hbecker@inf.ufrgs.br}           %  \\
	%\emph{Present address:} of F. Author  %  if needed
\and
	Olinto Araujo \at
	Federal University of Santa Maria (UFSM), Postgraduate on Production Engineering Program, Av. Roraima, 1000, Santa Maria, RS, Brazil\\
	%Tel.: +123-45-678910\\
	%Fax: +123-45-678910\\
	\email{olinto@ctism.ufsm.br}           %  \\
	%\emph{Present address:} of F. Author  %  if needed
\and
	Luciana S. Buriol \at
	Federal University of Rio Grande do Sul (UFRGS), Postgraduate on Computer Science Program, Av. Bento Gonçalves, 9500, Porto Alegre, RS, Brazil\\
	%Tel.: +123-45-678910\\
	%Fax: +123-45-678910\\
	\email{buriol@inf.ufrgs.br}           %  \\
	%\emph{Present address:} of F. Author  %  if needed
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

\maketitle

\begin{abstract}
In this work, we propose two enhancements for the state-of-the-art integer linear programming model to the Guillotine Two-Dimensional Knapsack Problem (constrained by demand, no rotation).
The instances used to test the enhancements are the ones the original model had difficulty solving.
The first enhancement is a change in plate enumeration titled `plate size normalization'; it reduces the number of variables by about 30\% in the tested instances and cannot increase the number of variables.
The second enhancement modifies the model symmetry-breaking; it reduces the number of variables by about 96\% in the tested instances but pathological instances may exist in which it has more variables than the original model.
Together both enhancements decrease the solving time by at least one order of magnitude compared to the original model.
A proof of correctness accompanies each enhancement.
The revised model proves the optimal value of two instances recently proposed, and updates either lower or upper bound of three other instances.

\keywords{
	Combinatorics
	\and Symmetry-breaking
	\and Pseudo-polynomial
	\and Formulation
}
% From: www.ams.org/msc
% 68R05 Combinatorics in computer science
% 68U99 Computing methodologies and applications -- General
% 05D99 Combinatorics -- Extremal combinatorics -- General
% 52B99 Convex and discrete geometry -- Polytopes and polyhedra -- General
\subclass{68R05 \and 68U99 \and 05D99 \and 52B99}
\end{abstract}

\section{Introduction}

The problem addressed in this work is the \emph{Guillotine Two-Dimensional Knapsack Problem} (G2KP), specifically the demand-constrained no-rotation variant.
An instance of the G2KP consists in: a rectangle of length~\(L\) and width~\(W\) (hereafter called \emph{original plate}); a set of rectangles~\(\bar{J}\) (also referred as \emph{pieces}) and for each rectangle~\(j \in \bar{J}\) their length~\(l_j\), width~\(w_j\), profit~\(p_j\), and demand~\(d_j\).
The objective of the G2KP is to maximize the profit of pieces sold while respecting the maximum number of copies to be sold (i.e., the piece demand).
The original plate may be cut vertically (or horizontally) from one side to the other giving origin to two new rectangles (with the summed area equal to the original plate).
Such rectangles are referred to as~\emph{plates} and together with the original plate they make up the set of plates~\(J, \bar{J} \subseteq J\); \(\forall j \in J, \) \(l_j\) and \(w_j\) are defined.
If plate~\(j\) is not a piece~(\(j \in J\setminus\bar{J}\)) then it can either: be further cut into smaller plates (i.e., become an intermediary plate), or be kept as it is (i.e., become trim/waste).
If plate~\(j\) is a piece~(\(j \in \bar{J}\)), and the number of pieces~\(j\) sold is smaller than~\(d_j\), then the plate~\(j\) has also the option of being sold for a~\(p_j\) increase in the objective function instead of being cut or kept.
Given the variant adressed has the no-rotation restriction, the length and width of a given plate are never switched; for example, a plate of length~\(a\) and width~\(b\) cannot be sold as a piece of length~\(b\) and width~\(a\); i.e., plates are never subjected to a \(90^\circ\) rotation. This is especially important for the wood industry.

The G2KP occurs in wood, glass, and metal industries as well as other contexts~\cite[p. 6]{dimitri_thesis}.
Given these different usage scenarios for G2KP, an Integer Linear Programming (ILP) model has some advantages over an ad hoc solution: easy adaptation to problem variants; the continuous relaxation gives an optimistic guess on the optimal solution value; and all the advantages of a solver framework (automatically implemented heuristics, parallelization, problem decomposition, et cetera).
%These advantages are useful for a problem with diverse variants motivated by different industries as the G2KP.
The first ILP model for the G2KP with potential to solve the instances found in the literature is a recent feat~\cite{furini:2016}.
The contributions of this work explore the ILP path further by: proposing two enhancements to reduce the number of variables of the state-of-the-art model; proving the correctness of these enhancements; providing empirical evidence of the magnitude of the performance of the revised model on the literature instances; and proving optimal values and tighter lower and upper bound values for some recently proposed instances of the literature.

% Motivation
% Prior work

%As this work make modifications to the model proposed in~\cite{furini:2016},
%we adopt the notation already used there and extend it.
% use the prior work to make reference to proofs we will make reference
% cite nicos:1977 for the proof of being able to cut just on the first half of the plate, and every cut always containing at least one piece border segment
% already make use of the notation

% Contribution

%The original model may be easily adapted from the \emph{Guillotine Two-Dimensional Knapsack Problem} (G2KP) to other variants of the same problem (multiple equal/distinct knapsacks, unconstrained by demand), or other closely related problems as the \emph{Guillotine Two-Dimensional Cutting Stock Problem} (2CSP), and the \emph{Guillotine Strip Packing Problem} (GSSP). Our modifications to the model benefit all those variants but, for sake of conciseness, we will only consider the G2KP in this work.

\section{Plate size normalization}
\label{sec:psn}

In this section, we prove that no valid solution is lost if plate dimensions are shortened to a discretization point.
For convenience, we denote the set of pieces that fit a plate~\(j\) by \(\bar{J}_j\); a piece~\(i\) fits a plate~\(j\) iff \(l_i \leq L_j \land w_i \leq W_j\).%, and that plates cut from a shortened plate may need additional shortening after the cut.

%\begin{definition}
%The set of pieces that fit a plate~\(j\) is denoted by \(\bar{J}_j\), a piece~\(i\) fit a plate~\(j\) if \(l_i \leq L_j \land w_i \leq W_j\).
%\end{definition}

\begin{definition}
The set of horizontal normal cuts of a plate~\(j\) is the set of all non-trivial linear combinations \(\sum_{i \in \bar{J}_j} a_i \times l_i \leq L\) for which the coefficients \(a_i\) are restricted by \(0 \leq a_i \leq d_i~\forall.~i \in \bar{J}_j\). The analogue is valid for vertical cuts.
\end{definition}

In~\cite{nicos:1977}, \emph{normal cuts} are defined for the variant of the problem without the demand constraints.
Our definition of normal cuts is extended to take into consideration the demand.
The following theorem and its proof (provided in \autoref{app:proof_only_normal_cuts_needed}) are also extensions to account for the demand.
%We also extend a theorem, and its proof, that restricting the cuts to (our definition of) normal cuts allows packing any demand-abiding piece mulstiset that could be packed with non-normal cuts.
%We also extend a theorem, and its proof, that restricting the cuts to (our definition of) normal cuts allows packing any demand-abiding piece mulstiset that could be packed with non-normal cuts.

\begin{theorem}\label{only_normal_cuts_needed}
For every guillotine packing with non-normal cuts packing a set of pieces~\(s\), there is a guillotine packing of only normal cuts packing the largest subset of~\(s\) that respects the demand.
%producing the same pieces or, if the former packing produced an amount exceeding the demand for some piece type, the latter paproduces the maximum amount allowed by the demand for such piece types.
\end{theorem}

%is in the appendix, as similar proofs are presented in the literature.

\begin{corollary}\label{co:size_normalized_plate}
Given a plate~\(j\) in which the right (or top) border do not overlap with the rightmost vertical (or topmost horizontal) normal cut, any demand-abiding piece multiset packed in~\(j\) can be packed in its size-normalized alternative, in which the plate size is reduced until the borders overlap with normal cuts.
\end{corollary}

\begin{proof}
The proof of the~\autoref{only_normal_cuts_needed} describes an alternative packing with the property that any space between the rightmost vertical cut and the plate right border (topmost horizontal cut and the plate top border) is waste. Therefore, if a plate is replaced by its size-normalized alternative, no used space would be lost, and no packing would be invalid.\qed
\end{proof}

\begin{remark}
If a size-normalized plate is cut by a normal cut, the first child is also size-normalized. The second child, however, may or may not be size-normalized.
\end{remark}

%In the dimension parallel to the cut, the border of the first child will overlap with the normal cut applyed to the parent plate; on the other dimension, the border already overlapped a normal cut.

%\begin{example}
%Given \(l = [5, 7]\), \(d = [2, 3]\), and a size-normalized plate of length~\(21\), a normal cut at length~\(10, 12\), or \(17\) creates a non-normalized second child of length~\(11, 9\), or \(4\), respectively; though a normal cut at length~\(5, 7, 12,\) or \(14\) creates a normalized second child of length~\(14, 12, 7\) or \(5\), also respectively.
%\end{example}

The variables of the formulation proposed in~\cite{furini:2016} represent each normal cut over each distinctly-sized plate.
The substitution of many plates of similar but distinct dimensions by a single size-normalized plate removes all variables representing cuts over the dismissed plates.

\section{Our changes to Furini's model}
\label{sec:enhanced_model}

% NEED TO DEFINE:
% Q_{jo} as the subset of the linear combinations for some plate and orientation
% in the original paper the only cuts removed are the symmetric ones, in our
% case we remove all after midplate
% we also make use of the corollary in last section to reduce the number of
% plates considered, what consequently reduces the number of variables
% * such sacrifice allows to remove some symmetries with a simpler method than redundant-cuts but, most importantly, it allows us to remove a large number of cut variables by inserting a lower number of extraction variables
% * there is a typo on the definition of 'a' at the source (say this after explaining coefficient a)

The formulation proposed in~\cite{furini:2016} is elegant: the pieces are just plates of specific size that can be sold.
Our contribution consists of small changes to the preprocessing step and the formulation.
The number of variables is greatly reduced by these changes.
However, these changes deepen the distinction between plates and pieces and, consequently, can be seen as sacrificing some elegance for performance.
The essentials of the formulation remain the same and, for this reason, we consider the model presented here as a revised model, not a new model.

% TODO: should we say that this supersedes the furini original symm-breaking
% and their redundant-cut reduction?

The cut enumeration in~\cite{furini:2016} excludes some symmetric cuts; that is, if two distinct cuts create the same set of two child plates, then the symmetric cut in the second half of the plate may be ignored.
However,~\cite{nicos:1977} disregards \emph{all} cuts after the middle of the plate because of symmetry.
If~\cite{furini:2016} did the same it could become impossible to trim a plate to a piece size.
For example, if there was a piece with length larger than half the length of a plate, and such plate has no normal cut with the exact length of the needed trim, then the piece could not be extracted from the plate, even if the piece fits the plate.
The goal of our changes is to reduce the number of cuts (i.e., model variables) by getting closer to the symmetry-breaking rule used in~\cite{nicos:1977}.
%First we present our changes to the formulation and the variable enumeration, then we prove the model correctness is not affected.

%Often, there are many more normal cuts in the second half of a plate than there is in the first half. % need explanation?
%Also, if all cuts that generated some plate type are disregarded, then every cut over such plate type is also disregarded.
%Taking all of this into account, the main purpose of our revised version of Furini's formulation is to improve its symmetry breaking. % TODO: This has also the effect of superseding the Redundant-Cut reduction, which EXPLAIN SUCCINTLY THE REDUNDANT CUT.

\subsection{The enhanced formulation}
\label{sec:enhanced}

Our changes to the formulation are restricted to replacing the set of integer variables~\(y_j, i \in \bar{J},\) with a new set of variables~\(e_{ij}, (i, j) \in E, E \subseteq \bar{J} \times J\), and the necessary adaptations to accomodate this change.
Such \emph{extraction variables}~\(e_{ij}\) denote a piece~\(i\) was extracted from plate~\(j\).
For convenience, we also define \(E_{i*} = \{~j~|~\exists~(i, j) \in E \}\) and \(E_{*j} = \{~i~|~\exists~(i, j) \in E \}\).  
The set \(O = \{h, v\}\) denote the horizontal and vertical cut orientations.
The set \(Q_{jo}\) (\(\forall j \in J, o \in O\)) denotes the set of possible cuts (or cut positions) of orientation~\(o\) over plate~\(j\).

The parameter~\(a\) is a byproduct of the plate enumeration process.
If cutting a plate of type~\(k \in J\) with a cut of orientation~\(o \in O\) at position~\(q \in Q_{jo}\) adds a plate of type~\(j \in J\) to the stock, then~\(a^o_{qkj} = 1\); otherwise~\(a^o_{qkj} = 0\).
%This parameter is needed to write the constraint that control which plates are available.
The description of this parameter in~\cite{furini:2016} has a typo, as pointed by~\cite{martin:2020}:
``[...] there is a typo in their definition of parameter~\(a^o_{qkj}\), as the indices~\(j\) and~\(k\) seem to be exchanged.''.

In a valid solution, the value of \(x^o_{qj}\) is the number of times a plate of type~\(j \in J\) is cut with orientation~\(o \in O\) at position~\(q \in Q_{jo}\); while the value of~\(e_{ij}\) is the number of sold pieces of type~\(i \in \bar{J}\) that were extracted from plates of type~\(j \in J\).
The plate~\(0 \in J\) is the original plate, and it may also be in~\(\bar{J}\), as there may exist a piece of the same size as the original plate.

%\hspace*{\fill}
\begin{align}
\mbox{max.} &\sum_{(i, j) \in E} p_i e_{ij} \label{eq:objfun}\\
\mbox{s.t.} &\sum_{o \in O}\sum_{q \in Q_{jo}} x^o_{qj} + \sum_{i \in E_{*j}} e_{ij} \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qkj} x^o_{qk} \hspace*{0.05\textwidth} & \forall j \in J, j \neq 0,\label{eq:plates_conservation}\\
%	& \specialcell{\sum_{o \in O}\sum_{q \in Q_{jo}} x^o_{qj} \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qkj} x^o_{qk} \hspace*{\fill} \forall j \in J\setminus\bar{J},}\label{eq:generic_plates_conservation}\\
	& \sum_{o \in O}\sum_{q \in Q_{0o}} x^o_{q0} + \sum_{i \in E_{*0}} e_{i0} \leq 1 &,\label{eq:just_one_original_plate}\\
	& \sum_{j \in E_{i*}} e_{ij} \leq u_i & \forall i \in \bar{J},\label{eq:demand_limit}\\
% TODO: fix equation below, the forall part is too long and clashes with the long equation in the first line
	& x^o_{qj} \in \mathbb{N}^0 & \forall j \in J, o \in O, q \in Q_{jo},\label{eq:trivial_x}\\
	& e_{ij} \in \mathbb{N}^0 & \forall (i, j) \in E.\label{eq:trivial_e}
\end{align}

The objective function maximizes the profit of the extracted pieces~\eqref{eq:objfun}.
Constraint~\eqref{eq:plates_conservation} guarantees that for every plate~\(j\) that was further cut or had a piece extracted from it (left-hand side), there must be a cut making available a copy of such plate (right-hand side).
One copy of the original plate is available from the start~\eqref{eq:just_one_original_plate}.
The amount of extracted copies of some piece type must respect the demand for that piece type (a piece extracted is a piece sold)~\eqref{eq:demand_limit}.
Finally, the domain of all variables is the non-negative integers~\eqref{eq:trivial_x}-\eqref{eq:trivial_e}.

\subsection{The revised variable enumeration}

The variable enumeration described in~\cite{furini:2016} employed some rules to reduce the number of variables; they are symmetry-breaking, \emph{Cut-Position}, and \emph{Redundant-Cut}.
The two last rules are not discussed here:~\cite{furini:2016} prove their correctness and they do not conflict with the revised model.

The use of the \(x\)~variables does not change from the original formulation to our revised formulation -- however, the size of the enumerated set of variables changes.
Our revised enumeration does not create any variable~\(x^o_{jq}\) in which \((o = h \land q > \lceil w_j / 2 \rceil) \lor (o = v \land q > \lceil l_j / 2 \rceil)\).
%given that \(D^h_j \equiv W_j\) and \(D^v_j \equiv L_j\).

The original formulation has variables~\(y_i\), \(i \in \bar{J}\), while the revised formulation replaces them with variables~\(e_{ij}\), \((i, j) \in E\), \(E \subseteq \bar{J} \times J\).
Set~\(\bar{J} \times J\) is orders of magnitude larger than~\(\bar{J}\).
Consequently, set~\(E\) must be a small subset to avoid having a revised model with more variables than the original.
A suitable subset may be obtained by a simple rule: \((i, j) \in E\) if, and only if, packing piece~\(i\) in plate~\(j\) does not allow any other piece to be packed in~\(j\).
%The reason this restricted subset is enough to keep the model correctness is presented in next section.

%If an extra piece could be packed, then there is a normal cut that creates both a plate that may be used for this extra piece and a plate that may be used to pack~\(i\).
%So the idea here is to do the extraction as late as possible: if the piece may be extracted from a descendant, then the plate may be cut until this descendant is generated to then have the piece extracted from it.

\subsection{The proof of correctness}

The previous section presented a detailed explanation of the changes to the formulation and variable enumeration.
This section proves such changes do not affect the correctness of the model.
The changes may be summarized to:

\begin{enumerate}
\item There is no variable for any cut that occurs after the middle of a plate.
\item A piece may be obtained from a plate if, and only if, the piece fits the plate, and the plate cannot fit an extra piece (of any type).
\end{enumerate}

The second change alone cannot affect the model correctness.
The original formulation was even more restrictive in this aspect:
a piece could only be sold if a plate of the same dimensions existed.
In our revised formulation there will always exist an extraction variable in such case:
if a piece and plate perfectly match, there is no space for any other piece, fulfilling our only criteria for the existence of extraction variables.
Consequently, what needs to be proved is that:

\begin{theorem}
Every normal cut after the middle of a plate may be replaced by one or more cuts before the middle of a plate and/or piece extractions without changing the demand-abiding set of pieces obtained from the packing.
\end{theorem}

\begin{proof}
This is a proof by exhaustion. The set of all normal cuts after the middle of a plate may be split into the following cases:
\begin{enumerate}
  \item The cut has a perfect symmetry. \label{case:perfectly_symmetric}
  \item The cut does not have a perfect symmetry.
  \begin{enumerate}
    \item Its second child can fit at least one piece. \label{case:usable_second_child}
    \item Its second child cannot fit a single piece.
    \begin{enumerate}
      \item Its first child packs no pieces. \label{case:no_pieces}
      \item Its first child packs a single piece. \label{case:one_piece} % call luffy to help
      \item Its first child packs two or more pieces. \label{case:many_pieces}
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

We believe to be self-evident that the union of~\cref{case:perfectly_symmetric,case:usable_second_child,case:no_pieces,case:one_piece,case:many_pieces} is equal to the set of all normal cuts after the middle of a plate. We present an individual proof for each of these cases.

\begin{description}
\item[\Cref{case:perfectly_symmetric} -- The cut has a perfect symmetry.]
If two distinct cuts have the same children (with the only difference being the first child of one cut is the second child of the other cut, and vice-versa), then the cuts are perfectly symmetric.
Whether a plate is the first or second child of a cut does not make any difference for the formulation.
If the cut is in the second half of the plate, then its symmetry is in the first half of the plate.
Consequently, both cuts are interchangeable, and we may keep only the cut in the first half of the plate.
\item[\Cref{case:usable_second_child} -- Its second child can fit at least one piece.]
\autoref{co:size_normalized_plate} allows us to replace the second child by a size-normalized plate that can pack any demand-abiding set of pieces the original second child could pack.
The second child of a cut that happens after the middle of the plate is smaller than half a plate, and its size-normalized variant may only be the same size or smaller.
So the size-normalized plate could be cut as the first child by a normal cut in the first half of the plate.
The original first child would then become the second child.
Anything that could be packed in the original first child may be packed in the new second child, as the size of such plate may only have stayed the same or grown (because of the size-normalization of its sibling).

\item[\Cref{case:no_pieces} -- Its first child packs no piece.] 
If both children of a single cut do not pack any pieces, then the cut may be safely ignored.
\item[\Cref{case:one_piece} -- Its first child packs a single piece]
First, let us ignore this cut for a moment and consider the plate being cut by it (i.e., the parent plate).
The parent plate either: can fit an extra piece together with the piece the first child would pack, or cannot fit any extra pieces.
If it cannot fit any extra pieces, this fulfills our criteria for having an extraction variable, and the piece may be obtained through it.
The cut in question can then be disregarded (i.e., replaced by the use of such variable).
However, if it is possible to fit another piece, then there is a normal cut in the first half of the plate that would separate the two pieces, and such cut may be used to shorten the plate.
This kind of normal cuts may successively shorten the plate until it is impossible to pack another piece, and the single piece that was originally packed in the first child may then be obtained employing an extraction variable.
\item[\Cref{case:many_pieces} -- Its first child packs two or more pieces.]
If the first child packs two or more pieces, but the second child cannot fit a single piece (i.e., it is waste), then the cut separating the first and second child may be omitted and any cuts separating pieces inside the first child may still be done.
If some of the plates obtained by such cuts need the trimming that was provided by the omitted cut, then these plates will be packing a single piece each, and they are already considered in~\cref{case:one_piece}.
\end{description}

Given the cases cover all possibilities, and each case has a proof, the theorem is correct. \qed

\end{proof}

\section{Experimental results}

There are three formulation implementations that provide data used in our comparisons:
\emph{original} refers to the implementation presented in~\cite{furini:2016,dimitri_thesis};
\emph{faithful} refers to our reimplementation of \emph{original};
\emph{enhanced} refers to our enhanced formulation presented in~\autoref{sec:enhanced_model}.
The \emph{original} implementation was not available\footnote{
	We asked the authors of~\cite{furini:2016} for the \emph{original} implementation and Dimitri Thomopulos informed us it was not available.
}.
Consequently, all data relative to \emph{original} presented in this work comes from~\cite{dimitri_thesis}.
Both \emph{faithful} and \emph{enhanced} data were obtained by runs using the setup described in~\autoref{sec:setup}.

Each formulation may be modified by applying any combination of the following optional procedures:
\emph{priced} -- refer to the pricing procedure described in~\cite{furini:2016,dimitri_thesis};
\emph{rounded} -- the procedure proposed by us and described in~\autoref{sec:psn};
\emph{warmed} -- the MIP models solved were warm-started with a solution found by a previous step;
\emph{Cut-Position} and \emph{Redundant-Cut} -- are reduction procedures described in~\cite{furini:2016,dimitri_thesis}, that may be enabled and disabled individually.
For each experiment described in the next sections, if we do not mention a procedure, then it is disabled.
The term \emph{restricted priced} refers to the smaller model for the restricted version of the problem that is solved inside the pricing procedure mentioned above.
Consequently, for each run of a \emph{priced} variant, there will be a \emph{restricted priced} run with the same combination of optional procedures.

The goal of the pricing procedure is to remove unneeded variables from the model.
However, the priced model often ends up with unneeded constraints and variables due to pricing.
If pricing removes all variables that made plate~\(p\) available (by cutting it from larger plates), then the constraint representing plate~\(p\) become unneeded.
Consequently, the variables that cut plate~\(p\) into smaller plates also becomes unneeded; this can make new constraints unneeded, and so on, recursively.
The effort to remove such unnecessary variables and constraints is negligible.
In \emph{priced} variants of \emph{faithful} and \emph{enhanced} this \emph{purge} procedure is done unless stated otherwise.
Our experiments will show that this \emph{purge} drastically reduces the number of variables and constraints, but have almost no effect on the solving times.
We encourage future comparisons to implement this \emph{purge} procedure, as it helps determine the real size of the solved models.

Each experiment fills a gap for the next experiments:
\autoref{sec:lp_method} explains the choice of LP algorithms made in all remaining experiments;
\autoref{sec:faithful_reimplementation} provides evidence that \emph{faithful} is on par with \emph{original}, allowing us to use it as a replacement;
\autoref{sec:comparison} compares \emph{faithful} to \emph{enhanced} and show the value of our contributions (namely, the \emph{round} procedure and the \emph{enhanced} formulation);
\autoref{sec:new_results} uses variants with best results in the last experiment to prove new optimal values and bounds for harder instances.

\subsection{Setup}
\label{sec:setup}

Every experiment in this work uses the following setup unless stated otherwise.
The CPU was an AMD\textsuperscript{\textregistered} Ryzen\textsuperscript{TM} 9 3900X 12-Core Processor (3.8GHz, cache: L1 -- 768KiB, L2 -- 6 MiB, L3 -- 64 MiB) and 32GiB of RAM available (2 x Crucial Ballistix Sport Red DDR4 16GB 2.4GHz).
The operating system used was Ubuntu 20.04 LTS (Linux 5.4.0-42-generic).
Hyper-Threading was disabled.
Each run executed on a single thread, and no runs executed simultaneously.
The computer did not run any other CPU bound task during the experiments.
The exact version of the code used is available online\footnote{
	See the repository in \url{https://github.com/henriquebecker91/GuillotineModels.jl/tree/0.2.4}
}, and it was run using Julia 1.4.2~\cite{julia} with JuMP 0.20.1~\cite{JuMP} and Gurobi 9.0.2~\cite{gurobi}.
The following Gurobi parameters had non-default values: Threads~\(= 1\); Seed~\(= 1\); MIPGap~\(= 10^{-6}\) (to guarantee optimality); and TimeLimit~\(= 10800\) (i.e., three hours).
The next section explains the rationale for using Method~\(= 2\) (i.e., barrier) to solve the root node relaxation of the final built model; and Method~\(= 1\) (i.e., dual simplex) inside pricing (when it is used).

\subsection{Rationale about LP method choice}
\label{sec:lp_method}

In~\cite{furini:2016,dimitri_thesis}, the algorithm for solving LPs is not specified.
The LP algorithm executes in two contexts of the mentioned works:
(1) multiple times inside the pricing procedure;
(2) a single time to solve the root node relaxation of the final model (this happens no matter pricing is used or not).
The choice of algorithm can drastically impact solving times.
A preliminary experiment included all LP algorithms available in Gurobi.
\autoref{tab:lp_method_comparison} presents the data of the two algorithms selected for use.
They are the \emph{Dual Simplex} and the \emph{Barrier}.

The runs use the \emph{faithful} implementation, with \emph{Cut-Position} and \emph{Redundant-Cut} enabled, in its \emph{priced} (Priced PP-G2KP in~\cite{furini:2016,dimitri_thesis}) and \emph{not priced} (PP-G2KP in~\cite{furini:2016,dimitri_thesis}) variants.
For convenience, we limited the experiment to a few instances\footnote{
	The subset consists of all instances for which the \emph{Complete PP-G2KP Model} finds the optimal solution within the time limit in~\cite{furini:2016} (Table 2).
	If pricing is disabled, the root node relaxation spends most of the solving time.
	This characteristic makes them a good choice for this experiment.
}.

\begin{table}
\caption{Comparison of LP-solving algorithms used inside solving procedure.}
\begin{tabular}{@{\extracolsep{4pt}}lrrrrrrr@{}}
\hline\hline
Instance & \multicolumn{3}{c}{Dual Simplex} & \multicolumn{3}{c}{Barrier} & D. S. + B. \\\cline{2-4}\cline{5-7}
& N. P. & R. \% & Priced & N. P. & R. \% & Priced & Priced \\\hline
CU1 & 27.37 & 92.11 & 3.79 & 24.18 & 94.68 & 3040.82 & \textbf{3.58} \\
STS4 & 93.49 & 89.88 & 48.80 & 49.94 & 77.32 & 7851.30 & \textbf{47.75} \\
STS4s & 103.20 & 94.92 & 39.29 & 43.74 & 86.34 & 8470.41 & \textbf{38.36} \\
gcut9 & 226.68 & 72.29 & \textbf{3.92} & 51.48 & 85.77 & 2060.04 & 4.01 \\
okp1 & 51.95 & 84.18 & 38.89 & \textbf{32.41} & 67.78 & -- & 38.79 \\
okp4 & 98.25 & 93.35 & 144.30 & \textbf{72.09} & 92.31 & -- & 141.53 \\
okp5 & 178.13 & 89.89 & 252.09 & \textbf{96.38} & 67.24 & -- & 239.44 \\\hline\hline
\end{tabular}
\label{tab:lp_method_comparison}
\end{table}

In \autoref{tab:lp_method_comparison}, \emph{Dual Simplex} and \emph{Barrier} indicate the respective algorithm was used for all LPs;
and \emph{D. S. + B.} means that \emph{Dual Simplex} was used to solve any LPs inside the pricing phase and \emph{Barrier} was used to solve the root node relaxation of the final model.
The columns \emph{N. P.} (\emph{Not Priced}) and \emph{Priced} display the time to solve (in seconds) using the aforementioned variant.
The columns \emph{R. \%} refer to the percent of the time spent by \emph{Not Priced} in the root node relaxation of the final model.

The following conclusions can be derived from \autoref{tab:lp_method_comparison}.
The use of the \emph{Barrier} algorithm in the pricing phase is not viable.
The pricing phase includes an iterative variable pricing phase.
This phase repeatedly adds variables to an LP model and resolve it.
The \emph{Barrier} algorithm solves every LP from scratch;
the \emph{Dual Simplex} reuses the previous basis and save considerable effort.
However, \emph{Barrier} performs better when just one massive LP is solved
(i.e., pricing does not execute).
Consequently, the configuration chosen was \emph{Dual Simplex} for the pricing phase, and \emph{Barrier} for the root relaxation of the final model.

\subsection{Comparison of \emph{faithful} against \emph{original}}
\label{sec:faithful_reimplementation}

Without a reimplementation of \emph{original}, any comparison would need to be made directly against the data in~\cite{dimitri_thesis}.
However, such comparison would hardly be fair, as it compares across machines, solvers, and programming languages.
Also, for example, it does not allow us to evaluate the effect plate size normalization (i.e., the \emph{round} procedure) would have in the \emph{original} formulation.
The purpose of this section is to show that \emph{faithful} may be fairly used in place of \emph{original}.
To do this, \autoref{tab:faithful_reimplementation} compares the number of model variables and number of plates
\footnote{
	Both~\cite{furini:2016} and~\cite{dimitri_thesis} do not present the number of constraints of the model.
	Instead, they present the number of plates, which strongly correlates to it.
} of the diverse model variants presented in~\cite{furini:2016,dimitri_thesis} (using the same 59 instances).

The \emph{Priced PP-G2KP} runs in~\cite{furini:2016,dimitri_thesis} had three time limits of one hour\footnote{
        The three one-hour time limits are to solve the:
        restricted model (i.e., obtaining a lower bound);
        iterative variable pricing (i.e., obtaining an upper bound);
        final model.
}.
Such configuration always generates a final model.
However, it also has two drawbacks:
(1) the computer performance may define the answer given in the first two phases, affecting the size of the final model (and making it harder to make a fair comparison);
(2) if the restricted model, or the iterated variable pricing, cannot be done in one hour, then the final model will probably hit the time limit too\footnote{
	In~\cite{furini:2016}, every run that hits one of the two first time limits hits the third time limit.
}.
We chose to use a single three-hour time limit.

\begin{table}
\caption{Comparison of \emph{faithful} against \emph{original}.}
\begin{tabular}{lccrrrr}
\hline\hline
Variant & T. L. & E. R. & O. \#v & F. \%v & O. \#p & F. \%p\\\hline
Complete PP-G2KP & 0 & 0 & 156,553,107 & 100.00 & 1,882,693 & 100.00\\
Complete +Cut-Position & 0 & 0 & 103,503,930 & 99.99 & 1,738,263 & 100.01\\
Complete +Redundant-Cut & 0 & 0 & 121,009,381 & 109.94 & 1,882,693 & 100.00\\
PP-G2KP (CP + RC) & 0 & 0 & 74,052,541 & 120.05 & 1,738,263 & 100.01\\
Restricted PP-G2KP & 0 & 0 & 5,335,976 & 99.28 & 306,673 & 99.99\\
Priced Restricted PP-G2KP & 0 & 1 & 3,904,683 & 102.20 & 305,690 & 99.99\\
(no purge) Priced PP-G2KP & 3 & 7 & 14,619,460 & 93.74 & 1,642,382 & 100.01\\
Priced PP-G2KP & 3 & 7 & 14,619,460 & 31.92 & 1,642,382 & 25.55\\\hline\hline
\end{tabular}
\label{tab:faithful_reimplementation}
\end{table}

\autoref{tab:faithful_reimplementation} references the names used in~\cite{furini:2016,dimitri_thesis}.
The \emph{Complete PP-G2KP} is the base variant with nothing enabled.
The \emph{PP-G2KP} just enables \emph{Cut-Position} and \emph{Redundant-Cut}.
\emph{Restricted PP-G2KP} and its priced version are solved inside \emph{Priced PP-G2KP} runs.
The \emph{original} had no \emph{purge} phase after pricing.
Consequently, for the columns that refer to \emph{original}, the last row presents the same data as the row above.

The sum of columns \emph{T. L.} (Time Limit) and \emph{E. R.} (Early Return) gives the number of instances excluded from consideration in the respective row.
Column \emph{T. L.} has the number of instances for which \emph{faithful} reached the time limit without generating the respective model variant\footnote{
	The instances stopped by the time limit are Hchl7s, okp2, and okp3.
}.
The column \emph{E. R.} has the number of instances for which our reimplementation found an optimal solution before generating the respective model variant\footnote{
	If the lower and upper bounds found during pricing are the same, then the optimal solution was found before generating the final model.
	The instances in which this happened for an unrestricted solution are 3s, A1s, CU1, CU2, W, cgcut1, and wang20.
	The instance A1s presented this behavior already in the pricing of the restricted model.
}.
Columns \emph{O. \#v} and \emph{O. \#v} refer to \emph{original}.
Column \emph{O. \#v} (\emph{O. \#p}) presents the sum of variables (plates) for the instances in which \emph{faithful} generated a model.
Columns \emph{F. \%v} and \emph{F. \%p} refer to \emph{faithful}.
Column \emph{R. \%v} (\emph{R. \%p}) has the sum of variables (plates) in the generated models, as a percentage of the quantity obtained by the original implementation.

The following conclusions can be derived from \autoref{tab:faithful_reimplementation}.
All variants, except \emph{Priced PP-G2KP}, are within \(\pm0.01\)\% of the expected number of plates (and, consequently, of constraints).
The \emph{Complete PP-G2KP}, \emph{Complete +Cut-Position}, and \emph{Restricted PP-G2KP} are within \(\pm1\)\% of the expected number of variables.
The number of variables in both \emph{Complete +Redundant-Cut} and \emph{PP-G2KP (CP + RC)} is \(10\sim20\)\% larger than expected.
Our reimplementation of \emph{Redundant-cut} reduction seems responsible for both deviations; however, it follows closely the description given in~\cite{dimitri_thesis}, so there was nothing else left to do.
The number of variables and plates in \emph{Priced} variants is not entirely deterministic.
The number of variables of \emph{Priced} variants is either slightly above (\(+2\)\%) or lower (\emph{\(-6\sim68\)\%}).

For all non-\emph{priced} variants, the fraction of the solving time spent in the model generation is negligible.
Consequently, the comparison presented in~\autoref{tab:faithful_reimplementation} is sufficient.
We cannot say the same for the \emph{priced} variants.
\cite{furini:2016,dimitri_thesis} does not report the size of the multiple LP models solved inside the iterative pricing (a phase of the pricing).
For instances in which \emph{original} and \emph{faithful} executed all phases of pricing and solved the final model, the \emph{original} spent 34.35\% of its time in the iterative pricing phase, while \emph{faithful} spent 61.69\%.
It is hard to pinpoint the source of this discrepancy.
One possible explanation is that, in \emph{original}, other phases took more time than they took in \emph{faithful}.
For example, \emph{faithful} uses the \emph{barrier} algorithm for the root node relaxation of the final model, which reduces the percentage of time spent in this phase.
Nevertheless, for the subset of the instances aforementioned, the total time spent by \emph{faithful} was about 13\% of the time spent by \emph{original}.
While the difference between machines and solvers does not allow us to infer much from that figure, we believe that the magnitude of the difference guarantees that we are not making a gross misrepresentation.

\subsection{Comparison of \emph{faithful} against \emph{enhanced}}
\label{sec:comparison}

The primary purpose of this section is to evaluate our contributions to the state-of-the-art.
Our contributions are the \emph{round} reduction (i.e., the plate size normalization presented in~\autoref{sec:psn}) and the \emph{enhanced} formulation (presented in \autoref{sec:enhanced}).
The state-of-the-art consists in a formulation (\emph{Complete PP-G2KP}), two reductions (\emph{Cut-Position} and \emph{Redundant-Cut}), and a pricing procedure presented in~\cite{furini:2016,dimitri_thesis}.
In this section, our reimplementation of \emph{Complete PP-G2KP} named \emph{faithful} (to distinguish from the data of the \emph{original}) is used.
We also reimplemented the reductions and the pricing procedure, but as \emph{enhanced} may also enable them, we avoid labeling these procedures as \emph{faithful}.

The \emph{faithful} and \emph{enhanced} formulations cannot be combined.
However, both allow enabling any combination of the optional procedures.
The only exception is \emph{Redundant-Cut}, which is unnecessary for \emph{enhanced} and, therefore, never applied to it.
Outside of this exception, in this section, \emph{Redundant-Cut} and \emph{Cut-Position} are always enabled.
These reductions never increase the number of variables (or constraints), cost a negligible amount of computational effort, and were already discussed in~\cite{furini:2016,dimitri_thesis}

We also examine the effects of our \emph{purge} procedure and warm-starting the non-\emph{priced} model.
The deterministic heuristic used to MIP-start the non-\emph{priced} models is the same used in the restricted priced model solved inside the pricing procedure.

\begin{table}
\rowcolors{1}{white}{gray-table-row}
\caption{Comparison of \emph{faithful} vs. \emph{enhanced} over the 59 instances used in~\cite{dimitri_thesis}.}
\begin{tabular}{lrrrrrrrr}
\hline\hline
Variant & T. T. & \#e & \#m & \#s & \#b & S. T. T. & \#variables & \#plates \\\hline
Faithful & 106,057 & -- & 59 & 53 & 0 & 41,257 & 88,901,964 & 1,738,366 \\
Enhanced & 25,538 & -- & 59 & 58 & 2 & 14,738 & 3,216,774 & 231,836 \\
F. +Rounding & 60,078 & -- & 59 & 56 & 0 & 27,678 & 60,316,964 & 610,402 \\
E. +Rounding & 14,169 & -- & 59 & 59 & 52 & 14,169 & 2,733,125 & 145,157 \\
F. +R. +Warming & 60,542 & -- & 59 & 56 & 0 & 28,142 & 60,316,964 & 610,402 \\
E. +R. +Warming & 9,778 & -- & 59 & 59 & 4 & 9,778 & 2,733,125 & 145,157 \\
Priced F. +R. +W. & 49,919 & 8 & 50 & 55 & 0 & 6,719 & 3,210,857 & 174,214 \\
Priced E. +R. +W. & 9,108 & 8 & 51 & 59 & 1 & 9,108 & 600,778 & 64,904 \\
P. F. +R. +W. -Purge & 50,054 & 8 & 50 & 55 & 0 & 6,854 & 8,072,810 & 544,892 \\
P. E. +R. +W. -Purge & 9,209 & 8 & 51 & 59 & 0 & 9,209 & 1,021,526 & 134,102 \\\hline\hline
\end{tabular}
\label{tab:contribution}
\end{table}

\begin{table}
\rowcolors{1}{white}{gray-table-row}
\caption{Fraction of the total time spent in each step (only runs that executed all steps).}
\begin{tabular}{lrrrrrrrrr}
\hline\hline
Variant & Time & E & H & RP & IP & FP & LP & BB \\\hline
Priced Faithful +R. +W. & 6,632 & 0.12 & 0.38 & 26.16 & 57.36 & 2.91 & 4.56 & 8.29 \\
Priced Enhanced +R. +W. & 1,178 & 0.03 & 2.18 & 50.89 & 23.66 & 0.46 & 2.70 & 19.95 \\
P. F. +R. +W. -Purge & 6,766 & 0.11 & 0.37 & 26.00 & 57.03 & 2.81 & 5.12 & 8.45 \\
P. E. +R. +W. -Purge & 1,185 & 0.03 & 2.18 & 50.70 & 23.64 & 0.46 & 2.83 & 20.09 \\\hline\hline
\end{tabular}
\label{tab:time_fractions}
\end{table}

The meaning of the columns in~\autoref{tab:contribution} follow:
\emph{T. T.} (Total Time) -- sum of the time spent in all instances including timeouts\footnote{
	The first row (Faithful) has two runs that ended in memory exhaustion. Those were penalized as they were timeouts.
} in seconds;
\emph{\#e} (early) -- number of instances in which pricing found an optimal solution (and, consequently, did not generate a final model);
\emph{\#m} (modeled) -- number of instances that generated a final model;
\emph{\#s} (solved) -- number of solved instances;
\emph{\#b} (best) -- number of instances that the respective variant solved faster than any other variant;
\emph{S. T. T.} (Solved Total Time) -- same as Total Time but excluding runs ended by time or memory limit;
\emph{\#variables} (\emph{\#plates}) -- sum of the variables (plates) in all generated final models (see column~\emph{\#m}).

Considering the data from~\autoref{tab:contribution} we can state that:
\begin{enumerate}
\item \emph{enhanced} solves more instances than \emph{faithful} (using at most 24\% of its time);
\item the number of variables of `Enhanced' is almost the same as `Priced F. +R. +W.';
\item between `Enhanced' and `Priced F. +R. +W.' the former has better results;
\item \emph{round} further reduces variables by \(14\sim32\)\% and plates by \(37\sim65\)\%;
\item MIP-starting \emph{enhanced} makes its slightly slower in 52 instances;
\item MIP-starting \emph{enhanced} saves more than one hour in the other 7 instances;
\item in `F. +R. +Warming', instances that could benefit from MIP-start timeout;
\item \emph{purge} greatly reduces the model size but has almost no effect on solving time;
\item the effects of applying \emph{pricing} to \emph{enhanced} are not much better than \emph{purge};
\item applying \emph{pricing} to \emph{faithful} is positive overall but loses one solved instance.
\end{enumerate}

In~\autoref{tab:time_fractions}, \emph{Time} is the sum of all time (in seconds) spent in the 47 instances that had all phases executed by all four variants considered\footnote{
	These are the same 47 indicated in row \emph{Priced F. +R. +W.} of \autoref{tab:contribution}.
	From the 59 instances dataset, 4 had timeout (Hchl4s, Hchl7s, okp2, and okp3), and 8 found an optimal solution inside pricing (3s, A1s, CU1, CU2, W, cgcut1, okp4, and wang20).
}.
All remaining columns present percentages of the time spent in a specific phase:
\emph{E} -- enumeration of cuts and plates (and all reductions);
\emph{H} -- restricted heuristic used to warm-start the restricted priced model;
\emph{RP} -- restricted pricing (not including the heuristic time);
\emph{IP} -- iterative pricing;
\emph{FP} -- final pricing;
\emph{LP} -- root node relaxation of the final model;
\emph{BB} -- branch-and-bound over the final model.

Considering the data from~\autoref{tab:time_fractions} we can state that:
\begin{enumerate}
\item both \emph{BB} and \emph{LP} phases are slightly faster with \emph{purge} as expected;
\item both \emph{E} and \emph{H} phases are almost negligible (at most 2\% with \emph{H} in \emph{enhanced});
\item together the \emph{RP} and \emph{IP} phases account for \(74.5\sim83.5\)\%;
\item \emph{RP} and \emph{IP} swap percentages between \emph{enhanced} and \emph{faithful};
\item \emph{faithful} shows some overhead in all phases strongly affected by model size;
\end{enumerate}

\subsection{Evaluating \emph{enhanced} against harder instances}
\label{sec:new_results}

The purposes of the experiment described by this section are:
(1) to show the limitations of the \emph{enhanced} against harder instances;
(2) to provide better bounds and new proven optimal values for such instances.
For this experiment, Gurobi was allowed to use the 12 physical cores of our machine\footnote{
	Gurobi distributes the effort of the B\&B phase equally among all cores.
	However, solving an LP (as a root node relaxation, or not) calls barrier, primal simplex, and dual simplex.
	Each of these three uses a single thread, and Gurobi stops when the first of them finish.
}.
\cite{velasco:2019} proposes a set of 80 hard instances to test the limitations of their bounding procedures; we use these instances in this section.
Only two variants were executed for this experiment, the \emph{priced} and non-\emph{priced} versions of \emph{enhanced} with \emph{Cut-Position}, \emph{round}, and \emph{MIP-start} enabled.
We also present the results for the \emph{restricted priced} variant because it executes inside \emph{priced} (the same reductions apply to it).
\autoref{tab:velasco_summary} presents a summary of all runs, and \autoref{tab:velasco_new_results} presents the improved bounds and solved instances.

\begin{table}
\caption{Summary table for the instances proposed in~\cite{velasco:2019}.}
\begin{tabular}{lrrrrrrr}
\hline\hline
C. & Variant & \#m & Avg. \#v & Avg. \#p & T. T. & \#s & Avg. S. T. \\\hline
\multirow{3}{*}{1} & Not Priced & 20 & 1,787,864.55 & 22,316.50 & 172,574 & 5 & 2,114.85 \\
                   & Restricted Priced & 13 & 467,692.15 & 17,139.00 & 180,051 & 5 & 3,610.29 \\
\vspace{1.5mm}     & Priced & 5 & 264,315.80 & 11,978.40 & 196,733 & 3 & 4,377.77 \\
\multirow{3}{*}{2} & Not Priced & 20 & 1,533,490.70 & 18,638.50 & 167,973 & 5 & 1,194.68 \\
                   & Restricted Priced & 20 & 453,159.70 & 18,638.30 & 155,184 & 8 & 3,198.11 \\
\vspace{1.5mm}     & Priced & 8 & 394,613.88 & 9,735.50 & 178,812 & 4 & 1,503.01 \\
\multirow{3}{*}{3} & Not Priced & 20 & 2,895,300.75 & 33,249.40 & 171,155 & 5 & 1,831.11 \\
                   & Restricted Priced & 10 & 431,913.00 & 15,895.80 & 174,569 & 5 & 2,513.80 \\
\vspace{1.5mm}     & Priced & 5 & 372,597.00 & 13,287.80 & 179,712 & 4 & 1,728.08 \\
\multirow{3}{*}{4} & Not Priced & 20 & 3,201,374.45 & 35,197.10 & 167,776 & 7 & 3,910.89 \\
                   & Restricted Priced & 10 & 497,802.20 & 17,011.00 & 197,047 & 2 & 1,323.65 \\
                   & Priced & 2 & 211,093.00 & 14,227.00 & 199,477 & 2 & 2,538.79 \\\hline\hline
\end{tabular}
\label{tab:velasco_summary}
\end{table}

\autoref{tab:velasco_summary} columns are:
\emph{C.} -- instance class (described in~\cite{velasco:2019}, 20 instances each);
\emph{Variant} -- the variant considered;
\emph{\#m} (modeled) -- number of instances in which the model was built before timeout;
\emph{Avg. \#v} and \emph{Avg. \#p} -- the average number of variables and plates in the \emph{\#m} instances that generated a final model for the respective variant;
\emph{T. T.} (Total Time) -- sum of the time spent in all instances in seconds, including timeouts;
\emph{\#s} (solved) -- number of instances solved;
\emph{Avg. S. T.} (Avg. Solved Time) -- as total time but excludes timeouts and divides by \emph{\#s}.
Averages were used instead of simple sums because the differences in the number of generated and solved models made the sums misleading.

Concerning the data from~\autoref{tab:velasco_summary}, we want to highlight some unexpected results:
(1) the total number of instances solved by the \emph{restricted priced} was slightly smaller than non-\emph{priced}, even with non-\emph{priced} solving the harder unrestricted problem;
(2) \emph{restricted priced} struggled to generate a final model before timeout;
(3) non-\emph{priced} solved more instances than \emph{priced} in all cases.
Ideally, the pricing procedure would significantly reduce the size of the model and, consequently, the root node relaxation and B\&B phases would take much less time to solve.
However, the gain in decreasing the size of the (already reduced) \emph{enhanced} model further does not seem to compensate for the cost of solving hard LPs more than once.
Also, previous sections have shown that reducing the model size does not guarantee that the solving time will be reduced by the same magnitude.

\autoref{tab:velasco_new_results} purpose is to allow querying the exact values for specific instances.
Even so, there are some gaps to fill.
For the instances presented in \autoref{tab:velasco_new_results},
the min / mean / max gap between the heuristic lower bound and the final lower bound were: 0.38 / 18.08 / 37.03 (non-\emph{priced}); 0.68 / 20.62 / 37.29 (\emph{restricted priced}); 9.17 / 19.38 / 32.24 (\emph{priced}).
In other words, no solution, or best bound, was given by the heuristic, and most of the time its solution was considerably improved.
For the reader convenience, we can also summarize that our experiment has:
proved 22 unrestricted optimal values (5 already proven by~\cite{velasco:2019}, confirming their results);
proved 22 restricted optimal values (in an overlapping but distinct subset of the instances);
improved lower bounds for 25 instances;
improved upper bounds for 58 instances.

\autoref{tab:velasco_new_results} groups lower and upper bounds that are valid for the unrestricted problem.
Column \emph{RP UB} (restricted priced upper bound) is kept separate as it is not a valid bound for the unrestricted problem.
Bold indicates the best unrestricted bounds for the instance.
If, for the same instance and variant, the LB and the UB are the same, both values are underlined.
The sub-headers mean:
\emph{RP} -- Restricted Priced (solved inside \emph{P} runs);
\emph{P} -- Priced;
\emph{NP} -- Not Priced;
\emph{V\&U} -- obtained by Velasco and Uchoa in~\cite{velasco:2019}.

\begin{table}
\let\mc\multicolumn
\rowcolors{3}{white}{gray-table-row}
\caption{Instances solved (restricted or unrestricted) or with improved bounds.}
\begin{tabular}{lrrrrrrrr}
\hline\hline
\hiderowcolors
Instance & \mc4c{Lower Bounds for Unrestricted} & RP UB & \mc3c{Upper Bounds for Unr.} \\\cline{2-5}\cline{7-9}
 & \mc1c{RP} & \mc1c{P} & \mc1c{NP} & \mc1c{V\&U} & & \mc1c{P} & \mc1c{NP} & \mc1c{V\&U} \\\hline
\showrowcolors
P1\_100\_200\_25\_1 & \underline{\textbf{27,251}} & \underline{\textbf{27,251}} & \underline{\textbf{27,251}} & \textbf{27,251} & \underline{27,251} & \underline{\textbf{27,251}} & \underline{\textbf{27,251}} & 27,340 \\
P1\_100\_200\_25\_2 & \underline{\textbf{25,090}} & \textbf{25,090} & \textbf{25,090} & 24,870 & \underline{25,090} & 25,403 & \textbf{25,389} & 25,522 \\
P1\_100\_200\_25\_3 & \underline{\textbf{25,730}} & \textbf{25,730} & \textbf{25,730} & \textbf{25,730} & \underline{25,730} & 25,974 & \textbf{25,909} & 26,088 \\
P1\_100\_200\_25\_4 & \underline{26,732} & \underline{\textbf{26,896}} & \underline{\textbf{26,896}} & 26,769 & \underline{26,732} & \underline{\textbf{26,896}} & \underline{\textbf{26,896}} & 27,051 \\
P1\_100\_200\_25\_5 & \textbf{26,152} & -- & \textbf{26,152} & 25,772 & 26,565 & -- & \textbf{26,617} & 26,857 \\
P1\_100\_200\_50\_1 & 28,388 & -- & \underline{\textbf{28,440}} & 28,388 & 28,504 & -- & \underline{\textbf{28,440}} & 28,558 \\
P1\_100\_200\_50\_2 & \underline{\textbf{26,276}} & \underline{\textbf{26,276}} & \underline{\textbf{26,276}} & \textbf{26,276} & \underline{26,276} & \underline{\textbf{26,276}} & \underline{\textbf{26,276}} & 26,326 \\
P1\_100\_200\_50\_3 & \textbf{27,192} & -- & \textbf{27,192} & 27,165 & 27,536 & -- & \textbf{27,483} & 27,679 \\
P1\_100\_200\_50\_4 & 28,058 & -- & \textbf{28,095} & 27,977 & 28,345 & -- & \textbf{28,340} & 28,388 \\
P1\_100\_200\_50\_5 & \textbf{27,722} & -- & \underline{\textbf{27,722}} & 27,603 & 27,930 & -- & \underline{\textbf{27,722}} & 28,009 \\
P1\_100\_400\_25\_1 & 53,247 & -- & 53,008 & \textbf{53,904} & 54,540 & -- & \textbf{54,707} & 55,038 \\
P1\_100\_400\_25\_2 & -- & -- & 41,275 & \textbf{44,581} & -- & -- & \textbf{47,091} & 47,097 \\
P1\_100\_400\_25\_3 & \underline{42,748} & -- & 46,222 & \textbf{47,455} & -- & -- & \textbf{49,371} & 49,473 \\
P1\_100\_400\_25\_4 & -- & -- & 38,567 & \textbf{40,517} & -- & -- & \textbf{46,069} & 46,078 \\
P1\_100\_400\_25\_5 & \underline{44,482} & -- & \textbf{53,220} & 53,205 & -- & -- & 54,120 & \textbf{54,063} \\
P1\_100\_400\_50\_1 & -- & -- & 53,831 & \textbf{55,856} & -- & -- & \textbf{56,897} & 57,074 \\
P1\_100\_400\_50\_2 & -- & -- & 40,440 & \textbf{48,373} & -- & -- & \textbf{51,754} & 51,893 \\
P1\_100\_400\_50\_4 & -- & -- & \textbf{55,107} & 52,708 & -- & -- & \textbf{55,654} & 55,661 \\
P1\_100\_400\_50\_5 & -- & -- & \textbf{53,749} & 53,502 & -- & -- & \textbf{55,005} & 55,454 \\
P2\_200\_100\_25\_1 & \underline{\textbf{21,494}} & \underline{\textbf{21,494}} & \underline{\textbf{21,494}} & \underline{\textbf{21,494}} & \underline{21,494} & \underline{\textbf{21,494}} & \underline{\textbf{21,494}} & \underline{\textbf{21,494}} \\
P2\_200\_100\_25\_2 & \underline{25,244} & \underline{\textbf{25,413}} & \underline{\textbf{25,413}} & \textbf{25,413} & \underline{25,244} & \underline{\textbf{25,413}} & \underline{\textbf{25,413}} & 25,648 \\
P2\_200\_100\_25\_3 & \underline{25,282} & \textbf{25,397} & \textbf{25,397} & \textbf{25,397} & \underline{25,282} & \textbf{25,640} & 25,647 & 25,723 \\
P2\_200\_100\_25\_4 & 25,729 & -- & \textbf{25,734} & 25,437 & 26,181 & -- & \textbf{26,239} & 26,898 \\
P2\_200\_100\_25\_5 & \underline{26,211} & \textbf{26,413} & \underline{\textbf{26,413}} & 26,220 & \underline{26,211} & 26,728 & \underline{\textbf{26,413}} & 26,898 \\
P2\_200\_100\_50\_1 & \textbf{25,679} & -- & 25,626 & 25,627 & 26,233 & -- & \textbf{26,282} & 26,447 \\
P2\_200\_100\_50\_2 & \underline{\textbf{27,801}} & \underline{\textbf{27,801}} & \underline{\textbf{27,801}} & 27,789 & \underline{27,801} & \underline{\textbf{27,801}} & \underline{\textbf{27,801}} & 27,943 \\
P2\_200\_100\_50\_3 & \underline{27,435} & \textbf{27,453} & \textbf{27,453} & \textbf{27,453} & \underline{27,435} & 27,584 & \textbf{27,579} & 27,596 \\
P2\_200\_100\_50\_4 & 27,395 & -- & \textbf{27,439} & 27,362 & 27,668 & -- & \textbf{27,704} & 27,718 \\
P2\_200\_100\_50\_5 & \underline{\textbf{29,386}} & \underline{\textbf{29,386}} & \underline{\textbf{29,386}} & \underline{\textbf{29,386}} & \underline{29,386} & \underline{\textbf{29,386}} & \underline{\textbf{29,386}} & \underline{\textbf{29,386}} \\
P2\_400\_100\_25\_1 & 49,327 & -- & \textbf{49,947} & 49,026 & 50,218 & -- & \textbf{50,365} & 51,006 \\
P2\_400\_100\_25\_2 & 48,312 & -- & \textbf{48,542} & 47,773 & 49,268 & -- & \textbf{49,315} & 49,908 \\
P2\_400\_100\_25\_3 & \textbf{46,970} & -- & 46,860 & 45,406 & 47,113 & -- & \textbf{47,204} & 48,938 \\
P2\_400\_100\_25\_4 & \textbf{51,051} & -- & 49,847 & 49,521 & 51,526 & -- & \textbf{51,600} & 52,229 \\
P2\_400\_100\_25\_5 & \textbf{49,620} & -- & 48,832 & 47,403 & 50,440 & -- & \textbf{50,580} & 54,248 \\
P2\_400\_100\_50\_1 & \underline{54,550} & 54,550 & \textbf{54,679} & 52,890 & \underline{54,550} & 54,981 & \textbf{54,916} & 55,629 \\
P2\_400\_100\_50\_2 & \textbf{54,821} & -- & 54,768 & 53,492 & 55,183 & -- & \textbf{55,181} & 55,543 \\
P2\_400\_100\_50\_3 & 54,141 & -- & \textbf{54,747} & 54,216 & 55,537 & -- & \textbf{55,709} & 56,065 \\
P2\_400\_100\_50\_4 & 53,375 & -- & \textbf{54,240} & 48,649 & 54,857 & -- & \textbf{54,987} & 55,604 \\
P2\_400\_100\_50\_5 & \textbf{53,763} & -- & 53,541 & 50,047 & 54,893 & -- & \textbf{54,918} & 55,471 \\
P3\_150\_150\_25\_1 & \underline{29,896} & \underline{\textbf{29,989}} & \underline{\textbf{29,989}} & 29,896 & \underline{29,896} & \underline{\textbf{29,989}} & \underline{\textbf{29,989}} & 30,005 \\
P3\_150\_150\_25\_2 & \textbf{29,345} & -- & 29,196 & 29,101 & 29,906 & -- & 29,965 & \textbf{29,961} \\
P3\_150\_150\_25\_3 & \underline{\textbf{30,286}} & \underline{\textbf{30,286}} & \underline{\textbf{30,286}} & \textbf{30,286} & \underline{30,286} & \underline{\textbf{30,286}} & \underline{\textbf{30,286}} & 30,327 \\
P3\_150\_150\_25\_5 & \underline{\textbf{31,332}} & \textbf{31,332} & \textbf{31,332} & 30,924 & \underline{31,332} & 31,715 & \textbf{31,682} & 31,839 \\
P3\_150\_150\_50\_1 & \underline{31,377} & \underline{\textbf{31,701}} & \underline{\textbf{31,701}} & \textbf{31,701} & \underline{31,377} & \underline{\textbf{31,701}} & \underline{\textbf{31,701}} & 31,892 \\
P3\_150\_150\_50\_2 & 30,846 & -- & \textbf{30,884} & \textbf{30,884} & 31,110 & -- & \textbf{31,008} & 31,115 \\
P3\_150\_150\_50\_3 & \underline{32,037} & \underline{\textbf{32,121}} & \underline{\textbf{32,121}} & 32,050 & \underline{32,037} & \underline{\textbf{32,121}} & \underline{\textbf{32,121}} & 32,240 \\
P3\_150\_150\_50\_4 & \textbf{31,925} & -- & \underline{\textbf{31,925}} & \textbf{31,925} & 32,210 & -- & \underline{\textbf{31,925}} & 32,070 \\
P3\_150\_150\_50\_5 & \textbf{31,631} & -- & 31,521 & 31,448 & 31,857 & -- & \textbf{31,896} & 31,901 \\
P3\_250\_250\_25\_1 & -- & -- & 51,027 & \textbf{58,480} & -- & -- & \textbf{60,548} & 60,611 \\
P3\_250\_250\_25\_2 & -- & -- & 63,646 & \textbf{68,070} & -- & -- & \textbf{73,316} & 73,339 \\
P3\_250\_250\_50\_1 & -- & -- & 59,072 & \textbf{67,603} & -- & -- & \textbf{76,117} & 76,341 \\
P3\_250\_250\_50\_2 & -- & -- & 62,772 & \textbf{75,569} & -- & -- & \textbf{82,644} & 82,666 \\
P4\_150\_150\_25\_1 & 30,870 & -- & \underline{\textbf{30,923}} & \textbf{30,923} & 31,094 & -- & \underline{\textbf{30,923}} & 31,130 \\
P4\_150\_150\_25\_2 & 30,576 & -- & \underline{\textbf{30,687}} & 30,460 & 30,786 & -- & \underline{\textbf{30,687}} & 30,931 \\
P4\_150\_150\_25\_3 & 30,257 & -- & \underline{\textbf{30,352}} & \underline{\textbf{30,352}} & 30,501 & -- & \underline{\textbf{30,352}} & \underline{\textbf{30,352}} \\
P4\_150\_150\_25\_4 & \underline{30,055} & \underline{\textbf{30,106}} & \underline{\textbf{30,106}} & \underline{\textbf{30,106}} & \underline{30,055} & \underline{\textbf{30,106}} & \underline{\textbf{30,106}} & \underline{\textbf{30,106}} \\
P4\_150\_150\_25\_5 & \textbf{30,582} & -- & 30,102 & \textbf{30,582} & 30,952 & -- & \textbf{31,228} & 31,286 \\
P4\_150\_150\_50\_1 & \underline{\textbf{31,673}} & \underline{\textbf{31,673}} & \underline{\textbf{31,673}} & \underline{\textbf{31,673}} & \underline{31,673} & \underline{\textbf{31,673}} & \underline{\textbf{31,673}} & \underline{\textbf{31,673}} \\
P4\_150\_150\_50\_2 & 32,302 & -- & \underline{\textbf{32,317}} & \textbf{32,317} & 32,434 & -- & \underline{\textbf{32,317}} & 32,423 \\
P4\_150\_150\_50\_3 & 30,906 & -- & \textbf{30,913} & 30,882 & 31,500 & -- & \textbf{31,519} & 31,756 \\
P4\_150\_150\_50\_4 & 31,912 & -- & \underline{\textbf{31,961}} & 31,912 & 32,206 & -- & \underline{\textbf{31,961}} & 32,140 \\
P4\_150\_150\_50\_5 & \textbf{32,027} & -- & 31,845 & 31,864 & 32,331 & -- & \textbf{32,308} & 32,484 \\
P4\_250\_250\_25\_4 & -- & -- & 69,530 & \textbf{79,476} & -- & -- & \textbf{81,634} & 81,839 \\
P4\_250\_250\_50\_2 & -- & -- & 67,675 & \textbf{77,206} & -- & -- & \textbf{87,314} & 87,331 \\
P4\_250\_250\_50\_4 & -- & -- & 69,063 & \textbf{78,359} & -- & -- & \textbf{86,941} & 87,069 \\\hline\hline
\end{tabular}
\label{tab:velasco_new_results}
\end{table}

%The original model solved only 7 of the 33 instances within timeout~\cite{furini:2016}.

\section{Conclusions}

The formulation proposed in~\cite{furini:2016} was the first to solve instances with more than a few pieces.
However, for most instances, the formulation was only able to solve them if they were first reduced by a complex pricing procedure which involved solving a restricted MIP and multiple LPs.
In this work, we propose an alternative formulation~\emph{enhanced} and an additional reduction procedure (plate size normalization, or \emph{round}).
Our alternative formulation already reduces the size of the model, and the total time spent, by more than the \emph{pricing} procedure.
Our reduction procedure is a simple change to the pseudo-polynomial enumeration ...

%Our contributions are threefold:
%(1) 
%This work 
The present work shows that small changes to the formulation and preprocessing improve the performance of the state-of-the-art ILP model for the G2KP by at least one order of magnitude.
We do believe that our main contribution, besides the performance gain for the specific model, is the suggestion that clever dominance rules may considerably improve pseudo-polynomial models (which often have strong bounds but large formulations) before resorting to more complicated techniques (as the pricing procedure proposed in~\cite{furini:2016} or column generation techniques).
Some ideas for future works follow: check if other related pseudo-polynomial formulations have the same potential for enhancement; apply the pricing procedure in~\cite{furini:2016} to the revised model; adapt the model to consider plates with defects.

% the environments 'definition', 'lemma', 'proposition', 'corollary',
% 'remark', and 'example' are defined in the LLNCS documentclass as well.
%\begin{proof}
% after a theorem follows a proof
%\end{proof}

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%\bibliographystyle{splncs04}
%\bibliography{revised_furini}

\begin{comment}
\pagebreak
\appendix
%
\section{The proof of~\autoref{only_normal_cuts_needed}}
\label{app:proof_only_normal_cuts_needed}

%The proof of \autoref{only_normal_cuts_needed} follows:
\begin{proof}
A guillotine packing may be represented as a binary tree.
A binary may represent a guillotine packing.
Each node of the tree is a plate; the root node is the original plate.
A node may have either two or zero children.
% If it has two, the plate was cut (vertically or horizontally) and the left child is the left one (vertical cut) or bottom one (horizontal cut); the right child is the opposite one.
If there are more leaf nodes with the same dimensions as a piece type than there is demand for such piece type, then let us consider as pieces just an arbitrary node subset of cardinality equal to the piece type demand, and the rest of the piece-sized leaf nodes as they are waste.

Consider a guillotine packing with one or more non-normal cuts.
If a node has exactly one piece among their left descendants and one piece among their right descendants, then the node and their descendants may be replaced by a build node of the same orientation.
A simple vertical (horizontal) build for pieces~\(i\) and~\(j\) is a plate of length~\(max(l_i, l_j)\) and width~\(w_i + w_j\) (length~\(l_i + l_j\) and width~\(max(w_i, w_j)\)), with a vertical cut at~\(l_i\) (horizontal cut at \(w_i\)) and, if the two pieces have not the same length (width), a horizontal (vertical) trimming cut at the length (width) of the piece with smaller length, in one of the child plates with the same width (length) as the piece with smaller length (width).
%Such build will always be a plate of equal or smaller dimensions than the original node and therefore may replace it without loss.

If we consider a build node as it was a piece node, the process may be repeated until the tree is reduced to a single node of dimensions equal to or smaller than the original plate.
An alternative guillotine packing tree can then be built by cutting the difference between the dimensions of this single build node and the original plate from the top and right with up to two trimming cuts, and then expanding the builds nodes.

For each piece type present in the original packing, the alternative packing contains either the same amount of copies, or the maximum amount allowed by the piece demand.
Every cut is distant from the left (bottom) borders by the summed width (length) of a set of demand-abiding pieces.
Therefore, the alternative packing has only normal cuts, and respects the demand constraints imposed by our claim. \qed

%All cuts in such alternative packing 
%Each cut in a guillotine packing may be seen as a node in a binary tree.
%The root node is a cut over the original plate.
%The left child node (if it exists) is the cut over the left subplate.
%The right child node is analogue.
%In the case the order the cuts were made is ambiguous (i.e., all could have done at the same stage and, consequently, their position in the three is interchangeable), the vertical cut in the left (horizontal cut in the bottom) was made before the one at its right (top).
%The cut is considered normal or non-normal based on its distance from the left (right) border of the original plate (not the border of the subplate it is cutting).

%Consider the leftest non-normal vertical cut~\(c\), and the cut that would be obtained by shifting it to the left until it is a normal cut~\(c^\prime\).
%If~\(c\) is replaced by~\(c^\prime\) it is clear that its right descendants (cuts over a now larger plate) may be adapted (by shifts to left and maybe the addition of new cuts) to keep generating the same pieces as before.
%The final subplates which had their right border delimited by~\(c\), however, are shortened by this change.
%Consider the horizontal distance between~\(c\) and a normal vertical cut~\(c^*\) delimiting the left border of one of such final subplates.
%If the distance is not the length of a piece, then the subplate was waste and shortening it will not affect the pieces produced.
%If the distance is the length of a piece, then the distance between~\(c^*\) and the left border is a linear combination including all copies of all pieces with such length (otherwise \(c\) would be a normal cut, it would be the linear combination of \(c^*\) plus one more copy of a piece that had yet demand).

%Each cut in a guillotine packing may be seem as the only cut over the plate it cuts, and any other cut which could be 

%For simplicity, in this proof, consider the borders of the original plate as they were obtained by normal cuts.
%Starting from the leftest non-normal vertical cut, such cut may be shifted to the left until it is a normal vertical cut (either transforming into a new cut, or becoming one with the closest vertical cut at its left already in the packing).
%Consider the leftest non-normal vertical cut~\(c\) and the closest normal cuts at its left (i.e., all normal cuts which would block an horizontal ray coming from the non-normal cut to the left), \(c\)~and each one of such normal cuts define one or more subplates delimited by them and by some horizontal cuts.
%Let us call~\(c^\prime\) the vertical cut obtained by shifting~\(c\) to the closest position at its left which makes it a normal cut.
%Replacing~\(c\) by~\(c^\prime\) reduces the subplates mentioned above (possibly to zero, if \(c^\prime\) was already present in the packing).
%The distance in the horizontal axis from the non-normal cut to the normal cut at its left may be the length of a piece or not.
%If it is not the length of a piece, then no piece was being extracted from the space between both cuts, and the shift do not change which pieces are obtained.
%If it is the length of a piece, and considering the cut is non-normal, this can only mean the already existing normal cut at the left of the non-normal cut is a linear combination which uses all the demand for pieces with such length (if this was not the case, the non-normal cut would be a valid linear combination with one more piece of that length and, consequently, a normal cut).
%Any piece extracted from such space between the non-normal cut and the normal cut at its left could have been extracted between such normal cut and the plate border.
% there could not be a plate there or the cut would be normal
%The process may be repeated until all non-normal vertical cuts are replaced by normal cuts.
%The same can be done for the horizontal cuts starting from the bottommost one.
\end{proof}

\section{Statistics of the revised model for considered instances}
\label{sec:appendix_table}

The data summarized in~\autoref{tab:appendix} is for the revised model with Cut-Position, Redundant-Cut, and plate size normalization enabled.
The setup is the same presented in~\autoref{sec:experiment_setup}.
The meaning of the columns follows:
Instance -- name of the instance;
\#vars -- number or variables in the model;
\#plates -- number of enumerated plates;
ABT (s) -- average time spent building the model (in seconds);
AST (s) -- average time spent solving the model (in seconds);
SDST (s) -- standard deviation of the time spent solving the model (in seconds);
Max LB -- maximum lower bound found among all runs;
Min UB -- minimum upper bound found among all runs;
\#o -- number of runs that found the optimal value (ten runs total).

\begin{table}
\caption{Summary of revised model ten runs over each considered instance.}
\setlength\tabcolsep{2.5px}
\def\arraystretch{1.1}
\begin{tabular}{@{\extracolsep{2pt}}lrrrrrrrrr@{}}
% instfname & num\_vars & num\_plates & solve\_time & lb & ub & finished \\ 
Instance & \#vars & \#plates & ABT (s) & AST (s) & SDST (s) & Max LB & Min UB & \#o\\
% latex table generated in R 3.6.1 by xtable 1.8-4 package
% Thu Nov 28 09:10:15 2019
\hline
A5 & 49,583 & 3,469 & 0.29 & 25.59 & 0.80 & 12,985 & 12,985 & 10 \\
CHL1 & 106,322 & 5,216 & 0.61 & 146.81 & 18.95 & 8,699 & 8,699 & 10 \\
CHL1s & 106,322 & 5,216 & 0.79 & 50.92 & 2.43 & 13,099 & 13,099 & 10 \\
CHL6 & 151,446 & 6,625 & 1.04 & 96.90 & 3.37 & 16,869 & 16,869 & 10 \\
CHL7 & 145,728 & 6,507 & 0.87 & 84.30 & 3.73 & 16,881 & 16,881 & 10 \\
CU1 & 8,681 & 1,014 & 0.07 & 0.71 & 0.01 & 12,330 & 12,330 & 10 \\
CU2 & 23,465 & 2,065 & 0.15 & 1.95 & 0.05 & 26,100 & 26,100 & 10 \\
CW1 & 20,983 & 2,090 & 0.16 & 4.09 & 0.08 & 6,402 & 6,402 & 10 \\
CW2 & 21,754 & 1,844 & 0.17 & 4.57 & 1.17 & 5,354 & 5,354 & 10 \\
CW3 & 49,788 & 3,406 & 0.39 & 9.25 & 0.40 & 5,689 & 5,689 & 10 \\
gcut10 & 1,026 & 201 & 0.01 & 0.05 & 0.00 & 903,435 & 903,435 & 10 \\
gcut11 & 6,424 & 751 & 0.05 & 0.31 & 0.00 & 955,389 & 955,389 & 10 \\
gcut12 & 22,581 & 1,647 & 0.13 & 0.88 & 0.02 & 970,744 & 970,744 & 10 \\
gcut2 & 2,319 & 376 & 0.05 & 0.13 & 0.00 & 59,307 & 59,307 & 10 \\
gcut3 & 8,760 & 973 & 0.08 & 0.26 & 0.00 & 60,241 & 60,241 & 10 \\
gcut4 & 28,387 & 1,995 & 0.30 & 2.31 & 0.06 & 60,942 & 60,942 & 10 \\
gcut5 & 394 & 112 & 0.03 & 0.07 & 0.00 & 195,582 & 195,582 & 10 \\
gcut6 & 1,100 & 218 & 0.02 & 0.05 & 0.00 & 236,305 & 236,305 & 10 \\
gcut7 & 3,786 & 521 & 0.03 & 0.14 & 0.00 & 238,974 & 238,974 & 10 \\
gcut8 & 32,369 & 2,249 & 0.18 & 1.24 & 0.02 & 245,758 & 245,758 & 10 \\
gcut9 & 513 & 132 & 0.03 & 0.05 & 0.01 & 919,476 & 919,476 & 10 \\
Hchl2 & 143,537 & 6,478 & 0.97 & 177.07 & 10.27 & 9,954 & 9,954 & 10 \\
Hchl3s & 72,880 & 4,309 & 0.43 & 164.99 & 30.44 & 12,215 & 12,215 & 10 \\
Hchl4s & 72,900 & 4,311 & 0.50 &  &  & 12,006 & 12,180 & 0 \\
Hchl6s & 178,510 & 9,762 & 1.26 & 144.81 & 1.83 & 61,040 & 61,040 & 10 \\
Hchl7s & 584,954 & 18,970 & 3.75 & 1,516.88 & 85.36 & 63,112 & 63,112 & 10 \\
okp1 & 112,103 & 5,447 & 0.82 & 46.93 & 1.94 & 27,589 & 27,589 & 10 \\
okp2 & 110,422 & 5,458 & 0.71 &  &  & 22,502 & 23,683 & 0 \\
okp3 & 42,360 & 3,356 & 0.27 & 1,255.36 & 269.38 & 24,019 & 24,019 & 10 \\
okp4 & 133,785 & 5,930 & 0.94 & 45.89 & 1.27 & 32,893 & 32,893 & 10 \\
okp5 & 184,109 & 6,860 & 1.18 & 92.39 & 2.87 & 27,923 & 27,923 & 10 \\
P1\_1* & 421,154 & 10,854 & 2.64 & 980.57 & 83.58 & 27,251 & 27,251 & 10 \\
P1\_2* & 584,842 & 13,473 & 3.74 &  &  & 25,089 & 25,523 & 0 \\
P1\_3* & 585,354 & 13,143 & 3.86 &  &  & 25,730 & 26,024 & 0 \\
P1\_4* & 501,172 & 12,647 & 3.33 & 2,403.55 & 422.92 & 26,896 & 26,896 & 8 \\
P1\_5* & 584,810 & 12,877 & 3.80 &  &  & 26,152 & 26,621 & 0 \\
STS4 & 43,843 & 3,048 & 0.25 & 17.96 & 0.71 & 9,700 & 9,700 & 10 \\
STS4s & 43,843 & 3,048 & 0.31 & 17.18 & 0.42 & 9,770 & 9,770 & 10 \\
\hline
\end{tabular}
\text{* These instances are in fact the P1\_100\_200\_25\_1 to P1\_100\_200\_25\_5 from~\cite{velasco:2019}.}
\label{tab:appendix}
\end{table}

\begin{theorem}
Every non-trivial linear combination of the vector \emph{s} with nonnegative coefficients restricted by the pieces demand and the value smaller-than-or-equal-to 
\end{theorem}

\begin{algorithm}[!htb]
\caption{}
\begin{algorithmic}[1]
\Procedure{rlc}{$S, s_1, \dots, s_n, d_1, \dots, d_n$}
  % TODO: get better emptyset
  \State \(C \gets \emptyset\) 

  \For{\(i \gets 1\) to \(n\)}
    \State \(C^\prime \gets \emptyset\)
    \For{\(y \in C\)}
      \For{\(q \gets 1\) to \(d_i\)}
        \State \(C^\prime \gets C^\prime \cup \{y + s_i \times q\}\)
      \EndFor
    \EndFor
    \State \(C \gets C \cup C^\prime\)

    \For{\(q \gets 1\) to \(d_i\)}
      \State \(C \gets C \cup \{s_i \times q\}\)
    \EndFor
  \EndFor

  \State \textbf{return}~\(C\)
\EndProcedure
\end{algorithmic}
\end{algorithm}

% TODO: check if there is a way to supress the block ends and use only
% identation to demark blocks
% TODO: ASK OLINTO IF WE SHOULD USE BRACKETS INSTEAD OF SUBSCRIPT FOR
% INDEXING VECTORS
\begin{algorithm}[!htb]
\caption{}
\begin{algorithmic}[1]
\Procedure{rlc}{$S, s_1, \dots, s_n, d_1, \dots, d_n$}
  \State \(b_1, \dots, b_S \gets\) false\(,\dots,\)false
  
  \For{\(i \gets 1\) to \(n\)}%\label{begin_trivial_bounds}\Comment{Stores one-item solutions}

    \For{\(y \gets S\) to \(1\) by step \(-1\)}
      \If{\(b_y\)}
        \For{\(q \gets 1\) to \(d_i\)}
	  \State \(y^\prime \gets y + s_i \times q\)
	  \If{\(y^\prime \leq S\)}
            \State \(b_{y^\prime} \gets\) true
	  \EndIf
	\EndFor
      \EndIf
    \EndFor

    \For{\(q \gets 1\) to \(d_i\)}
      \If{\(s_i \times q \leq S\)}
        \State \(b_{s_i \times q} \gets\) true
      \EndIf
    \EndFor
  \EndFor

  \State \textbf{return}~\(b\)
\EndProcedure
\end{algorithmic}
\end{algorithm}
\end{comment}

\begin{comment}
% ORIGINAL FURINI MODEL
\begin{align}
\mbox{max.} & \sum_{j \in \bar{J}} p_j y_j \label{eq:objfun}\\
% UNFORTUNATELY, THE HSPACE BELOW MAY NEED MANUAL ADJUSTMENT
\mbox{s.t.} & \specialcell{\sum_{o \in O}\sum_{q \in Q_{jo}} x^o_{qj} + y_j \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qkj} x^o_{qk} \hspace*{0.15\textwidth} \forall j \in \bar{J}, j \neq 0,}\label{eq:}\\
            & \specialcell{\sum_{o \in O}\sum_{q \in Q_{jo}} x^o_{qj} \leq \sum_{k \in J}\sum_{o \in O}\sum_{q \in Q_{ko}} a^o_{qkj} x^o_{qk} \hspace*{\fill} \forall j \in J\setminus\bar{J},}\label{eq:}\\
	    & \specialcell{\sum_{o \in O}\sum_{q \in Q_{0o}} x^o_{q0} + y_0 \leq 1 \hspace*{\fill} ,}\label{eq:}\\
            & \specialcell{y_j \leq u_j \hspace*{\fill} \forall j \in \bar{J},}\label{eq:}\\
	    % TODO: fix equation below, the forall part is too long and clashes with the long equation in the first line
	    & \specialcell{x^o_{qj} \in \mathbb{N}^0 \hspace*{\fill} \forall j \in J, o \in O, q \in Q_{jo},}\label{eq:}\\
            & \specialcell{y_j \in \mathbb{N}^0 \hspace*{\fill} \forall j \in \bar{J}.}\label{eq:}
\end{align}
\end{comment}

\begin{acknowledgements}
ACKNOWLEDGE Furini MESSAGE EXCHANGE AND Martin MESSAGE EXCHANGE
\end{acknowledgements}

% Authors must disclose all relationships or interests that
% could have direct or potential influence or impart bias on
% the work:
%
\section*{Conflict of interest}
The authors declare that they have no conflict of interest.

% BibTeX users please use one of
%\bibliographystyle{spbasic}  % basic style, author-year citations
\bibliographystyle{spmpsci}   % mathematics and physical sciences
%\bibliographystyle{spphys}   % APS-like style for physics
\bibliography{mybib} % name your BibTeX data base

\end{document}

